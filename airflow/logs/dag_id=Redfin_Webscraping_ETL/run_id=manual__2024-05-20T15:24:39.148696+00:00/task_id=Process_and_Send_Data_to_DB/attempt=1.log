[2024-05-20T15:24:41.120+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-05-20T15:24:41.145+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: Redfin_Webscraping_ETL.Process_and_Send_Data_to_DB manual__2024-05-20T15:24:39.148696+00:00 [queued]>
[2024-05-20T15:24:41.150+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: Redfin_Webscraping_ETL.Process_and_Send_Data_to_DB manual__2024-05-20T15:24:39.148696+00:00 [queued]>
[2024-05-20T15:24:41.150+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 6
[2024-05-20T15:24:41.159+0000] {taskinstance.py:2330} INFO - Executing <Task(SparkSubmitOperator): Process_and_Send_Data_to_DB> on 2024-05-20 15:24:39.148696+00:00
[2024-05-20T15:24:41.173+0000] {standard_task_runner.py:63} INFO - Started process 345 to run task
[2024-05-20T15:24:41.177+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'Redfin_Webscraping_ETL', 'Process_and_Send_Data_to_DB', 'manual__2024-05-20T15:24:39.148696+00:00', '--job-id', '473', '--raw', '--subdir', 'DAGS_FOLDER/webscrapping.py', '--cfg-path', '/tmp/tmpxwmfz5_0']
[2024-05-20T15:24:41.181+0000] {standard_task_runner.py:91} INFO - Job 473: Subtask Process_and_Send_Data_to_DB
[2024-05-20T15:24:41.222+0000] {task_command.py:426} INFO - Running <TaskInstance: Redfin_Webscraping_ETL.Process_and_Send_Data_to_DB manual__2024-05-20T15:24:39.148696+00:00 [running]> on host f39758f400c5
[2024-05-20T15:24:41.292+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='svc' AIRFLOW_CTX_DAG_ID='Redfin_Webscraping_ETL' AIRFLOW_CTX_TASK_ID='Process_and_Send_Data_to_DB' AIRFLOW_CTX_EXECUTION_DATE='2024-05-20T15:24:39.148696+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-05-20T15:24:39.148696+00:00'
[2024-05-20T15:24:41.294+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-05-20T15:24:41.309+0000] {base.py:84} INFO - Using connection ID 'spark_default' for task execution.
[2024-05-20T15:24:41.311+0000] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,com.datastax.spark:spark-cassandra-connector_2.12:3.5.0,org.apache.kafka:kafka-clients:3.5.1,commons-logging:commons-logging:1.2 --name arrow-spark --deploy-mode client dags/includes/jobs/spark_consumer.py
[2024-05-20T15:24:41.412+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.11/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-05-20T15:24:43.466+0000] {spark_submit.py:571} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-05-20T15:24:43.527+0000] {spark_submit.py:571} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2024-05-20T15:24:43.530+0000] {spark_submit.py:571} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2024-05-20T15:24:43.534+0000] {spark_submit.py:571} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2024-05-20T15:24:43.535+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency
[2024-05-20T15:24:43.536+0000] {spark_submit.py:571} INFO - org.apache.kafka#kafka-clients added as a dependency
[2024-05-20T15:24:43.537+0000] {spark_submit.py:571} INFO - commons-logging#commons-logging added as a dependency
[2024-05-20T15:24:43.537+0000] {spark_submit.py:571} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-f854a231-f0ba-47dd-b366-3112861b0962;1.0
[2024-05-20T15:24:43.538+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-05-20T15:24:43.671+0000] {spark_submit.py:571} INFO - found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
[2024-05-20T15:24:43.710+0000] {spark_submit.py:571} INFO - found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
[2024-05-20T15:24:43.754+0000] {spark_submit.py:571} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2024-05-20T15:24:43.777+0000] {spark_submit.py:571} INFO - found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2024-05-20T15:24:43.810+0000] {spark_submit.py:571} INFO - found org.xerial.snappy#snappy-java;1.1.10.3 in central
[2024-05-20T15:24:43.830+0000] {spark_submit.py:571} INFO - found org.slf4j#slf4j-api;2.0.7 in central
[2024-05-20T15:24:43.842+0000] {spark_submit.py:571} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2024-05-20T15:24:43.848+0000] {spark_submit.py:571} INFO - found org.apache.commons#commons-pool2;2.11.1 in central
[2024-05-20T15:24:43.857+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector_2.12;3.5.0 in central
[2024-05-20T15:24:43.865+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0 in central
[2024-05-20T15:24:43.873+0000] {spark_submit.py:571} INFO - found org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central
[2024-05-20T15:24:43.881+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-core-shaded;4.13.0 in central
[2024-05-20T15:24:43.890+0000] {spark_submit.py:571} INFO - found com.datastax.oss#native-protocol;1.5.0 in central
[2024-05-20T15:24:43.897+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central
[2024-05-20T15:24:43.904+0000] {spark_submit.py:571} INFO - found com.typesafe#config;1.4.1 in central
[2024-05-20T15:24:43.911+0000] {spark_submit.py:571} INFO - found io.dropwizard.metrics#metrics-core;4.1.18 in central
[2024-05-20T15:24:43.919+0000] {spark_submit.py:571} INFO - found org.hdrhistogram#HdrHistogram;2.1.12 in central
[2024-05-20T15:24:43.928+0000] {spark_submit.py:571} INFO - found org.reactivestreams#reactive-streams;1.0.3 in central
[2024-05-20T15:24:43.936+0000] {spark_submit.py:571} INFO - found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2024-05-20T15:24:43.946+0000] {spark_submit.py:571} INFO - found com.github.spotbugs#spotbugs-annotations;3.1.12 in central
[2024-05-20T15:24:43.953+0000] {spark_submit.py:571} INFO - found com.google.code.findbugs#jsr305;3.0.2 in central
[2024-05-20T15:24:43.970+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central
[2024-05-20T15:24:43.985+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-query-builder;4.13.0 in central
[2024-05-20T15:24:43.992+0000] {spark_submit.py:571} INFO - found org.apache.commons#commons-lang3;3.10 in central
[2024-05-20T15:24:44.014+0000] {spark_submit.py:571} INFO - found com.thoughtworks.paranamer#paranamer;2.8 in central
[2024-05-20T15:24:44.020+0000] {spark_submit.py:571} INFO - found org.scala-lang#scala-reflect;2.12.11 in central
[2024-05-20T15:24:44.033+0000] {spark_submit.py:571} INFO - found org.apache.kafka#kafka-clients;3.5.1 in central
[2024-05-20T15:24:44.043+0000] {spark_submit.py:571} INFO - found com.github.luben#zstd-jni;1.5.5-1 in central
[2024-05-20T15:24:44.056+0000] {spark_submit.py:571} INFO - found org.lz4#lz4-java;1.8.0 in central
[2024-05-20T15:24:44.069+0000] {spark_submit.py:571} INFO - found commons-logging#commons-logging;1.2 in central
[2024-05-20T15:24:44.482+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.5.1/kafka-clients-3.5.1.jar ...
[2024-05-20T15:24:45.903+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] org.apache.kafka#kafka-clients;3.5.1!kafka-clients.jar (1817ms)
[2024-05-20T15:24:45.941+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.2/commons-logging-1.2.jar ...
[2024-05-20T15:24:45.986+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] commons-logging#commons-logging;1.2!commons-logging.jar (80ms)
[2024-05-20T15:24:46.028+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.1/spark-token-provider-kafka-0-10_2.12-3.5.1.jar ...
[2024-05-20T15:24:46.077+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1!spark-token-provider-kafka-0-10_2.12.jar (90ms)
[2024-05-20T15:24:46.123+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
[2024-05-20T15:24:46.183+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (105ms)
[2024-05-20T15:24:46.239+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...
[2024-05-20T15:24:53.169+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (6981ms)
[2024-05-20T15:24:53.227+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...
[2024-05-20T15:24:57.658+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (4482ms)
[2024-05-20T15:24:57.694+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...
[2024-05-20T15:24:58.407+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (745ms)
[2024-05-20T15:24:58.444+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...
[2024-05-20T15:24:58.505+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (96ms)
[2024-05-20T15:24:58.592+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector-driver_2.12/3.5.0/spark-cassandra-connector-driver_2.12-3.5.0.jar ...
[2024-05-20T15:24:59.240+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0!spark-cassandra-connector-driver_2.12.jar (728ms)
[2024-05-20T15:24:59.281+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.11.0/scala-collection-compat_2.12-2.11.0.jar ...
[2024-05-20T15:24:59.418+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] org.scala-lang.modules#scala-collection-compat_2.12;2.11.0!scala-collection-compat_2.12.jar (175ms)
[2024-05-20T15:24:59.749+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-core-shaded/4.13.0/java-driver-core-shaded-4.13.0.jar ...
[2024-05-20T15:25:01.021+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] com.datastax.oss#java-driver-core-shaded;4.13.0!java-driver-core-shaded.jar (1598ms)
[2024-05-20T15:25:01.075+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-mapper-runtime/4.13.0/java-driver-mapper-runtime-4.13.0.jar ...
[2024-05-20T15:25:01.122+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] com.datastax.oss#java-driver-mapper-runtime;4.13.0!java-driver-mapper-runtime.jar(bundle) (96ms)
[2024-05-20T15:25:01.152+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.10/commons-lang3-3.10.jar ...
[2024-05-20T15:25:01.265+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] org.apache.commons#commons-lang3;3.10!commons-lang3.jar (141ms)
[2024-05-20T15:25:01.303+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar ...
[2024-05-20T15:25:01.349+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.8!paranamer.jar(bundle) (83ms)
[2024-05-20T15:25:01.388+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.11/scala-reflect-2.12.11.jar ...
[2024-05-20T15:25:02.669+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.11!scala-reflect.jar (1318ms)
[2024-05-20T15:25:02.714+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/com/datastax/oss/native-protocol/1.5.0/native-protocol-1.5.0.jar ...
[2024-05-20T15:25:02.896+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] com.datastax.oss#native-protocol;1.5.0!native-protocol.jar(bundle) (225ms)
[2024-05-20T15:25:02.929+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-shaded-guava/25.1-jre-graal-sub-1/java-driver-shaded-guava-25.1-jre-graal-sub-1.jar ...
[2024-05-20T15:25:03.652+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1!java-driver-shaded-guava.jar (755ms)
[2024-05-20T15:25:03.699+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/com/typesafe/config/1.4.1/config-1.4.1.jar ...
[2024-05-20T15:25:03.811+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] com.typesafe#config;1.4.1!config.jar(bundle) (157ms)
[2024-05-20T15:25:03.849+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/4.1.18/metrics-core-4.1.18.jar ...
[2024-05-20T15:25:03.906+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] io.dropwizard.metrics#metrics-core;4.1.18!metrics-core.jar(bundle) (92ms)
[2024-05-20T15:25:03.955+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar ...
[2024-05-20T15:25:04.019+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] org.hdrhistogram#HdrHistogram;2.1.12!HdrHistogram.jar(bundle) (112ms)
[2024-05-20T15:25:04.053+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar ...
[2024-05-20T15:25:04.104+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.3!reactive-streams.jar (83ms)
[2024-05-20T15:25:04.149+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar ...
[2024-05-20T15:25:04.182+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] com.github.stephenc.jcip#jcip-annotations;1.0-1!jcip-annotations.jar (76ms)
[2024-05-20T15:25:04.219+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/com/github/spotbugs/spotbugs-annotations/3.1.12/spotbugs-annotations-3.1.12.jar ...
[2024-05-20T15:25:04.277+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] com.github.spotbugs#spotbugs-annotations;3.1.12!spotbugs-annotations.jar (93ms)
[2024-05-20T15:25:04.344+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...
[2024-05-20T15:25:04.395+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (113ms)
[2024-05-20T15:25:04.464+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-query-builder/4.13.0/java-driver-query-builder-4.13.0.jar ...
[2024-05-20T15:25:04.558+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] com.datastax.oss#java-driver-query-builder;4.13.0!java-driver-query-builder.jar(bundle) (161ms)
[2024-05-20T15:25:04.599+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/com/github/luben/zstd-jni/1.5.5-1/zstd-jni-1.5.5-1.jar ...
[2024-05-20T15:25:06.167+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] com.github.luben#zstd-jni;1.5.5-1!zstd-jni.jar (1608ms)
[2024-05-20T15:25:06.216+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...
[2024-05-20T15:25:06.401+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (232ms)
[2024-05-20T15:25:06.403+0000] {spark_submit.py:571} INFO - :: resolution report :: resolve 546ms :: artifacts dl 22321ms
[2024-05-20T15:25:06.406+0000] {spark_submit.py:571} INFO - :: modules in use:
[2024-05-20T15:25:06.411+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]
[2024-05-20T15:25:06.412+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]
[2024-05-20T15:25:06.413+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]
[2024-05-20T15:25:06.413+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]
[2024-05-20T15:25:06.414+0000] {spark_submit.py:571} INFO - com.datastax.oss#native-protocol;1.5.0 from central in [default]
[2024-05-20T15:25:06.415+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0 from central in [default]
[2024-05-20T15:25:06.415+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12;3.5.0 from central in [default]
[2024-05-20T15:25:06.416+0000] {spark_submit.py:571} INFO - com.github.luben#zstd-jni;1.5.5-1 from central in [default]
[2024-05-20T15:25:06.417+0000] {spark_submit.py:571} INFO - com.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]
[2024-05-20T15:25:06.418+0000] {spark_submit.py:571} INFO - com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2024-05-20T15:25:06.419+0000] {spark_submit.py:571} INFO - com.google.code.findbugs#jsr305;3.0.2 from central in [default]
[2024-05-20T15:25:06.420+0000] {spark_submit.py:571} INFO - com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
[2024-05-20T15:25:06.421+0000] {spark_submit.py:571} INFO - com.typesafe#config;1.4.1 from central in [default]
[2024-05-20T15:25:06.421+0000] {spark_submit.py:571} INFO - commons-logging#commons-logging;1.2 from central in [default]
[2024-05-20T15:25:06.422+0000] {spark_submit.py:571} INFO - io.dropwizard.metrics#metrics-core;4.1.18 from central in [default]
[2024-05-20T15:25:06.422+0000] {spark_submit.py:571} INFO - org.apache.commons#commons-lang3;3.10 from central in [default]
[2024-05-20T15:25:06.423+0000] {spark_submit.py:571} INFO - org.apache.commons#commons-pool2;2.11.1 from central in [default]
[2024-05-20T15:25:06.423+0000] {spark_submit.py:571} INFO - org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
[2024-05-20T15:25:06.424+0000] {spark_submit.py:571} INFO - org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
[2024-05-20T15:25:06.424+0000] {spark_submit.py:571} INFO - org.apache.kafka#kafka-clients;3.5.1 from central in [default]
[2024-05-20T15:25:06.425+0000] {spark_submit.py:571} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
[2024-05-20T15:25:06.425+0000] {spark_submit.py:571} INFO - org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
[2024-05-20T15:25:06.426+0000] {spark_submit.py:571} INFO - org.hdrhistogram#HdrHistogram;2.1.12 from central in [default]
[2024-05-20T15:25:06.426+0000] {spark_submit.py:571} INFO - org.lz4#lz4-java;1.8.0 from central in [default]
[2024-05-20T15:25:06.427+0000] {spark_submit.py:571} INFO - org.reactivestreams#reactive-streams;1.0.3 from central in [default]
[2024-05-20T15:25:06.428+0000] {spark_submit.py:571} INFO - org.scala-lang#scala-reflect;2.12.11 from central in [default]
[2024-05-20T15:25:06.428+0000] {spark_submit.py:571} INFO - org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]
[2024-05-20T15:25:06.429+0000] {spark_submit.py:571} INFO - org.slf4j#slf4j-api;2.0.7 from central in [default]
[2024-05-20T15:25:06.430+0000] {spark_submit.py:571} INFO - org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
[2024-05-20T15:25:06.431+0000] {spark_submit.py:571} INFO - :: evicted modules:
[2024-05-20T15:25:06.431+0000] {spark_submit.py:571} INFO - org.apache.kafka#kafka-clients;3.4.1 by [org.apache.kafka#kafka-clients;3.5.1] in [default]
[2024-05-20T15:25:06.432+0000] {spark_submit.py:571} INFO - commons-logging#commons-logging;1.1.3 by [commons-logging#commons-logging;1.2] in [default]
[2024-05-20T15:25:06.433+0000] {spark_submit.py:571} INFO - com.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]
[2024-05-20T15:25:06.433+0000] {spark_submit.py:571} INFO - org.slf4j#slf4j-api;1.7.26 by [org.slf4j#slf4j-api;2.0.7] in [default]
[2024-05-20T15:25:06.435+0000] {spark_submit.py:571} INFO - org.xerial.snappy#snappy-java;1.1.10.1 by [org.xerial.snappy#snappy-java;1.1.10.3] in [default]
[2024-05-20T15:25:06.435+0000] {spark_submit.py:571} INFO - org.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.7] in [default]
[2024-05-20T15:25:06.436+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-05-20T15:25:06.437+0000] {spark_submit.py:571} INFO - |                  |            modules            ||   artifacts   |
[2024-05-20T15:25:06.438+0000] {spark_submit.py:571} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-05-20T15:25:06.443+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-05-20T15:25:06.444+0000] {spark_submit.py:571} INFO - |      default     |   35  |   0   |   0   |   6   ||   29  |   27  |
[2024-05-20T15:25:06.445+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-05-20T15:25:06.467+0000] {spark_submit.py:571} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-f854a231-f0ba-47dd-b366-3112861b0962
[2024-05-20T15:25:06.468+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-05-20T15:25:06.627+0000] {spark_submit.py:571} INFO - 29 artifacts copied, 0 already retrieved (81094kB/160ms)
[2024-05-20T15:25:06.940+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-05-20T15:25:09.216+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO SparkContext: Running Spark version 3.5.1
[2024-05-20T15:25:09.219+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO SparkContext: OS info Linux, 6.6.26-linuxkit, aarch64
[2024-05-20T15:25:09.219+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO SparkContext: Java version 11.0.23
[2024-05-20T15:25:09.241+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO ResourceUtils: ==============================================================
[2024-05-20T15:25:09.242+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-05-20T15:25:09.242+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO ResourceUtils: ==============================================================
[2024-05-20T15:25:09.242+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO SparkContext: Submitted application: Redfin_Properties_Consumer
[2024-05-20T15:25:09.271+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-05-20T15:25:09.278+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO ResourceProfile: Limiting resource is cpu
[2024-05-20T15:25:09.279+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-05-20T15:25:09.383+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO SecurityManager: Changing view acls to: ***
[2024-05-20T15:25:09.383+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO SecurityManager: Changing modify acls to: ***
[2024-05-20T15:25:09.384+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO SecurityManager: Changing view acls groups to:
[2024-05-20T15:25:09.384+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO SecurityManager: Changing modify acls groups to:
[2024-05-20T15:25:09.384+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-05-20T15:25:09.802+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO Utils: Successfully started service 'sparkDriver' on port 34871.
[2024-05-20T15:25:09.855+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO SparkEnv: Registering MapOutputTracker
[2024-05-20T15:25:09.893+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO SparkEnv: Registering BlockManagerMaster
[2024-05-20T15:25:09.907+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-05-20T15:25:09.908+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-05-20T15:25:09.911+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-05-20T15:25:09.929+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f3fb9d16-ef8c-4eab-9c88-f9bef6adb0d2
[2024-05-20T15:25:09.939+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-05-20T15:25:09.950+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:09 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-05-20T15:25:10.053+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-05-20T15:25:10.114+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-05-20T15:25:10.142+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://f39758f400c5:34871/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1716218709202
[2024-05-20T15:25:10.142+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar at spark://f39758f400c5:34871/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1716218709202
[2024-05-20T15:25:10.143+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.5.1.jar at spark://f39758f400c5:34871/jars/org.apache.kafka_kafka-clients-3.5.1.jar with timestamp 1716218709202
[2024-05-20T15:25:10.143+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.2.jar at spark://f39758f400c5:34871/jars/commons-logging_commons-logging-1.2.jar with timestamp 1716218709202
[2024-05-20T15:25:10.144+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://f39758f400c5:34871/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1716218709202
[2024-05-20T15:25:10.144+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://f39758f400c5:34871/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1716218709202
[2024-05-20T15:25:10.145+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://f39758f400c5:34871/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1716218709202
[2024-05-20T15:25:10.146+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://f39758f400c5:34871/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1716218709202
[2024-05-20T15:25:10.147+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://f39758f400c5:34871/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1716218709202
[2024-05-20T15:25:10.147+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://f39758f400c5:34871/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1716218709202
[2024-05-20T15:25:10.147+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar at spark://f39758f400c5:34871/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1716218709202
[2024-05-20T15:25:10.147+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://f39758f400c5:34871/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1716218709202
[2024-05-20T15:25:10.148+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://f39758f400c5:34871/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1716218709202
[2024-05-20T15:25:10.148+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://f39758f400c5:34871/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1716218709202
[2024-05-20T15:25:10.152+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://f39758f400c5:34871/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1716218709202
[2024-05-20T15:25:10.152+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://f39758f400c5:34871/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1716218709202
[2024-05-20T15:25:10.152+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://f39758f400c5:34871/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1716218709202
[2024-05-20T15:25:10.153+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://f39758f400c5:34871/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1716218709202
[2024-05-20T15:25:10.154+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://f39758f400c5:34871/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1716218709202
[2024-05-20T15:25:10.154+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://f39758f400c5:34871/jars/com.typesafe_config-1.4.1.jar with timestamp 1716218709202
[2024-05-20T15:25:10.154+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://f39758f400c5:34871/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1716218709202
[2024-05-20T15:25:10.155+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://f39758f400c5:34871/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1716218709202
[2024-05-20T15:25:10.155+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://f39758f400c5:34871/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1716218709202
[2024-05-20T15:25:10.155+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://f39758f400c5:34871/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1716218709202
[2024-05-20T15:25:10.155+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://f39758f400c5:34871/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1716218709202
[2024-05-20T15:25:10.155+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://f39758f400c5:34871/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1716218709202
[2024-05-20T15:25:10.155+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://f39758f400c5:34871/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1716218709202
[2024-05-20T15:25:10.156+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.luben_zstd-jni-1.5.5-1.jar at spark://f39758f400c5:34871/jars/com.github.luben_zstd-jni-1.5.5-1.jar with timestamp 1716218709202
[2024-05-20T15:25:10.156+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://f39758f400c5:34871/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1716218709202
[2024-05-20T15:25:10.156+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://f39758f400c5:34871/files/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1716218709202
[2024-05-20T15:25:10.156+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
[2024-05-20T15:25:10.164+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar at spark://f39758f400c5:34871/files/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1716218709202
[2024-05-20T15:25:10.165+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2024-05-20T15:25:10.174+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.5.1.jar at spark://f39758f400c5:34871/files/org.apache.kafka_kafka-clients-3.5.1.jar with timestamp 1716218709202
[2024-05-20T15:25:10.175+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.5.1.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/org.apache.kafka_kafka-clients-3.5.1.jar
[2024-05-20T15:25:10.185+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.2.jar at spark://f39758f400c5:34871/files/commons-logging_commons-logging-1.2.jar with timestamp 1716218709202
[2024-05-20T15:25:10.185+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/commons-logging_commons-logging-1.2.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/commons-logging_commons-logging-1.2.jar
[2024-05-20T15:25:10.190+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://f39758f400c5:34871/files/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1716218709202
[2024-05-20T15:25:10.190+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
[2024-05-20T15:25:10.199+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://f39758f400c5:34871/files/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1716218709202
[2024-05-20T15:25:10.200+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/org.apache.commons_commons-pool2-2.11.1.jar
[2024-05-20T15:25:10.214+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://f39758f400c5:34871/files/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1716218709202
[2024-05-20T15:25:10.215+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2024-05-20T15:25:10.245+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://f39758f400c5:34871/files/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1716218709202
[2024-05-20T15:25:10.247+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2024-05-20T15:25:10.283+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://f39758f400c5:34871/files/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1716218709202
[2024-05-20T15:25:10.285+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2024-05-20T15:25:10.315+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://f39758f400c5:34871/files/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1716218709202
[2024-05-20T15:25:10.315+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/org.slf4j_slf4j-api-2.0.7.jar
[2024-05-20T15:25:10.323+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar at spark://f39758f400c5:34871/files/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1716218709202
[2024-05-20T15:25:10.323+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2024-05-20T15:25:10.327+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://f39758f400c5:34871/files/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1716218709202
[2024-05-20T15:25:10.327+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2024-05-20T15:25:10.332+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://f39758f400c5:34871/files/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1716218709202
[2024-05-20T15:25:10.332+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2024-05-20T15:25:10.348+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://f39758f400c5:34871/files/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1716218709202
[2024-05-20T15:25:10.352+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2024-05-20T15:25:10.363+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://f39758f400c5:34871/files/org.apache.commons_commons-lang3-3.10.jar with timestamp 1716218709202
[2024-05-20T15:25:10.364+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/org.apache.commons_commons-lang3-3.10.jar
[2024-05-20T15:25:10.372+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://f39758f400c5:34871/files/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1716218709202
[2024-05-20T15:25:10.373+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/com.thoughtworks.paranamer_paranamer-2.8.jar
[2024-05-20T15:25:10.383+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://f39758f400c5:34871/files/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1716218709202
[2024-05-20T15:25:10.384+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/org.scala-lang_scala-reflect-2.12.11.jar
[2024-05-20T15:25:10.392+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://f39758f400c5:34871/files/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1716218709202
[2024-05-20T15:25:10.393+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/com.datastax.oss_native-protocol-1.5.0.jar
[2024-05-20T15:25:10.396+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://f39758f400c5:34871/files/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1716218709202
[2024-05-20T15:25:10.397+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2024-05-20T15:25:10.404+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://f39758f400c5:34871/files/com.typesafe_config-1.4.1.jar with timestamp 1716218709202
[2024-05-20T15:25:10.405+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/com.typesafe_config-1.4.1.jar
[2024-05-20T15:25:10.412+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://f39758f400c5:34871/files/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1716218709202
[2024-05-20T15:25:10.412+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2024-05-20T15:25:10.419+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://f39758f400c5:34871/files/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1716218709202
[2024-05-20T15:25:10.419+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2024-05-20T15:25:10.429+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://f39758f400c5:34871/files/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1716218709202
[2024-05-20T15:25:10.429+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/org.reactivestreams_reactive-streams-1.0.3.jar
[2024-05-20T15:25:10.446+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://f39758f400c5:34871/files/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1716218709202
[2024-05-20T15:25:10.446+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2024-05-20T15:25:10.468+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://f39758f400c5:34871/files/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1716218709202
[2024-05-20T15:25:10.468+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2024-05-20T15:25:10.482+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://f39758f400c5:34871/files/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1716218709202
[2024-05-20T15:25:10.483+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/com.google.code.findbugs_jsr305-3.0.2.jar
[2024-05-20T15:25:10.486+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://f39758f400c5:34871/files/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1716218709202
[2024-05-20T15:25:10.486+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2024-05-20T15:25:10.496+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.luben_zstd-jni-1.5.5-1.jar at spark://f39758f400c5:34871/files/com.github.luben_zstd-jni-1.5.5-1.jar with timestamp 1716218709202
[2024-05-20T15:25:10.496+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/com.github.luben_zstd-jni-1.5.5-1.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/com.github.luben_zstd-jni-1.5.5-1.jar
[2024-05-20T15:25:10.505+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://f39758f400c5:34871/files/org.lz4_lz4-java-1.8.0.jar with timestamp 1716218709202
[2024-05-20T15:25:10.505+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO Utils: Copying /home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-5d4091e8-1cbe-48f4-a60b-f6b2b23917b0/userFiles-e76cfa3e-8e5f-4d5a-9168-a14f9df73b50/org.lz4_lz4-java-1.8.0.jar
[2024-05-20T15:25:10.783+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2024-05-20T15:25:10.817+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:10 INFO TransportClientFactory: Successfully created connection to spark-master/172.25.0.3:7077 after 21 ms (0 ms spent in bootstraps)
[2024-05-20T15:25:11.609+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:11 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240520152511-0000
[2024-05-20T15:25:11.627+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39575.
[2024-05-20T15:25:11.627+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:11 INFO NettyBlockTransferService: Server created on f39758f400c5:39575
[2024-05-20T15:25:11.629+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-05-20T15:25:11.636+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f39758f400c5, 39575, None)
[2024-05-20T15:25:11.639+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:11 INFO BlockManagerMasterEndpoint: Registering block manager f39758f400c5:39575 with 434.4 MiB RAM, BlockManagerId(driver, f39758f400c5, 39575, None)
[2024-05-20T15:25:11.641+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f39758f400c5, 39575, None)
[2024-05-20T15:25:11.644+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f39758f400c5, 39575, None)
[2024-05-20T15:25:11.664+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:11 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240520152511-0000/0 on worker-20240520151450-172.25.0.7-33931 (172.25.0.7:33931) with 1 core(s)
[2024-05-20T15:25:11.665+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:11 INFO StandaloneSchedulerBackend: Granted executor ID app-20240520152511-0000/0 on hostPort 172.25.0.7:33931 with 1 core(s), 1024.0 MiB RAM
[2024-05-20T15:25:11.905+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:11 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-05-20T15:25:12.556+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:12 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-05-20T15:25:12.561+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:12 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2024-05-20T15:25:13.393+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:13 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240520152511-0000/0 is now RUNNING
[2024-05-20T15:25:16.879+0000] {spark_submit.py:571} INFO - INFO:py4j.java_gateway:Callback Server Starting
[2024-05-20T15:25:16.896+0000] {spark_submit.py:571} INFO - INFO:py4j.java_gateway:Socket listening on ('127.0.0.1', 35191)
[2024-05-20T15:25:17.017+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:17 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2024-05-20T15:25:17.253+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:17 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
[2024-05-20T15:25:17.361+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:17 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694 resolved to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694.
[2024-05-20T15:25:17.362+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:17 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2024-05-20T15:25:17.627+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/metadata using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/.metadata.438a781c-abf8-4d25-9e24-524d95c41692.tmp
[2024-05-20T15:25:18.070+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/.metadata.438a781c-abf8-4d25-9e24-524d95c41692.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/metadata
[2024-05-20T15:25:18.164+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:18 INFO MicroBatchExecution: Starting [id = 5f42858b-b652-4f45-a8c9-eeb4fd889226, runId = 9cc76651-4fe1-43e8-a73c-096daec0f336]. Use file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694 to store the query checkpoint.
[2024-05-20T15:25:18.178+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:18 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3f51b748] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@5366e9cc]
[2024-05-20T15:25:18.224+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:18 INFO OffsetSeqLog: BatchIds found from listing:
[2024-05-20T15:25:18.225+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:18 INFO OffsetSeqLog: BatchIds found from listing:
[2024-05-20T15:25:18.225+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:18 INFO MicroBatchExecution: Starting new streaming query.
[2024-05-20T15:25:18.227+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:18 INFO MicroBatchExecution: Stream started from {}
[2024-05-20T15:25:18.709+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:18 INFO AdminClientConfig: AdminClientConfig values:
[2024-05-20T15:25:18.714+0000] {spark_submit.py:571} INFO - auto.include.jmx.reporter = true
[2024-05-20T15:25:18.715+0000] {spark_submit.py:571} INFO - bootstrap.servers = [kafka:9092]
[2024-05-20T15:25:18.716+0000] {spark_submit.py:571} INFO - client.dns.lookup = use_all_dns_ips
[2024-05-20T15:25:18.717+0000] {spark_submit.py:571} INFO - client.id =
[2024-05-20T15:25:18.717+0000] {spark_submit.py:571} INFO - connections.max.idle.ms = 300000
[2024-05-20T15:25:18.718+0000] {spark_submit.py:571} INFO - default.api.timeout.ms = 60000
[2024-05-20T15:25:18.718+0000] {spark_submit.py:571} INFO - metadata.max.age.ms = 300000
[2024-05-20T15:25:18.719+0000] {spark_submit.py:571} INFO - metric.reporters = []
[2024-05-20T15:25:18.721+0000] {spark_submit.py:571} INFO - metrics.num.samples = 2
[2024-05-20T15:25:18.721+0000] {spark_submit.py:571} INFO - metrics.recording.level = INFO
[2024-05-20T15:25:18.723+0000] {spark_submit.py:571} INFO - metrics.sample.window.ms = 30000
[2024-05-20T15:25:18.724+0000] {spark_submit.py:571} INFO - receive.buffer.bytes = 65536
[2024-05-20T15:25:18.724+0000] {spark_submit.py:571} INFO - reconnect.backoff.max.ms = 1000
[2024-05-20T15:25:18.724+0000] {spark_submit.py:571} INFO - reconnect.backoff.ms = 50
[2024-05-20T15:25:18.725+0000] {spark_submit.py:571} INFO - request.timeout.ms = 30000
[2024-05-20T15:25:18.725+0000] {spark_submit.py:571} INFO - retries = 2147483647
[2024-05-20T15:25:18.725+0000] {spark_submit.py:571} INFO - retry.backoff.ms = 100
[2024-05-20T15:25:18.725+0000] {spark_submit.py:571} INFO - sasl.client.callback.handler.class = null
[2024-05-20T15:25:18.725+0000] {spark_submit.py:571} INFO - sasl.jaas.config = null
[2024-05-20T15:25:18.726+0000] {spark_submit.py:571} INFO - sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2024-05-20T15:25:18.726+0000] {spark_submit.py:571} INFO - sasl.kerberos.min.time.before.relogin = 60000
[2024-05-20T15:25:18.726+0000] {spark_submit.py:571} INFO - sasl.kerberos.service.name = null
[2024-05-20T15:25:18.726+0000] {spark_submit.py:571} INFO - sasl.kerberos.ticket.renew.jitter = 0.05
[2024-05-20T15:25:18.726+0000] {spark_submit.py:571} INFO - sasl.kerberos.ticket.renew.window.factor = 0.8
[2024-05-20T15:25:18.726+0000] {spark_submit.py:571} INFO - sasl.login.callback.handler.class = null
[2024-05-20T15:25:18.727+0000] {spark_submit.py:571} INFO - sasl.login.class = null
[2024-05-20T15:25:18.727+0000] {spark_submit.py:571} INFO - sasl.login.connect.timeout.ms = null
[2024-05-20T15:25:18.727+0000] {spark_submit.py:571} INFO - sasl.login.read.timeout.ms = null
[2024-05-20T15:25:18.727+0000] {spark_submit.py:571} INFO - sasl.login.refresh.buffer.seconds = 300
[2024-05-20T15:25:18.728+0000] {spark_submit.py:571} INFO - sasl.login.refresh.min.period.seconds = 60
[2024-05-20T15:25:18.728+0000] {spark_submit.py:571} INFO - sasl.login.refresh.window.factor = 0.8
[2024-05-20T15:25:18.728+0000] {spark_submit.py:571} INFO - sasl.login.refresh.window.jitter = 0.05
[2024-05-20T15:25:18.728+0000] {spark_submit.py:571} INFO - sasl.login.retry.backoff.max.ms = 10000
[2024-05-20T15:25:18.729+0000] {spark_submit.py:571} INFO - sasl.login.retry.backoff.ms = 100
[2024-05-20T15:25:18.729+0000] {spark_submit.py:571} INFO - sasl.mechanism = GSSAPI
[2024-05-20T15:25:18.729+0000] {spark_submit.py:571} INFO - sasl.oauthbearer.clock.skew.seconds = 30
[2024-05-20T15:25:18.730+0000] {spark_submit.py:571} INFO - sasl.oauthbearer.expected.audience = null
[2024-05-20T15:25:18.730+0000] {spark_submit.py:571} INFO - sasl.oauthbearer.expected.issuer = null
[2024-05-20T15:25:18.730+0000] {spark_submit.py:571} INFO - sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2024-05-20T15:25:18.731+0000] {spark_submit.py:571} INFO - sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2024-05-20T15:25:18.731+0000] {spark_submit.py:571} INFO - sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2024-05-20T15:25:18.735+0000] {spark_submit.py:571} INFO - sasl.oauthbearer.jwks.endpoint.url = null
[2024-05-20T15:25:18.736+0000] {spark_submit.py:571} INFO - sasl.oauthbearer.scope.claim.name = scope
[2024-05-20T15:25:18.736+0000] {spark_submit.py:571} INFO - sasl.oauthbearer.sub.claim.name = sub
[2024-05-20T15:25:18.737+0000] {spark_submit.py:571} INFO - sasl.oauthbearer.token.endpoint.url = null
[2024-05-20T15:25:18.737+0000] {spark_submit.py:571} INFO - security.protocol = PLAINTEXT
[2024-05-20T15:25:18.743+0000] {spark_submit.py:571} INFO - security.providers = null
[2024-05-20T15:25:18.744+0000] {spark_submit.py:571} INFO - send.buffer.bytes = 131072
[2024-05-20T15:25:18.745+0000] {spark_submit.py:571} INFO - socket.connection.setup.timeout.max.ms = 30000
[2024-05-20T15:25:18.745+0000] {spark_submit.py:571} INFO - socket.connection.setup.timeout.ms = 10000
[2024-05-20T15:25:18.746+0000] {spark_submit.py:571} INFO - ssl.cipher.suites = null
[2024-05-20T15:25:18.746+0000] {spark_submit.py:571} INFO - ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2024-05-20T15:25:18.747+0000] {spark_submit.py:571} INFO - ssl.endpoint.identification.algorithm = https
[2024-05-20T15:25:18.749+0000] {spark_submit.py:571} INFO - ssl.engine.factory.class = null
[2024-05-20T15:25:18.750+0000] {spark_submit.py:571} INFO - ssl.key.password = null
[2024-05-20T15:25:18.751+0000] {spark_submit.py:571} INFO - ssl.keymanager.algorithm = SunX509
[2024-05-20T15:25:18.752+0000] {spark_submit.py:571} INFO - ssl.keystore.certificate.chain = null
[2024-05-20T15:25:18.753+0000] {spark_submit.py:571} INFO - ssl.keystore.key = null
[2024-05-20T15:25:18.753+0000] {spark_submit.py:571} INFO - ssl.keystore.location = null
[2024-05-20T15:25:18.754+0000] {spark_submit.py:571} INFO - ssl.keystore.password = null
[2024-05-20T15:25:18.754+0000] {spark_submit.py:571} INFO - ssl.keystore.type = JKS
[2024-05-20T15:25:18.755+0000] {spark_submit.py:571} INFO - ssl.protocol = TLSv1.3
[2024-05-20T15:25:18.756+0000] {spark_submit.py:571} INFO - ssl.provider = null
[2024-05-20T15:25:18.757+0000] {spark_submit.py:571} INFO - ssl.secure.random.implementation = null
[2024-05-20T15:25:18.758+0000] {spark_submit.py:571} INFO - ssl.trustmanager.algorithm = PKIX
[2024-05-20T15:25:18.758+0000] {spark_submit.py:571} INFO - ssl.truststore.certificates = null
[2024-05-20T15:25:18.759+0000] {spark_submit.py:571} INFO - ssl.truststore.location = null
[2024-05-20T15:25:18.759+0000] {spark_submit.py:571} INFO - ssl.truststore.password = null
[2024-05-20T15:25:18.760+0000] {spark_submit.py:571} INFO - ssl.truststore.type = JKS
[2024-05-20T15:25:18.761+0000] {spark_submit.py:571} INFO - 
[2024-05-20T15:25:18.800+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:18 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2024-05-20T15:25:18.803+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:18 INFO AppInfoParser: Kafka version: 3.5.1
[2024-05-20T15:25:18.803+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:18 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2024-05-20T15:25:18.804+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:18 INFO AppInfoParser: Kafka startTimeMs: 1716218718800
[2024-05-20T15:25:19.332+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/sources/0/0 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/sources/0/.0.edc0cbf2-ac22-48d3-9729-a543311b21c9.tmp
[2024-05-20T15:25:19.451+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:19 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.25.0.7:37798) with ID 0,  ResourceProfileId 0
[2024-05-20T15:25:19.452+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/sources/0/.0.edc0cbf2-ac22-48d3-9729-a543311b21c9.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/sources/0/0
[2024-05-20T15:25:19.453+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:19 INFO KafkaMicroBatchStream: Initial offsets: {"redfin_properties":{"0":0}}
[2024-05-20T15:25:19.503+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/0 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.0.5a1c393c-8bb2-4c27-ae4f-2f11670ae3cd.tmp
[2024-05-20T15:25:19.543+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:19 INFO BlockManagerMasterEndpoint: Registering block manager 172.25.0.7:44267 with 434.4 MiB RAM, BlockManagerId(0, 172.25.0.7, 44267, None)
[2024-05-20T15:25:19.544+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.0.5a1c393c-8bb2-4c27-ae4f-2f11670ae3cd.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/0
[2024-05-20T15:25:19.546+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:19 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1716218719484,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-20T15:25:20.851+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:25:20.974+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:25:21.049+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:25:21.052+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:25:22.099+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:22 INFO CodeGenerator: Code generated in 419.730958 ms
[2024-05-20T15:25:22.266+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Python Server ready to receive messages
[2024-05-20T15:25:22.266+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-20T15:25:22.345+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:22 INFO CodeGenerator: Code generated in 37.484208 ms
[2024-05-20T15:25:23.085+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:23 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-20T15:25:23.108+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:23 INFO DAGScheduler: Job 0 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.012203 s
[2024-05-20T15:25:23.253+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/0 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.0.66fe6b68-e7b5-44a2-9905-7b7a6bd1d611.tmp
[2024-05-20T15:25:23.348+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.0.66fe6b68-e7b5-44a2-9905-7b7a6bd1d611.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/0
[2024-05-20T15:25:23.512+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:23 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-20T15:25:23.513+0000] {spark_submit.py:571} INFO - "id" : "5f42858b-b652-4f45-a8c9-eeb4fd889226",
[2024-05-20T15:25:23.513+0000] {spark_submit.py:571} INFO - "runId" : "9cc76651-4fe1-43e8-a73c-096daec0f336",
[2024-05-20T15:25:23.514+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-20T15:25:23.514+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-20T15:25:18.218Z",
[2024-05-20T15:25:23.514+0000] {spark_submit.py:571} INFO - "batchId" : 0,
[2024-05-20T15:25:23.516+0000] {spark_submit.py:571} INFO - "numInputRows" : 0,
[2024-05-20T15:25:23.517+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 0.0,
[2024-05-20T15:25:23.518+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.0,
[2024-05-20T15:25:23.518+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-20T15:25:23.519+0000] {spark_submit.py:571} INFO - "addBatch" : 2165,
[2024-05-20T15:25:23.519+0000] {spark_submit.py:571} INFO - "commitOffsets" : 171,
[2024-05-20T15:25:23.519+0000] {spark_submit.py:571} INFO - "getBatch" : 25,
[2024-05-20T15:25:23.520+0000] {spark_submit.py:571} INFO - "latestOffset" : 1252,
[2024-05-20T15:25:23.520+0000] {spark_submit.py:571} INFO - "queryPlanning" : 1424,
[2024-05-20T15:25:23.520+0000] {spark_submit.py:571} INFO - "triggerExecution" : 5133,
[2024-05-20T15:25:23.520+0000] {spark_submit.py:571} INFO - "walCommit" : 56
[2024-05-20T15:25:23.525+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:25:23.526+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-20T15:25:23.527+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-20T15:25:23.528+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-20T15:25:23.529+0000] {spark_submit.py:571} INFO - "startOffset" : null,
[2024-05-20T15:25:23.531+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-20T15:25:23.532+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:25:23.533+0000] {spark_submit.py:571} INFO - "0" : 0
[2024-05-20T15:25:23.534+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:25:23.535+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:25:23.536+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-20T15:25:23.537+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:25:23.539+0000] {spark_submit.py:571} INFO - "0" : 0
[2024-05-20T15:25:23.541+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:25:23.542+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:25:23.543+0000] {spark_submit.py:571} INFO - "numInputRows" : 0,
[2024-05-20T15:25:23.544+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 0.0,
[2024-05-20T15:25:23.546+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.0,
[2024-05-20T15:25:23.547+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-20T15:25:23.549+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-20T15:25:23.551+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-20T15:25:23.552+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-20T15:25:23.553+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:25:23.554+0000] {spark_submit.py:571} INFO - } ],
[2024-05-20T15:25:23.555+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-20T15:25:23.556+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-20T15:25:23.557+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-20T15:25:23.561+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:25:23.562+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:25:33.478+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:25:43.486+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:25:44.913+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/1 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.1.29440f15-4e40-49a2-852f-e53f77ddbb9a.tmp
[2024-05-20T15:25:44.949+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.1.29440f15-4e40-49a2-852f-e53f77ddbb9a.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/1
[2024-05-20T15:25:44.949+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:44 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1716218744885,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-20T15:25:45.031+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:25:45.036+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:25:45.062+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:25:45.064+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:25:45.096+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-20T15:25:45.169+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:45 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-20T15:25:45.181+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:45 INFO DAGScheduler: Got job 1 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-20T15:25:45.182+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:45 INFO DAGScheduler: Final stage: ResultStage 0 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-20T15:25:45.183+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:45 INFO DAGScheduler: Parents of final stage: List()
[2024-05-20T15:25:45.183+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:45 INFO DAGScheduler: Missing parents: List()
[2024-05-20T15:25:45.186+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:45 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[19] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-20T15:25:45.263+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-20T15:25:45.321+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 434.3 MiB)
[2024-05-20T15:25:45.329+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f39758f400c5:39575 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:25:45.330+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:45 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
[2024-05-20T15:25:45.374+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[19] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-20T15:25:45.375+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-05-20T15:25:45.406+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.25.0.7, executor 0, partition 0, PROCESS_LOCAL, 14270 bytes)
[2024-05-20T15:25:45.839+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.25.0.7:44267 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:25:50.800+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5400 ms on 172.25.0.7 (executor 0) (1/1)
[2024-05-20T15:25:50.808+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-05-20T15:25:50.823+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:50 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 59351
[2024-05-20T15:25:50.829+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:50 INFO DAGScheduler: ResultStage 0 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 5.630 s
[2024-05-20T15:25:50.833+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:50 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-20T15:25:50.834+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-05-20T15:25:50.836+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:50 INFO DAGScheduler: Job 1 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 5.666635 s
[2024-05-20T15:25:50.871+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/1 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.1.f9f0e42c-814a-4cbc-b209-690e01566895.tmp
[2024-05-20T15:25:50.923+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.1.f9f0e42c-814a-4cbc-b209-690e01566895.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/1
[2024-05-20T15:25:50.927+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:50 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-20T15:25:50.927+0000] {spark_submit.py:571} INFO - "id" : "5f42858b-b652-4f45-a8c9-eeb4fd889226",
[2024-05-20T15:25:50.928+0000] {spark_submit.py:571} INFO - "runId" : "9cc76651-4fe1-43e8-a73c-096daec0f336",
[2024-05-20T15:25:50.928+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-20T15:25:50.929+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-20T15:25:44.881Z",
[2024-05-20T15:25:50.929+0000] {spark_submit.py:571} INFO - "batchId" : 1,
[2024-05-20T15:25:50.929+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:25:50.929+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-05-20T15:25:50.929+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.165508109897385,
[2024-05-20T15:25:50.929+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-20T15:25:50.930+0000] {spark_submit.py:571} INFO - "addBatch" : 5805,
[2024-05-20T15:25:50.930+0000] {spark_submit.py:571} INFO - "commitOffsets" : 74,
[2024-05-20T15:25:50.930+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-20T15:25:50.930+0000] {spark_submit.py:571} INFO - "latestOffset" : 4,
[2024-05-20T15:25:50.930+0000] {spark_submit.py:571} INFO - "queryPlanning" : 83,
[2024-05-20T15:25:50.930+0000] {spark_submit.py:571} INFO - "triggerExecution" : 6042,
[2024-05-20T15:25:50.931+0000] {spark_submit.py:571} INFO - "walCommit" : 68
[2024-05-20T15:25:50.931+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:25:50.931+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-20T15:25:50.931+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-20T15:25:50.931+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-20T15:25:50.931+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-20T15:25:50.931+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:25:50.932+0000] {spark_submit.py:571} INFO - "0" : 0
[2024-05-20T15:25:50.932+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:25:50.932+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:25:50.932+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-20T15:25:50.932+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:25:50.932+0000] {spark_submit.py:571} INFO - "0" : 1
[2024-05-20T15:25:50.933+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:25:50.933+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:25:50.933+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-20T15:25:50.933+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:25:50.934+0000] {spark_submit.py:571} INFO - "0" : 1
[2024-05-20T15:25:50.934+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:25:50.934+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:25:50.934+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:25:50.935+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-05-20T15:25:50.935+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.165508109897385,
[2024-05-20T15:25:50.935+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-20T15:25:50.935+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-20T15:25:50.936+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-20T15:25:50.936+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-20T15:25:50.936+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:25:50.936+0000] {spark_submit.py:571} INFO - } ],
[2024-05-20T15:25:50.936+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-20T15:25:50.936+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-20T15:25:50.936+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-20T15:25:50.937+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:25:50.937+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:25:59.217+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:59 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f39758f400c5:39575 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:25:59.317+0000] {spark_submit.py:571} INFO - 24/05/20 15:25:59 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.25.0.7:44267 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:26:00.931+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:26:10.933+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:26:15.040+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/2 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.2.06973b7c-877f-4d42-a2e6-f2a97763789d.tmp
[2024-05-20T15:26:15.133+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.2.06973b7c-877f-4d42-a2e6-f2a97763789d.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/2
[2024-05-20T15:26:15.133+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:15 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1716218774883,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-20T15:26:15.216+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:26:15.227+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:26:15.254+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:26:15.258+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:26:15.310+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-20T15:26:15.375+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:15 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-20T15:26:15.377+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:15 INFO DAGScheduler: Got job 2 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-20T15:26:15.377+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:15 INFO DAGScheduler: Final stage: ResultStage 1 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-20T15:26:15.377+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:15 INFO DAGScheduler: Parents of final stage: List()
[2024-05-20T15:26:15.377+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:15 INFO DAGScheduler: Missing parents: List()
[2024-05-20T15:26:15.378+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:15 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[29] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-20T15:26:15.382+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-20T15:26:15.399+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 434.3 MiB)
[2024-05-20T15:26:15.400+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f39758f400c5:39575 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:26:15.401+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2024-05-20T15:26:15.403+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[29] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-20T15:26:15.405+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:15 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-05-20T15:26:15.411+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:15 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.25.0.7, executor 0, partition 0, PROCESS_LOCAL, 14270 bytes)
[2024-05-20T15:26:15.513+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.25.0.7:44267 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:26:16.504+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1093 ms on 172.25.0.7 (executor 0) (1/1)
[2024-05-20T15:26:16.511+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-05-20T15:26:16.512+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:16 INFO DAGScheduler: ResultStage 1 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 1.127 s
[2024-05-20T15:26:16.513+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:16 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-20T15:26:16.514+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-05-20T15:26:16.514+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:16 INFO DAGScheduler: Job 2 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 1.135236 s
[2024-05-20T15:26:16.545+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/2 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.2.b843fd47-1ef8-4fdf-84be-36285cfe60e9.tmp
[2024-05-20T15:26:16.596+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.2.b843fd47-1ef8-4fdf-84be-36285cfe60e9.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/2
[2024-05-20T15:26:16.600+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:16 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-20T15:26:16.600+0000] {spark_submit.py:571} INFO - "id" : "5f42858b-b652-4f45-a8c9-eeb4fd889226",
[2024-05-20T15:26:16.600+0000] {spark_submit.py:571} INFO - "runId" : "9cc76651-4fe1-43e8-a73c-096daec0f336",
[2024-05-20T15:26:16.601+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-20T15:26:16.601+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-20T15:26:14.879Z",
[2024-05-20T15:26:16.601+0000] {spark_submit.py:571} INFO - "batchId" : 2,
[2024-05-20T15:26:16.601+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:26:16.601+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-05-20T15:26:16.601+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.5824111822947,
[2024-05-20T15:26:16.602+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-20T15:26:16.602+0000] {spark_submit.py:571} INFO - "addBatch" : 1284,
[2024-05-20T15:26:16.602+0000] {spark_submit.py:571} INFO - "commitOffsets" : 76,
[2024-05-20T15:26:16.602+0000] {spark_submit.py:571} INFO - "getBatch" : 2,
[2024-05-20T15:26:16.602+0000] {spark_submit.py:571} INFO - "latestOffset" : 4,
[2024-05-20T15:26:16.602+0000] {spark_submit.py:571} INFO - "queryPlanning" : 82,
[2024-05-20T15:26:16.602+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1717,
[2024-05-20T15:26:16.603+0000] {spark_submit.py:571} INFO - "walCommit" : 254
[2024-05-20T15:26:16.603+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:26:16.603+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-20T15:26:16.603+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-20T15:26:16.603+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-20T15:26:16.603+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-20T15:26:16.603+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:26:16.603+0000] {spark_submit.py:571} INFO - "0" : 1
[2024-05-20T15:26:16.604+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:26:16.604+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:26:16.604+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-20T15:26:16.604+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:26:16.604+0000] {spark_submit.py:571} INFO - "0" : 2
[2024-05-20T15:26:16.604+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:26:16.604+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:26:16.605+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-20T15:26:16.605+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:26:16.605+0000] {spark_submit.py:571} INFO - "0" : 2
[2024-05-20T15:26:16.605+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:26:16.605+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:26:16.605+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:26:16.605+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-05-20T15:26:16.606+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.5824111822947,
[2024-05-20T15:26:16.606+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-20T15:26:16.606+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-20T15:26:16.606+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-20T15:26:16.606+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-20T15:26:16.606+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:26:16.606+0000] {spark_submit.py:571} INFO - } ],
[2024-05-20T15:26:16.606+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-20T15:26:16.606+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-20T15:26:16.607+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-20T15:26:16.607+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:26:16.607+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:26:26.609+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:26:33.176+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:33 INFO BlockManagerInfo: Removed broadcast_1_piece0 on f39758f400c5:39575 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:26:33.207+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:33 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.25.0.7:44267 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:26:36.611+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:26:43.949+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/3 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.3.36e2f427-1506-42c2-bd62-4552b61e9a2f.tmp
[2024-05-20T15:26:43.999+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.3.36e2f427-1506-42c2-bd62-4552b61e9a2f.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/3
[2024-05-20T15:26:44.002+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:44 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1716218803899,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-20T15:26:44.137+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:26:44.144+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:26:44.197+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:26:44.201+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:26:44.292+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-20T15:26:44.380+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:44 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-20T15:26:44.388+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:44 INFO DAGScheduler: Got job 3 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-20T15:26:44.390+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:44 INFO DAGScheduler: Final stage: ResultStage 2 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-20T15:26:44.391+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:44 INFO DAGScheduler: Parents of final stage: List()
[2024-05-20T15:26:44.391+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:44 INFO DAGScheduler: Missing parents: List()
[2024-05-20T15:26:44.392+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:44 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[39] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-20T15:26:44.396+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:44 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-20T15:26:44.399+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:44 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 434.3 MiB)
[2024-05-20T15:26:44.399+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:44 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f39758f400c5:39575 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:26:44.400+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:44 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
[2024-05-20T15:26:44.401+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (PythonRDD[39] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-20T15:26:44.402+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:44 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-05-20T15:26:44.403+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:44 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.25.0.7, executor 0, partition 0, PROCESS_LOCAL, 14270 bytes)
[2024-05-20T15:26:44.485+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:44 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.25.0.7:44267 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:26:45.292+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:45 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 886 ms on 172.25.0.7 (executor 0) (1/1)
[2024-05-20T15:26:45.297+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:45 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-05-20T15:26:45.299+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:45 INFO DAGScheduler: ResultStage 2 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.901 s
[2024-05-20T15:26:45.299+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:45 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-20T15:26:45.300+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-05-20T15:26:45.300+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:45 INFO DAGScheduler: Job 3 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.915061 s
[2024-05-20T15:26:45.324+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/3 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.3.3cde094c-5cdc-4a30-8386-8aa7e6e6aa9a.tmp
[2024-05-20T15:26:45.369+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.3.3cde094c-5cdc-4a30-8386-8aa7e6e6aa9a.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/3
[2024-05-20T15:26:45.373+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:45 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-20T15:26:45.373+0000] {spark_submit.py:571} INFO - "id" : "5f42858b-b652-4f45-a8c9-eeb4fd889226",
[2024-05-20T15:26:45.373+0000] {spark_submit.py:571} INFO - "runId" : "9cc76651-4fe1-43e8-a73c-096daec0f336",
[2024-05-20T15:26:45.374+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-20T15:26:45.374+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-20T15:26:43.897Z",
[2024-05-20T15:26:45.374+0000] {spark_submit.py:571} INFO - "batchId" : 3,
[2024-05-20T15:26:45.374+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:26:45.374+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-20T15:26:45.375+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6793478260869565,
[2024-05-20T15:26:45.375+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-20T15:26:45.375+0000] {spark_submit.py:571} INFO - "addBatch" : 1148,
[2024-05-20T15:26:45.375+0000] {spark_submit.py:571} INFO - "commitOffsets" : 65,
[2024-05-20T15:26:45.376+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-20T15:26:45.376+0000] {spark_submit.py:571} INFO - "latestOffset" : 2,
[2024-05-20T15:26:45.376+0000] {spark_submit.py:571} INFO - "queryPlanning" : 144,
[2024-05-20T15:26:45.376+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1472,
[2024-05-20T15:26:45.376+0000] {spark_submit.py:571} INFO - "walCommit" : 102
[2024-05-20T15:26:45.377+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:26:45.377+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-20T15:26:45.377+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-20T15:26:45.377+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-20T15:26:45.377+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-20T15:26:45.377+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:26:45.377+0000] {spark_submit.py:571} INFO - "0" : 2
[2024-05-20T15:26:45.377+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:26:45.378+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:26:45.378+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-20T15:26:45.378+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:26:45.378+0000] {spark_submit.py:571} INFO - "0" : 3
[2024-05-20T15:26:45.378+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:26:45.378+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:26:45.378+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-20T15:26:45.378+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:26:45.379+0000] {spark_submit.py:571} INFO - "0" : 3
[2024-05-20T15:26:45.379+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:26:45.379+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:26:45.379+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:26:45.379+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-20T15:26:45.379+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6793478260869565,
[2024-05-20T15:26:45.379+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-20T15:26:45.379+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-20T15:26:45.380+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-20T15:26:45.380+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-20T15:26:45.380+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:26:45.380+0000] {spark_submit.py:571} INFO - } ],
[2024-05-20T15:26:45.380+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-20T15:26:45.380+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-20T15:26:45.380+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-20T15:26:45.380+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:26:45.380+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:26:46.542+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:46 INFO BlockManagerInfo: Removed broadcast_2_piece0 on f39758f400c5:39575 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:26:46.577+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:46 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.25.0.7:44267 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:26:55.382+0000] {spark_submit.py:571} INFO - 24/05/20 15:26:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:27:05.385+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:27:08.227+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/4 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.4.af66431d-94e7-4ac6-beb1-3930302b4d9b.tmp
[2024-05-20T15:27:08.274+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.4.af66431d-94e7-4ac6-beb1-3930302b4d9b.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/4
[2024-05-20T15:27:08.276+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:08 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1716218828194,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-20T15:27:08.346+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:27:08.354+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:27:08.397+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:27:08.398+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:27:08.423+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-20T15:27:08.458+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:08 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-20T15:27:08.464+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:08 INFO DAGScheduler: Got job 4 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-20T15:27:08.465+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:08 INFO DAGScheduler: Final stage: ResultStage 3 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-20T15:27:08.466+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:08 INFO DAGScheduler: Parents of final stage: List()
[2024-05-20T15:27:08.466+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:08 INFO DAGScheduler: Missing parents: List()
[2024-05-20T15:27:08.468+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:08 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[49] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-20T15:27:08.470+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:08 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-20T15:27:08.473+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:08 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 434.3 MiB)
[2024-05-20T15:27:08.474+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:08 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on f39758f400c5:39575 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:27:08.474+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:08 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2024-05-20T15:27:08.475+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (PythonRDD[49] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-20T15:27:08.479+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:08 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2024-05-20T15:27:08.480+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:08 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (172.25.0.7, executor 0, partition 0, PROCESS_LOCAL, 14270 bytes)
[2024-05-20T15:27:08.540+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:08 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.25.0.7:44267 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:27:09.312+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:09 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 834 ms on 172.25.0.7 (executor 0) (1/1)
[2024-05-20T15:27:09.318+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:09 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2024-05-20T15:27:09.319+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:09 INFO DAGScheduler: ResultStage 3 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.847 s
[2024-05-20T15:27:09.321+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:09 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-20T15:27:09.322+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2024-05-20T15:27:09.323+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:09 INFO DAGScheduler: Job 4 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.857733 s
[2024-05-20T15:27:09.384+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/4 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.4.68a923e7-9065-4ec1-9b03-6e30aae9547c.tmp
[2024-05-20T15:27:09.457+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.4.68a923e7-9065-4ec1-9b03-6e30aae9547c.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/4
[2024-05-20T15:27:09.461+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:09 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-20T15:27:09.464+0000] {spark_submit.py:571} INFO - "id" : "5f42858b-b652-4f45-a8c9-eeb4fd889226",
[2024-05-20T15:27:09.464+0000] {spark_submit.py:571} INFO - "runId" : "9cc76651-4fe1-43e8-a73c-096daec0f336",
[2024-05-20T15:27:09.464+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-20T15:27:09.464+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-20T15:27:08.190Z",
[2024-05-20T15:27:09.465+0000] {spark_submit.py:571} INFO - "batchId" : 4,
[2024-05-20T15:27:09.465+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:27:09.465+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 66.66666666666667,
[2024-05-20T15:27:09.465+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7892659826361484,
[2024-05-20T15:27:09.466+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-20T15:27:09.466+0000] {spark_submit.py:571} INFO - "addBatch" : 984,
[2024-05-20T15:27:09.466+0000] {spark_submit.py:571} INFO - "commitOffsets" : 106,
[2024-05-20T15:27:09.466+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-20T15:27:09.466+0000] {spark_submit.py:571} INFO - "latestOffset" : 4,
[2024-05-20T15:27:09.467+0000] {spark_submit.py:571} INFO - "queryPlanning" : 77,
[2024-05-20T15:27:09.467+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1267,
[2024-05-20T15:27:09.467+0000] {spark_submit.py:571} INFO - "walCommit" : 83
[2024-05-20T15:27:09.467+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:27:09.467+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-20T15:27:09.467+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-20T15:27:09.468+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-20T15:27:09.468+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-20T15:27:09.468+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:27:09.468+0000] {spark_submit.py:571} INFO - "0" : 3
[2024-05-20T15:27:09.468+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:27:09.468+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:27:09.469+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-20T15:27:09.469+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:27:09.469+0000] {spark_submit.py:571} INFO - "0" : 4
[2024-05-20T15:27:09.469+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:27:09.469+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:27:09.469+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-20T15:27:09.469+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:27:09.470+0000] {spark_submit.py:571} INFO - "0" : 4
[2024-05-20T15:27:09.470+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:27:09.470+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:27:09.470+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:27:09.470+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 66.66666666666667,
[2024-05-20T15:27:09.470+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7892659826361484,
[2024-05-20T15:27:09.470+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-20T15:27:09.470+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-20T15:27:09.471+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-20T15:27:09.471+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-20T15:27:09.471+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:27:09.471+0000] {spark_submit.py:571} INFO - } ],
[2024-05-20T15:27:09.471+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-20T15:27:09.471+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-20T15:27:09.471+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-20T15:27:09.472+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:27:09.472+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:27:19.473+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:27:19.585+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:19 INFO BlockManagerInfo: Removed broadcast_3_piece0 on f39758f400c5:39575 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:27:19.602+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:19 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.25.0.7:44267 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:27:29.477+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:27:39.472+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:27:49.489+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:27:50.983+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/5 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.5.8bbd7d1d-81cb-4954-9080-af92e5cab0d3.tmp
[2024-05-20T15:27:51.032+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.5.8bbd7d1d-81cb-4954-9080-af92e5cab0d3.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/5
[2024-05-20T15:27:51.033+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:51 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1716218870954,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-20T15:27:51.103+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:27:51.112+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:27:51.152+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:27:51.154+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:27:51.190+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-20T15:27:51.262+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:51 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-20T15:27:51.264+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:51 INFO DAGScheduler: Got job 5 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-20T15:27:51.264+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:51 INFO DAGScheduler: Final stage: ResultStage 4 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-20T15:27:51.265+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:51 INFO DAGScheduler: Parents of final stage: List()
[2024-05-20T15:27:51.266+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:51 INFO DAGScheduler: Missing parents: List()
[2024-05-20T15:27:51.266+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:51 INFO DAGScheduler: Submitting ResultStage 4 (PythonRDD[59] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-20T15:27:51.268+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:51 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-20T15:27:51.271+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:51 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 434.3 MiB)
[2024-05-20T15:27:51.272+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:51 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on f39758f400c5:39575 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:27:51.272+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:51 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
[2024-05-20T15:27:51.273+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (PythonRDD[59] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-20T15:27:51.273+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:51 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2024-05-20T15:27:51.275+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:51 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (172.25.0.7, executor 0, partition 0, PROCESS_LOCAL, 14270 bytes)
[2024-05-20T15:27:51.328+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:51 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.25.0.7:44267 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:27:52.077+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:52 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 802 ms on 172.25.0.7 (executor 0) (1/1)
[2024-05-20T15:27:52.078+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:52 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2024-05-20T15:27:52.081+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:52 INFO DAGScheduler: ResultStage 4 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.814 s
[2024-05-20T15:27:52.089+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:52 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-20T15:27:52.090+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2024-05-20T15:27:52.092+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:52 INFO DAGScheduler: Job 5 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.820034 s
[2024-05-20T15:27:52.139+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/5 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.5.56a6fc50-56a5-49b5-9ff6-a345a0c8e162.tmp
[2024-05-20T15:27:52.178+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.5.56a6fc50-56a5-49b5-9ff6-a345a0c8e162.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/5
[2024-05-20T15:27:52.182+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:52 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-20T15:27:52.182+0000] {spark_submit.py:571} INFO - "id" : "5f42858b-b652-4f45-a8c9-eeb4fd889226",
[2024-05-20T15:27:52.182+0000] {spark_submit.py:571} INFO - "runId" : "9cc76651-4fe1-43e8-a73c-096daec0f336",
[2024-05-20T15:27:52.183+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-20T15:27:52.183+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-20T15:27:50.952Z",
[2024-05-20T15:27:52.183+0000] {spark_submit.py:571} INFO - "batchId" : 5,
[2024-05-20T15:27:52.184+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:27:52.184+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-20T15:27:52.184+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8156606851549756,
[2024-05-20T15:27:52.184+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-20T15:27:52.184+0000] {spark_submit.py:571} INFO - "addBatch" : 977,
[2024-05-20T15:27:52.185+0000] {spark_submit.py:571} INFO - "commitOffsets" : 63,
[2024-05-20T15:27:52.185+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-20T15:27:52.185+0000] {spark_submit.py:571} INFO - "latestOffset" : 2,
[2024-05-20T15:27:52.185+0000] {spark_submit.py:571} INFO - "queryPlanning" : 88,
[2024-05-20T15:27:52.185+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1226,
[2024-05-20T15:27:52.186+0000] {spark_submit.py:571} INFO - "walCommit" : 79
[2024-05-20T15:27:52.186+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:27:52.186+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-20T15:27:52.186+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-20T15:27:52.186+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-20T15:27:52.186+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-20T15:27:52.187+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:27:52.187+0000] {spark_submit.py:571} INFO - "0" : 4
[2024-05-20T15:27:52.187+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:27:52.187+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:27:52.187+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-20T15:27:52.191+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:27:52.192+0000] {spark_submit.py:571} INFO - "0" : 5
[2024-05-20T15:27:52.192+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:27:52.192+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:27:52.192+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-20T15:27:52.192+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:27:52.193+0000] {spark_submit.py:571} INFO - "0" : 5
[2024-05-20T15:27:52.196+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:27:52.197+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:27:52.197+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:27:52.197+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-20T15:27:52.197+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8156606851549756,
[2024-05-20T15:27:52.198+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-20T15:27:52.198+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-20T15:27:52.198+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-20T15:27:52.198+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-20T15:27:52.198+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:27:52.199+0000] {spark_submit.py:571} INFO - } ],
[2024-05-20T15:27:52.199+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-20T15:27:52.199+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-20T15:27:52.199+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-20T15:27:52.199+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:27:52.200+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:27:54.706+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:54 INFO BlockManagerInfo: Removed broadcast_4_piece0 on f39758f400c5:39575 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:27:54.760+0000] {spark_submit.py:571} INFO - 24/05/20 15:27:54 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.25.0.7:44267 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:28:02.192+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:28:12.213+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:28:20.920+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/6 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.6.3e3a3810-990d-47bf-a69c-e19ee626bc6f.tmp
[2024-05-20T15:28:20.985+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.6.3e3a3810-990d-47bf-a69c-e19ee626bc6f.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/6
[2024-05-20T15:28:20.986+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:20 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1716218900841,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-20T15:28:21.097+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:28:21.102+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:28:21.178+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:28:21.180+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:28:21.250+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-20T15:28:21.311+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:21 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-20T15:28:21.312+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:21 INFO DAGScheduler: Got job 6 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-20T15:28:21.315+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:21 INFO DAGScheduler: Final stage: ResultStage 5 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-20T15:28:21.317+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:21 INFO DAGScheduler: Parents of final stage: List()
[2024-05-20T15:28:21.318+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:21 INFO DAGScheduler: Missing parents: List()
[2024-05-20T15:28:21.320+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:21 INFO DAGScheduler: Submitting ResultStage 5 (PythonRDD[69] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-20T15:28:21.321+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:21 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-20T15:28:21.324+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:21 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 434.3 MiB)
[2024-05-20T15:28:21.325+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:21 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on f39758f400c5:39575 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:28:21.327+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:21 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
[2024-05-20T15:28:21.329+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (PythonRDD[69] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-20T15:28:21.334+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:21 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2024-05-20T15:28:21.346+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:21 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (172.25.0.7, executor 0, partition 0, PROCESS_LOCAL, 14270 bytes)
[2024-05-20T15:28:21.412+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:21 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.25.0.7:44267 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:28:22.230+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:22 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 892 ms on 172.25.0.7 (executor 0) (1/1)
[2024-05-20T15:28:22.235+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:22 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2024-05-20T15:28:22.236+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:22 INFO DAGScheduler: ResultStage 5 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.916 s
[2024-05-20T15:28:22.238+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:22 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-20T15:28:22.239+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2024-05-20T15:28:22.239+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:22 INFO DAGScheduler: Job 6 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.923882 s
[2024-05-20T15:28:22.310+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/6 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.6.d81246ee-f4a7-4d94-b074-02a8cd2ca099.tmp
[2024-05-20T15:28:22.372+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.6.d81246ee-f4a7-4d94-b074-02a8cd2ca099.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/6
[2024-05-20T15:28:22.375+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:22 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-20T15:28:22.376+0000] {spark_submit.py:571} INFO - "id" : "5f42858b-b652-4f45-a8c9-eeb4fd889226",
[2024-05-20T15:28:22.377+0000] {spark_submit.py:571} INFO - "runId" : "9cc76651-4fe1-43e8-a73c-096daec0f336",
[2024-05-20T15:28:22.377+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-20T15:28:22.377+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-20T15:28:20.839Z",
[2024-05-20T15:28:22.377+0000] {spark_submit.py:571} INFO - "batchId" : 6,
[2024-05-20T15:28:22.377+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:28:22.377+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-20T15:28:22.378+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6523157208088716,
[2024-05-20T15:28:22.378+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-20T15:28:22.378+0000] {spark_submit.py:571} INFO - "addBatch" : 1117,
[2024-05-20T15:28:22.378+0000] {spark_submit.py:571} INFO - "commitOffsets" : 103,
[2024-05-20T15:28:22.378+0000] {spark_submit.py:571} INFO - "getBatch" : 1,
[2024-05-20T15:28:22.379+0000] {spark_submit.py:571} INFO - "latestOffset" : 2,
[2024-05-20T15:28:22.379+0000] {spark_submit.py:571} INFO - "queryPlanning" : 121,
[2024-05-20T15:28:22.379+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1533,
[2024-05-20T15:28:22.379+0000] {spark_submit.py:571} INFO - "walCommit" : 145
[2024-05-20T15:28:22.379+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:28:22.380+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-20T15:28:22.380+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-20T15:28:22.380+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-20T15:28:22.380+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-20T15:28:22.380+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:28:22.380+0000] {spark_submit.py:571} INFO - "0" : 5
[2024-05-20T15:28:22.380+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:28:22.381+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:28:22.381+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-20T15:28:22.381+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:28:22.381+0000] {spark_submit.py:571} INFO - "0" : 6
[2024-05-20T15:28:22.381+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:28:22.381+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:28:22.382+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-20T15:28:22.382+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:28:22.382+0000] {spark_submit.py:571} INFO - "0" : 6
[2024-05-20T15:28:22.382+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:28:22.382+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:28:22.382+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:28:22.382+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-20T15:28:22.383+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6523157208088716,
[2024-05-20T15:28:22.383+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-20T15:28:22.383+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-20T15:28:22.383+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-20T15:28:22.383+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-20T15:28:22.383+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:28:22.384+0000] {spark_submit.py:571} INFO - } ],
[2024-05-20T15:28:22.384+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-20T15:28:22.384+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-20T15:28:22.384+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-20T15:28:22.385+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:28:22.385+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:28:31.587+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:31 INFO BlockManagerInfo: Removed broadcast_5_piece0 on f39758f400c5:39575 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:28:31.598+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:31 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.25.0.7:44267 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:28:32.385+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:28:42.400+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:28:52.403+0000] {spark_submit.py:571} INFO - 24/05/20 15:28:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:29:02.410+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:29:12.407+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:29:20.311+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/7 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.7.6c39fc3b-8c8b-4612-b2f7-5c3d5bc2fe13.tmp
[2024-05-20T15:29:20.357+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.7.6c39fc3b-8c8b-4612-b2f7-5c3d5bc2fe13.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/7
[2024-05-20T15:29:20.358+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:20 INFO MicroBatchExecution: Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1716218960279,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-20T15:29:20.423+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:29:20.427+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:29:20.468+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:29:20.470+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:29:20.495+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-20T15:29:20.532+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:20 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-20T15:29:20.533+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:20 INFO DAGScheduler: Got job 7 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-20T15:29:20.534+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:20 INFO DAGScheduler: Final stage: ResultStage 6 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-20T15:29:20.534+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:20 INFO DAGScheduler: Parents of final stage: List()
[2024-05-20T15:29:20.535+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:20 INFO DAGScheduler: Missing parents: List()
[2024-05-20T15:29:20.536+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:20 INFO DAGScheduler: Submitting ResultStage 6 (PythonRDD[79] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-20T15:29:20.543+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:20 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-20T15:29:20.553+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:20 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 434.3 MiB)
[2024-05-20T15:29:20.554+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:20 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on f39758f400c5:39575 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:29:20.555+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:20 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
[2024-05-20T15:29:20.556+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (PythonRDD[79] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-20T15:29:20.556+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:20 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2024-05-20T15:29:20.559+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:20 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (172.25.0.7, executor 0, partition 0, PROCESS_LOCAL, 14270 bytes)
[2024-05-20T15:29:20.618+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:20 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.25.0.7:44267 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:29:21.376+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:21 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 808 ms on 172.25.0.7 (executor 0) (1/1)
[2024-05-20T15:29:21.393+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:21 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2024-05-20T15:29:21.395+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:21 INFO DAGScheduler: ResultStage 6 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.838 s
[2024-05-20T15:29:21.397+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:21 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-20T15:29:21.400+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2024-05-20T15:29:21.402+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:21 INFO DAGScheduler: Job 7 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.854535 s
[2024-05-20T15:29:21.469+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/7 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.7.00570bd5-7730-4f8f-b33b-dda779bb7435.tmp
[2024-05-20T15:29:21.498+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.7.00570bd5-7730-4f8f-b33b-dda779bb7435.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/7
[2024-05-20T15:29:21.502+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:21 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-20T15:29:21.503+0000] {spark_submit.py:571} INFO - "id" : "5f42858b-b652-4f45-a8c9-eeb4fd889226",
[2024-05-20T15:29:21.503+0000] {spark_submit.py:571} INFO - "runId" : "9cc76651-4fe1-43e8-a73c-096daec0f336",
[2024-05-20T15:29:21.503+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-20T15:29:21.504+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-20T15:29:20.277Z",
[2024-05-20T15:29:21.504+0000] {spark_submit.py:571} INFO - "batchId" : 7,
[2024-05-20T15:29:21.504+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:29:21.505+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-20T15:29:21.505+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8190008190008189,
[2024-05-20T15:29:21.505+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-20T15:29:21.505+0000] {spark_submit.py:571} INFO - "addBatch" : 979,
[2024-05-20T15:29:21.505+0000] {spark_submit.py:571} INFO - "commitOffsets" : 83,
[2024-05-20T15:29:21.505+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-20T15:29:21.506+0000] {spark_submit.py:571} INFO - "latestOffset" : 2,
[2024-05-20T15:29:21.506+0000] {spark_submit.py:571} INFO - "queryPlanning" : 70,
[2024-05-20T15:29:21.506+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1221,
[2024-05-20T15:29:21.506+0000] {spark_submit.py:571} INFO - "walCommit" : 80
[2024-05-20T15:29:21.506+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:29:21.506+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-20T15:29:21.507+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-20T15:29:21.507+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-20T15:29:21.507+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-20T15:29:21.507+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:29:21.507+0000] {spark_submit.py:571} INFO - "0" : 6
[2024-05-20T15:29:21.508+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:29:21.508+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:29:21.508+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-20T15:29:21.508+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:29:21.509+0000] {spark_submit.py:571} INFO - "0" : 7
[2024-05-20T15:29:21.509+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:29:21.509+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:29:21.509+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-20T15:29:21.509+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:29:21.510+0000] {spark_submit.py:571} INFO - "0" : 7
[2024-05-20T15:29:21.510+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:29:21.510+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:29:21.510+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:29:21.510+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-20T15:29:21.511+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8190008190008189,
[2024-05-20T15:29:21.511+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-20T15:29:21.511+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-20T15:29:21.511+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-20T15:29:21.512+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-20T15:29:21.512+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:29:21.512+0000] {spark_submit.py:571} INFO - } ],
[2024-05-20T15:29:21.512+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-20T15:29:21.512+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-20T15:29:21.512+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-20T15:29:21.512+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:29:21.512+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:29:29.794+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:29 INFO BlockManagerInfo: Removed broadcast_6_piece0 on f39758f400c5:39575 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:29:29.802+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:29 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.25.0.7:44267 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:29:31.503+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:29:41.511+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:29:46.312+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:46 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/8 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.8.e61519df-4d9b-4040-abb7-8edfe900accd.tmp
[2024-05-20T15:29:46.352+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.8.e61519df-4d9b-4040-abb7-8edfe900accd.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/8
[2024-05-20T15:29:46.353+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:46 INFO MicroBatchExecution: Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1716218986264,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-20T15:29:46.411+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:29:46.415+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:29:46.440+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:29:46.441+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:29:46.487+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-20T15:29:46.528+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:46 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-20T15:29:46.529+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:46 INFO DAGScheduler: Got job 8 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-20T15:29:46.530+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:46 INFO DAGScheduler: Final stage: ResultStage 7 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-20T15:29:46.530+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:46 INFO DAGScheduler: Parents of final stage: List()
[2024-05-20T15:29:46.530+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:46 INFO DAGScheduler: Missing parents: List()
[2024-05-20T15:29:46.530+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:46 INFO DAGScheduler: Submitting ResultStage 7 (PythonRDD[89] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-20T15:29:46.532+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:46 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-20T15:29:46.538+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:46 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 434.3 MiB)
[2024-05-20T15:29:46.539+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:46 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on f39758f400c5:39575 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:29:46.543+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:46 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
[2024-05-20T15:29:46.544+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (PythonRDD[89] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-20T15:29:46.544+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:46 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2024-05-20T15:29:46.545+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:46 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (172.25.0.7, executor 0, partition 0, PROCESS_LOCAL, 14270 bytes)
[2024-05-20T15:29:46.594+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:46 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.25.0.7:44267 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:29:47.345+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:47 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 800 ms on 172.25.0.7 (executor 0) (1/1)
[2024-05-20T15:29:47.347+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:47 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2024-05-20T15:29:47.348+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:47 INFO DAGScheduler: ResultStage 7 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.816 s
[2024-05-20T15:29:47.349+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:47 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-20T15:29:47.350+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2024-05-20T15:29:47.350+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:47 INFO DAGScheduler: Job 8 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.820216 s
[2024-05-20T15:29:47.380+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/8 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.8.d888eec8-9b33-4774-902e-5f1fcb9174ea.tmp
[2024-05-20T15:29:47.408+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.8.d888eec8-9b33-4774-902e-5f1fcb9174ea.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/8
[2024-05-20T15:29:47.411+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:47 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-20T15:29:47.412+0000] {spark_submit.py:571} INFO - "id" : "5f42858b-b652-4f45-a8c9-eeb4fd889226",
[2024-05-20T15:29:47.413+0000] {spark_submit.py:571} INFO - "runId" : "9cc76651-4fe1-43e8-a73c-096daec0f336",
[2024-05-20T15:29:47.413+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-20T15:29:47.413+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-20T15:29:46.262Z",
[2024-05-20T15:29:47.414+0000] {spark_submit.py:571} INFO - "batchId" : 8,
[2024-05-20T15:29:47.414+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:29:47.414+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 62.5,
[2024-05-20T15:29:47.415+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8726003490401397,
[2024-05-20T15:29:47.415+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-20T15:29:47.415+0000] {spark_submit.py:571} INFO - "addBatch" : 936,
[2024-05-20T15:29:47.415+0000] {spark_submit.py:571} INFO - "commitOffsets" : 46,
[2024-05-20T15:29:47.415+0000] {spark_submit.py:571} INFO - "getBatch" : 1,
[2024-05-20T15:29:47.416+0000] {spark_submit.py:571} INFO - "latestOffset" : 2,
[2024-05-20T15:29:47.416+0000] {spark_submit.py:571} INFO - "queryPlanning" : 62,
[2024-05-20T15:29:47.416+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1146,
[2024-05-20T15:29:47.416+0000] {spark_submit.py:571} INFO - "walCommit" : 89
[2024-05-20T15:29:47.416+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:29:47.417+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-20T15:29:47.417+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-20T15:29:47.417+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-20T15:29:47.417+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-20T15:29:47.417+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:29:47.417+0000] {spark_submit.py:571} INFO - "0" : 7
[2024-05-20T15:29:47.417+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:29:47.418+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:29:47.418+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-20T15:29:47.418+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:29:47.418+0000] {spark_submit.py:571} INFO - "0" : 8
[2024-05-20T15:29:47.418+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:29:47.418+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:29:47.418+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-20T15:29:47.419+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:29:47.419+0000] {spark_submit.py:571} INFO - "0" : 8
[2024-05-20T15:29:47.419+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:29:47.419+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:29:47.419+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:29:47.419+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 62.5,
[2024-05-20T15:29:47.419+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8726003490401397,
[2024-05-20T15:29:47.419+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-20T15:29:47.420+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-20T15:29:47.420+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-20T15:29:47.420+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-20T15:29:47.420+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:29:47.420+0000] {spark_submit.py:571} INFO - } ],
[2024-05-20T15:29:47.420+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-20T15:29:47.420+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-20T15:29:47.420+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-20T15:29:47.421+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:29:47.421+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:29:57.453+0000] {spark_submit.py:571} INFO - 24/05/20 15:29:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:30:07.426+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:07 INFO BlockManagerInfo: Removed broadcast_7_piece0 on f39758f400c5:39575 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:30:07.432+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:30:07.467+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:07 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.25.0.7:44267 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:30:17.508+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:30:19.309+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:19 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
[2024-05-20T15:30:26.327+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/9 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.9.b2f3978d-e2a2-4b0e-a2b7-9b8ee00b29e1.tmp
[2024-05-20T15:30:26.356+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.9.b2f3978d-e2a2-4b0e-a2b7-9b8ee00b29e1.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/9
[2024-05-20T15:30:26.357+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:26 INFO MicroBatchExecution: Committed offsets for batch 9. Metadata OffsetSeqMetadata(0,1716219026279,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-20T15:30:26.444+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:30:26.448+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:30:26.489+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:30:26.490+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:30:26.547+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-20T15:30:26.585+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:26 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-20T15:30:26.587+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:26 INFO DAGScheduler: Got job 9 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-20T15:30:26.590+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:26 INFO DAGScheduler: Final stage: ResultStage 8 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-20T15:30:26.591+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:26 INFO DAGScheduler: Parents of final stage: List()
[2024-05-20T15:30:26.591+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:26 INFO DAGScheduler: Missing parents: List()
[2024-05-20T15:30:26.592+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:26 INFO DAGScheduler: Submitting ResultStage 8 (PythonRDD[99] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-20T15:30:26.596+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:26 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-20T15:30:26.608+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:26 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 434.3 MiB)
[2024-05-20T15:30:26.609+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:26 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on f39758f400c5:39575 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:30:26.611+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:26 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585
[2024-05-20T15:30:26.611+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (PythonRDD[99] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-20T15:30:26.611+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:26 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2024-05-20T15:30:26.612+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:26 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (172.25.0.7, executor 0, partition 0, PROCESS_LOCAL, 14270 bytes)
[2024-05-20T15:30:26.675+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:26 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.25.0.7:44267 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:30:27.486+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:27 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 863 ms on 172.25.0.7 (executor 0) (1/1)
[2024-05-20T15:30:27.503+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:27 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2024-05-20T15:30:27.504+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:27 INFO DAGScheduler: ResultStage 8 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.900 s
[2024-05-20T15:30:27.505+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:27 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-20T15:30:27.505+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2024-05-20T15:30:27.505+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:27 INFO DAGScheduler: Job 9 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.915193 s
[2024-05-20T15:30:27.535+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/9 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.9.750f4357-b891-4379-ad6d-b258cfc1d94e.tmp
[2024-05-20T15:30:27.589+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.9.750f4357-b891-4379-ad6d-b258cfc1d94e.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/9
[2024-05-20T15:30:27.593+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:27 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-20T15:30:27.594+0000] {spark_submit.py:571} INFO - "id" : "5f42858b-b652-4f45-a8c9-eeb4fd889226",
[2024-05-20T15:30:27.594+0000] {spark_submit.py:571} INFO - "runId" : "9cc76651-4fe1-43e8-a73c-096daec0f336",
[2024-05-20T15:30:27.594+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-20T15:30:27.594+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-20T15:30:26.276Z",
[2024-05-20T15:30:27.595+0000] {spark_submit.py:571} INFO - "batchId" : 9,
[2024-05-20T15:30:27.595+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:30:27.595+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-20T15:30:27.595+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7616146230007617,
[2024-05-20T15:30:27.595+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-20T15:30:27.595+0000] {spark_submit.py:571} INFO - "addBatch" : 1056,
[2024-05-20T15:30:27.596+0000] {spark_submit.py:571} INFO - "commitOffsets" : 76,
[2024-05-20T15:30:27.596+0000] {spark_submit.py:571} INFO - "getBatch" : 1,
[2024-05-20T15:30:27.596+0000] {spark_submit.py:571} INFO - "latestOffset" : 3,
[2024-05-20T15:30:27.596+0000] {spark_submit.py:571} INFO - "queryPlanning" : 92,
[2024-05-20T15:30:27.596+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1313,
[2024-05-20T15:30:27.596+0000] {spark_submit.py:571} INFO - "walCommit" : 78
[2024-05-20T15:30:27.596+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:30:27.597+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-20T15:30:27.597+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-20T15:30:27.597+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-20T15:30:27.597+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-20T15:30:27.597+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:30:27.597+0000] {spark_submit.py:571} INFO - "0" : 8
[2024-05-20T15:30:27.597+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:30:27.597+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:30:27.597+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-20T15:30:27.598+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:30:27.598+0000] {spark_submit.py:571} INFO - "0" : 9
[2024-05-20T15:30:27.598+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:30:27.598+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:30:27.598+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-20T15:30:27.598+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:30:27.598+0000] {spark_submit.py:571} INFO - "0" : 9
[2024-05-20T15:30:27.598+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:30:27.599+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:30:27.599+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:30:27.599+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-20T15:30:27.599+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7616146230007617,
[2024-05-20T15:30:27.599+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-20T15:30:27.599+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-20T15:30:27.599+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-20T15:30:27.599+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-20T15:30:27.599+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:30:27.600+0000] {spark_submit.py:571} INFO - } ],
[2024-05-20T15:30:27.600+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-20T15:30:27.600+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-20T15:30:27.600+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-20T15:30:27.600+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:30:27.600+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:30:37.602+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:30:45.654+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:45 INFO BlockManagerInfo: Removed broadcast_8_piece0 on f39758f400c5:39575 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:30:45.688+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:45 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.25.0.7:44267 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:30:47.609+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:30:57.638+0000] {spark_submit.py:571} INFO - 24/05/20 15:30:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:31:07.633+0000] {spark_submit.py:571} INFO - 24/05/20 15:31:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:31:17.633+0000] {spark_submit.py:571} INFO - 24/05/20 15:31:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:31:27.640+0000] {spark_submit.py:571} INFO - 24/05/20 15:31:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:31:37.643+0000] {spark_submit.py:571} INFO - 24/05/20 15:31:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:31:47.650+0000] {spark_submit.py:571} INFO - 24/05/20 15:31:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:31:57.659+0000] {spark_submit.py:571} INFO - 24/05/20 15:31:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:32:07.684+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:32:17.702+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:32:27.711+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:32:37.728+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:32:40.883+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/10 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.10.3980fa58-ecae-46ee-a9a5-f0a567ca0e33.tmp
[2024-05-20T15:32:40.932+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.10.3980fa58-ecae-46ee-a9a5-f0a567ca0e33.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/10
[2024-05-20T15:32:40.933+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:40 INFO MicroBatchExecution: Committed offsets for batch 10. Metadata OffsetSeqMetadata(0,1716219160828,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-20T15:32:41.034+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:32:41.072+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:32:41.155+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:32:41.159+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:32:41.199+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-20T15:32:41.305+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:41 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-20T15:32:41.309+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:41 INFO DAGScheduler: Got job 10 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-20T15:32:41.310+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:41 INFO DAGScheduler: Final stage: ResultStage 9 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-20T15:32:41.311+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:41 INFO DAGScheduler: Parents of final stage: List()
[2024-05-20T15:32:41.312+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:41 INFO DAGScheduler: Missing parents: List()
[2024-05-20T15:32:41.313+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:41 INFO DAGScheduler: Submitting ResultStage 9 (PythonRDD[109] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-20T15:32:41.315+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:41 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-20T15:32:41.319+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:41 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 434.3 MiB)
[2024-05-20T15:32:41.320+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:41 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on f39758f400c5:39575 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:32:41.322+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:41 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
[2024-05-20T15:32:41.324+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (PythonRDD[109] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-20T15:32:41.324+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:41 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2024-05-20T15:32:41.327+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:41 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (172.25.0.7, executor 0, partition 0, PROCESS_LOCAL, 14270 bytes)
[2024-05-20T15:32:41.466+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:41 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.25.0.7:44267 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:32:42.411+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:42 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 1082 ms on 172.25.0.7 (executor 0) (1/1)
[2024-05-20T15:32:42.413+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:42 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2024-05-20T15:32:42.415+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:42 INFO DAGScheduler: ResultStage 9 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 1.102 s
[2024-05-20T15:32:42.418+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:42 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-20T15:32:42.419+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2024-05-20T15:32:42.419+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:42 INFO DAGScheduler: Job 10 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 1.111362 s
[2024-05-20T15:32:42.468+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/10 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.10.0a3eb550-372e-4de4-b307-efbf66725f51.tmp
[2024-05-20T15:32:42.510+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.10.0a3eb550-372e-4de4-b307-efbf66725f51.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/10
[2024-05-20T15:32:42.514+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:42 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-20T15:32:42.515+0000] {spark_submit.py:571} INFO - "id" : "5f42858b-b652-4f45-a8c9-eeb4fd889226",
[2024-05-20T15:32:42.515+0000] {spark_submit.py:571} INFO - "runId" : "9cc76651-4fe1-43e8-a73c-096daec0f336",
[2024-05-20T15:32:42.515+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-20T15:32:42.515+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-20T15:32:40.827Z",
[2024-05-20T15:32:42.515+0000] {spark_submit.py:571} INFO - "batchId" : 10,
[2024-05-20T15:32:42.516+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:32:42.516+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-20T15:32:42.516+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.5941770647653001,
[2024-05-20T15:32:42.516+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-20T15:32:42.517+0000] {spark_submit.py:571} INFO - "addBatch" : 1330,
[2024-05-20T15:32:42.517+0000] {spark_submit.py:571} INFO - "commitOffsets" : 75,
[2024-05-20T15:32:42.517+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-20T15:32:42.518+0000] {spark_submit.py:571} INFO - "latestOffset" : 1,
[2024-05-20T15:32:42.518+0000] {spark_submit.py:571} INFO - "queryPlanning" : 145,
[2024-05-20T15:32:42.518+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1683,
[2024-05-20T15:32:42.518+0000] {spark_submit.py:571} INFO - "walCommit" : 105
[2024-05-20T15:32:42.518+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:32:42.518+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-20T15:32:42.518+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-20T15:32:42.518+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-20T15:32:42.518+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-20T15:32:42.519+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:32:42.519+0000] {spark_submit.py:571} INFO - "0" : 9
[2024-05-20T15:32:42.519+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:32:42.520+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:32:42.520+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-20T15:32:42.520+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:32:42.520+0000] {spark_submit.py:571} INFO - "0" : 10
[2024-05-20T15:32:42.520+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:32:42.520+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:32:42.520+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-20T15:32:42.520+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:32:42.520+0000] {spark_submit.py:571} INFO - "0" : 10
[2024-05-20T15:32:42.520+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:32:42.521+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:32:42.521+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:32:42.521+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-20T15:32:42.521+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.5941770647653001,
[2024-05-20T15:32:42.521+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-20T15:32:42.521+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-20T15:32:42.522+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-20T15:32:42.522+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-20T15:32:42.522+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:32:42.522+0000] {spark_submit.py:571} INFO - } ],
[2024-05-20T15:32:42.522+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-20T15:32:42.522+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-20T15:32:42.522+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-20T15:32:42.522+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:32:42.522+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:32:52.515+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:32:58.151+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:58 INFO BlockManagerInfo: Removed broadcast_9_piece0 on f39758f400c5:39575 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:32:58.193+0000] {spark_submit.py:571} INFO - 24/05/20 15:32:58 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.25.0.7:44267 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:33:02.524+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:33:05.987+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/11 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.11.324e0498-64c1-4660-8bd8-8b04b177441f.tmp
[2024-05-20T15:33:06.025+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.11.324e0498-64c1-4660-8bd8-8b04b177441f.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/11
[2024-05-20T15:33:06.025+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:06 INFO MicroBatchExecution: Committed offsets for batch 11. Metadata OffsetSeqMetadata(0,1716219185938,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-20T15:33:06.093+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:33:06.102+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:33:06.156+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:33:06.159+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:33:06.191+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-20T15:33:06.244+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:06 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-20T15:33:06.245+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:06 INFO DAGScheduler: Got job 11 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-20T15:33:06.246+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:06 INFO DAGScheduler: Final stage: ResultStage 10 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-20T15:33:06.246+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:06 INFO DAGScheduler: Parents of final stage: List()
[2024-05-20T15:33:06.247+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:06 INFO DAGScheduler: Missing parents: List()
[2024-05-20T15:33:06.249+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:06 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[119] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-20T15:33:06.250+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:06 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-20T15:33:06.253+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:06 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 434.3 MiB)
[2024-05-20T15:33:06.254+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:06 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on f39758f400c5:39575 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:33:06.254+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:06 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
[2024-05-20T15:33:06.255+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (PythonRDD[119] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-20T15:33:06.255+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:06 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2024-05-20T15:33:06.257+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:06 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (172.25.0.7, executor 0, partition 0, PROCESS_LOCAL, 14270 bytes)
[2024-05-20T15:33:06.320+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:06 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.25.0.7:44267 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:33:07.127+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:07 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 868 ms on 172.25.0.7 (executor 0) (1/1)
[2024-05-20T15:33:07.131+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:07 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2024-05-20T15:33:07.132+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:07 INFO DAGScheduler: ResultStage 10 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.880 s
[2024-05-20T15:33:07.133+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:07 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-20T15:33:07.133+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
[2024-05-20T15:33:07.133+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:07 INFO DAGScheduler: Job 11 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.886259 s
[2024-05-20T15:33:07.155+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/11 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.11.e977114c-35ea-4582-bce2-8ed0b93f1cd3.tmp
[2024-05-20T15:33:07.207+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.11.e977114c-35ea-4582-bce2-8ed0b93f1cd3.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/11
[2024-05-20T15:33:07.215+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:07 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-20T15:33:07.216+0000] {spark_submit.py:571} INFO - "id" : "5f42858b-b652-4f45-a8c9-eeb4fd889226",
[2024-05-20T15:33:07.217+0000] {spark_submit.py:571} INFO - "runId" : "9cc76651-4fe1-43e8-a73c-096daec0f336",
[2024-05-20T15:33:07.217+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-20T15:33:07.218+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-20T15:33:05.932Z",
[2024-05-20T15:33:07.219+0000] {spark_submit.py:571} INFO - "batchId" : 11,
[2024-05-20T15:33:07.219+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:33:07.220+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-20T15:33:07.221+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7843137254901962,
[2024-05-20T15:33:07.221+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-20T15:33:07.222+0000] {spark_submit.py:571} INFO - "addBatch" : 1005,
[2024-05-20T15:33:07.222+0000] {spark_submit.py:571} INFO - "commitOffsets" : 71,
[2024-05-20T15:33:07.223+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-20T15:33:07.224+0000] {spark_submit.py:571} INFO - "latestOffset" : 6,
[2024-05-20T15:33:07.226+0000] {spark_submit.py:571} INFO - "queryPlanning" : 81,
[2024-05-20T15:33:07.226+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1275,
[2024-05-20T15:33:07.227+0000] {spark_submit.py:571} INFO - "walCommit" : 87
[2024-05-20T15:33:07.228+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:33:07.229+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-20T15:33:07.229+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-20T15:33:07.230+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-20T15:33:07.230+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-20T15:33:07.231+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:33:07.231+0000] {spark_submit.py:571} INFO - "0" : 10
[2024-05-20T15:33:07.232+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:33:07.232+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:33:07.233+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-20T15:33:07.237+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:33:07.238+0000] {spark_submit.py:571} INFO - "0" : 11
[2024-05-20T15:33:07.238+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:33:07.239+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:33:07.239+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-20T15:33:07.239+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:33:07.240+0000] {spark_submit.py:571} INFO - "0" : 11
[2024-05-20T15:33:07.240+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:33:07.240+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:33:07.241+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:33:07.241+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-20T15:33:07.242+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7843137254901962,
[2024-05-20T15:33:07.242+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-20T15:33:07.242+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-20T15:33:07.242+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-20T15:33:07.243+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-20T15:33:07.243+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:33:07.243+0000] {spark_submit.py:571} INFO - } ],
[2024-05-20T15:33:07.244+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-20T15:33:07.244+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-20T15:33:07.244+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-20T15:33:07.244+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:33:07.245+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:33:16.281+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:16 INFO BlockManagerInfo: Removed broadcast_10_piece0 on f39758f400c5:39575 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:33:16.299+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:16 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.25.0.7:44267 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:33:17.216+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:33:27.234+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:33:37.244+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:33:47.253+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:33:56.525+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/12 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.12.835da33c-9203-4a4e-8a8c-eeeb2498139f.tmp
[2024-05-20T15:33:56.593+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.12.835da33c-9203-4a4e-8a8c-eeeb2498139f.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/12
[2024-05-20T15:33:56.593+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:56 INFO MicroBatchExecution: Committed offsets for batch 12. Metadata OffsetSeqMetadata(0,1716219236362,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-20T15:33:56.653+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:33:56.656+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:33:56.685+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:33:56.686+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:33:56.727+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-20T15:33:56.875+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:56 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-20T15:33:56.880+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:56 INFO DAGScheduler: Got job 12 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-20T15:33:56.880+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:56 INFO DAGScheduler: Final stage: ResultStage 11 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-20T15:33:56.881+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:56 INFO DAGScheduler: Parents of final stage: List()
[2024-05-20T15:33:56.882+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:56 INFO DAGScheduler: Missing parents: List()
[2024-05-20T15:33:56.883+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:56 INFO DAGScheduler: Submitting ResultStage 11 (PythonRDD[129] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-20T15:33:56.885+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:56 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-20T15:33:56.888+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:56 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 434.3 MiB)
[2024-05-20T15:33:56.890+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:56 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on f39758f400c5:39575 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:33:56.893+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:56 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
[2024-05-20T15:33:56.895+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (PythonRDD[129] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-20T15:33:56.896+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:56 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2024-05-20T15:33:56.897+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:56 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (172.25.0.7, executor 0, partition 0, PROCESS_LOCAL, 14270 bytes)
[2024-05-20T15:33:56.994+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:56 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.25.0.7:44267 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:33:57.860+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:57 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 953 ms on 172.25.0.7 (executor 0) (1/1)
[2024-05-20T15:33:57.887+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:57 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2024-05-20T15:33:57.889+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:57 INFO DAGScheduler: ResultStage 11 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 1.001 s
[2024-05-20T15:33:57.890+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:57 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-20T15:33:57.894+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2024-05-20T15:33:57.899+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:57 INFO DAGScheduler: Job 12 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 1.011546 s
[2024-05-20T15:33:57.935+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/12 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.12.f08d1ca8-92e1-4a1a-9f00-debec856c924.tmp
[2024-05-20T15:33:57.970+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.12.f08d1ca8-92e1-4a1a-9f00-debec856c924.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/12
[2024-05-20T15:33:57.982+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:57 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-20T15:33:57.983+0000] {spark_submit.py:571} INFO - "id" : "5f42858b-b652-4f45-a8c9-eeb4fd889226",
[2024-05-20T15:33:57.984+0000] {spark_submit.py:571} INFO - "runId" : "9cc76651-4fe1-43e8-a73c-096daec0f336",
[2024-05-20T15:33:57.984+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-20T15:33:57.984+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-20T15:33:56.359Z",
[2024-05-20T15:33:57.985+0000] {spark_submit.py:571} INFO - "batchId" : 12,
[2024-05-20T15:33:57.985+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:33:57.985+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-20T15:33:57.985+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6207324643078833,
[2024-05-20T15:33:57.985+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-20T15:33:57.986+0000] {spark_submit.py:571} INFO - "addBatch" : 1243,
[2024-05-20T15:33:57.986+0000] {spark_submit.py:571} INFO - "commitOffsets" : 64,
[2024-05-20T15:33:57.986+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-20T15:33:57.986+0000] {spark_submit.py:571} INFO - "latestOffset" : 3,
[2024-05-20T15:33:57.986+0000] {spark_submit.py:571} INFO - "queryPlanning" : 63,
[2024-05-20T15:33:57.987+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1611,
[2024-05-20T15:33:57.987+0000] {spark_submit.py:571} INFO - "walCommit" : 232
[2024-05-20T15:33:57.987+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:33:57.987+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-20T15:33:57.987+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-20T15:33:57.987+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-20T15:33:57.987+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-20T15:33:57.988+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:33:57.988+0000] {spark_submit.py:571} INFO - "0" : 11
[2024-05-20T15:33:57.988+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:33:57.988+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:33:57.988+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-20T15:33:57.988+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:33:57.988+0000] {spark_submit.py:571} INFO - "0" : 12
[2024-05-20T15:33:57.988+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:33:57.988+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:33:57.989+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-20T15:33:57.989+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:33:57.989+0000] {spark_submit.py:571} INFO - "0" : 12
[2024-05-20T15:33:57.989+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:33:57.989+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:33:57.989+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:33:57.989+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-20T15:33:57.989+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6207324643078833,
[2024-05-20T15:33:57.990+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-20T15:33:57.990+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-20T15:33:57.990+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-20T15:33:57.990+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-20T15:33:57.990+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:33:57.990+0000] {spark_submit.py:571} INFO - } ],
[2024-05-20T15:33:57.990+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-20T15:33:57.990+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-20T15:33:57.990+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-20T15:33:57.991+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:33:57.991+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:33:58.286+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:58 INFO BlockManagerInfo: Removed broadcast_11_piece0 on f39758f400c5:39575 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:33:58.314+0000] {spark_submit.py:571} INFO - 24/05/20 15:33:58 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.25.0.7:44267 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:34:07.991+0000] {spark_submit.py:571} INFO - 24/05/20 15:34:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:34:17.997+0000] {spark_submit.py:571} INFO - 24/05/20 15:34:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:34:27.999+0000] {spark_submit.py:571} INFO - 24/05/20 15:34:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:34:38.005+0000] {spark_submit.py:571} INFO - 24/05/20 15:34:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:34:48.018+0000] {spark_submit.py:571} INFO - 24/05/20 15:34:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:34:58.028+0000] {spark_submit.py:571} INFO - 24/05/20 15:34:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:35:08.029+0000] {spark_submit.py:571} INFO - 24/05/20 15:35:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:35:18.031+0000] {spark_submit.py:571} INFO - 24/05/20 15:35:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:35:28.041+0000] {spark_submit.py:571} INFO - 24/05/20 15:35:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:35:38.046+0000] {spark_submit.py:571} INFO - 24/05/20 15:35:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:35:48.053+0000] {spark_submit.py:571} INFO - 24/05/20 15:35:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:35:58.072+0000] {spark_submit.py:571} INFO - 24/05/20 15:35:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:36:08.056+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:36:09.483+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/13 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.13.4fa3405c-261b-44b6-9b89-ea4a126d14da.tmp
[2024-05-20T15:36:09.588+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.13.4fa3405c-261b-44b6-9b89-ea4a126d14da.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/13
[2024-05-20T15:36:09.589+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:09 INFO MicroBatchExecution: Committed offsets for batch 13. Metadata OffsetSeqMetadata(0,1716219369434,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-20T15:36:09.671+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:36:09.674+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:36:09.702+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:36:09.704+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:36:09.765+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-20T15:36:09.838+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:09 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-20T15:36:09.840+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:09 INFO DAGScheduler: Got job 13 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-20T15:36:09.843+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:09 INFO DAGScheduler: Final stage: ResultStage 12 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-20T15:36:09.843+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:09 INFO DAGScheduler: Parents of final stage: List()
[2024-05-20T15:36:09.844+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:09 INFO DAGScheduler: Missing parents: List()
[2024-05-20T15:36:09.844+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:09 INFO DAGScheduler: Submitting ResultStage 12 (PythonRDD[139] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-20T15:36:09.844+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:09 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-20T15:36:09.845+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:09 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 434.3 MiB)
[2024-05-20T15:36:09.846+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:09 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on f39758f400c5:39575 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:36:09.846+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:09 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
[2024-05-20T15:36:09.846+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (PythonRDD[139] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-20T15:36:09.847+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:09 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2024-05-20T15:36:09.848+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:09 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (172.25.0.7, executor 0, partition 0, PROCESS_LOCAL, 14270 bytes)
[2024-05-20T15:36:09.893+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:09 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.25.0.7:44267 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:36:10.801+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:10 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 952 ms on 172.25.0.7 (executor 0) (1/1)
[2024-05-20T15:36:10.803+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:10 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2024-05-20T15:36:10.804+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:10 INFO DAGScheduler: ResultStage 12 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.962 s
[2024-05-20T15:36:10.805+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:10 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-20T15:36:10.806+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
[2024-05-20T15:36:10.807+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:10 INFO DAGScheduler: Job 13 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.967444 s
[2024-05-20T15:36:10.842+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/13 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.13.c67abfca-a5eb-45df-9cff-b74c7bc97f82.tmp
[2024-05-20T15:36:10.871+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.13.c67abfca-a5eb-45df-9cff-b74c7bc97f82.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/13
[2024-05-20T15:36:10.875+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:10 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-20T15:36:10.876+0000] {spark_submit.py:571} INFO - "id" : "5f42858b-b652-4f45-a8c9-eeb4fd889226",
[2024-05-20T15:36:10.876+0000] {spark_submit.py:571} INFO - "runId" : "9cc76651-4fe1-43e8-a73c-096daec0f336",
[2024-05-20T15:36:10.877+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-20T15:36:10.877+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-20T15:36:09.432Z",
[2024-05-20T15:36:10.877+0000] {spark_submit.py:571} INFO - "batchId" : 13,
[2024-05-20T15:36:10.878+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:36:10.878+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-20T15:36:10.878+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6944444444444444,
[2024-05-20T15:36:10.878+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-20T15:36:10.878+0000] {spark_submit.py:571} INFO - "addBatch" : 1135,
[2024-05-20T15:36:10.878+0000] {spark_submit.py:571} INFO - "commitOffsets" : 55,
[2024-05-20T15:36:10.878+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-20T15:36:10.879+0000] {spark_submit.py:571} INFO - "latestOffset" : 2,
[2024-05-20T15:36:10.879+0000] {spark_submit.py:571} INFO - "queryPlanning" : 73,
[2024-05-20T15:36:10.879+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1440,
[2024-05-20T15:36:10.879+0000] {spark_submit.py:571} INFO - "walCommit" : 159
[2024-05-20T15:36:10.879+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:36:10.879+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-20T15:36:10.880+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-20T15:36:10.880+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-20T15:36:10.880+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-20T15:36:10.880+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:36:10.880+0000] {spark_submit.py:571} INFO - "0" : 12
[2024-05-20T15:36:10.880+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:36:10.880+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:36:10.880+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-20T15:36:10.880+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:36:10.881+0000] {spark_submit.py:571} INFO - "0" : 13
[2024-05-20T15:36:10.881+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:36:10.881+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:36:10.881+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-20T15:36:10.881+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:36:10.881+0000] {spark_submit.py:571} INFO - "0" : 13
[2024-05-20T15:36:10.881+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:36:10.881+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:36:10.882+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:36:10.882+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-20T15:36:10.882+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6944444444444444,
[2024-05-20T15:36:10.882+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-20T15:36:10.882+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-20T15:36:10.882+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-20T15:36:10.882+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-20T15:36:10.882+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:36:10.882+0000] {spark_submit.py:571} INFO - } ],
[2024-05-20T15:36:10.883+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-20T15:36:10.883+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-20T15:36:10.883+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-20T15:36:10.883+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:36:10.883+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:36:15.235+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:15 INFO BlockManagerInfo: Removed broadcast_12_piece0 on f39758f400c5:39575 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:36:15.285+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:15 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.25.0.7:44267 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:36:20.889+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:36:30.895+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:36:40.899+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:36:45.132+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/14 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.14.8dc46935-7949-4646-8ba6-f71f1592c3c1.tmp
[2024-05-20T15:36:45.181+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.14.8dc46935-7949-4646-8ba6-f71f1592c3c1.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/14
[2024-05-20T15:36:45.182+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:45 INFO MicroBatchExecution: Committed offsets for batch 14. Metadata OffsetSeqMetadata(0,1716219405077,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-20T15:36:45.243+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:36:45.247+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:36:45.279+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:36:45.280+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:36:45.304+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-20T15:36:45.358+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:45 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-20T15:36:45.359+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:45 INFO DAGScheduler: Got job 14 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-20T15:36:45.359+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:45 INFO DAGScheduler: Final stage: ResultStage 13 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-20T15:36:45.360+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:45 INFO DAGScheduler: Parents of final stage: List()
[2024-05-20T15:36:45.360+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:45 INFO DAGScheduler: Missing parents: List()
[2024-05-20T15:36:45.361+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:45 INFO DAGScheduler: Submitting ResultStage 13 (PythonRDD[149] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-20T15:36:45.364+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:45 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-20T15:36:45.366+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:45 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 434.3 MiB)
[2024-05-20T15:36:45.367+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:45 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on f39758f400c5:39575 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:36:45.367+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:45 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585
[2024-05-20T15:36:45.367+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (PythonRDD[149] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-20T15:36:45.368+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:45 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2024-05-20T15:36:45.369+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:45 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (172.25.0.7, executor 0, partition 0, PROCESS_LOCAL, 14270 bytes)
[2024-05-20T15:36:45.432+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:45 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.25.0.7:44267 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:36:46.157+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:46 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 786 ms on 172.25.0.7 (executor 0) (1/1)
[2024-05-20T15:36:46.159+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:46 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2024-05-20T15:36:46.161+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:46 INFO DAGScheduler: ResultStage 13 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.797 s
[2024-05-20T15:36:46.162+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:46 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-20T15:36:46.162+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2024-05-20T15:36:46.163+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:46 INFO DAGScheduler: Job 14 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.803797 s
[2024-05-20T15:36:46.186+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:46 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/14 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.14.445c3fb2-0570-4c09-afc3-edde2decc4fb.tmp
[2024-05-20T15:36:46.216+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.14.445c3fb2-0570-4c09-afc3-edde2decc4fb.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/14
[2024-05-20T15:36:46.224+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:46 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-20T15:36:46.226+0000] {spark_submit.py:571} INFO - "id" : "5f42858b-b652-4f45-a8c9-eeb4fd889226",
[2024-05-20T15:36:46.226+0000] {spark_submit.py:571} INFO - "runId" : "9cc76651-4fe1-43e8-a73c-096daec0f336",
[2024-05-20T15:36:46.227+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-20T15:36:46.227+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-20T15:36:45.074Z",
[2024-05-20T15:36:46.227+0000] {spark_submit.py:571} INFO - "batchId" : 14,
[2024-05-20T15:36:46.228+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:36:46.229+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-05-20T15:36:46.229+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8756567425569177,
[2024-05-20T15:36:46.230+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-20T15:36:46.230+0000] {spark_submit.py:571} INFO - "addBatch" : 911,
[2024-05-20T15:36:46.230+0000] {spark_submit.py:571} INFO - "commitOffsets" : 47,
[2024-05-20T15:36:46.230+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-20T15:36:46.231+0000] {spark_submit.py:571} INFO - "latestOffset" : 2,
[2024-05-20T15:36:46.231+0000] {spark_submit.py:571} INFO - "queryPlanning" : 65,
[2024-05-20T15:36:46.231+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1142,
[2024-05-20T15:36:46.231+0000] {spark_submit.py:571} INFO - "walCommit" : 105
[2024-05-20T15:36:46.232+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:36:46.232+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-20T15:36:46.233+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-20T15:36:46.233+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-20T15:36:46.234+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-20T15:36:46.234+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:36:46.235+0000] {spark_submit.py:571} INFO - "0" : 13
[2024-05-20T15:36:46.235+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:36:46.235+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:36:46.236+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-20T15:36:46.236+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:36:46.236+0000] {spark_submit.py:571} INFO - "0" : 14
[2024-05-20T15:36:46.237+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:36:46.237+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:36:46.238+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-20T15:36:46.238+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:36:46.238+0000] {spark_submit.py:571} INFO - "0" : 14
[2024-05-20T15:36:46.238+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:36:46.239+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:36:46.239+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:36:46.239+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-05-20T15:36:46.240+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8756567425569177,
[2024-05-20T15:36:46.240+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-20T15:36:46.240+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-20T15:36:46.240+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-20T15:36:46.240+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-20T15:36:46.241+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:36:46.241+0000] {spark_submit.py:571} INFO - } ],
[2024-05-20T15:36:46.241+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-20T15:36:46.241+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-20T15:36:46.241+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-20T15:36:46.242+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:36:46.242+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:36:56.225+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:36:56.715+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:56 INFO BlockManagerInfo: Removed broadcast_13_piece0 on f39758f400c5:39575 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:36:56.760+0000] {spark_submit.py:571} INFO - 24/05/20 15:36:56 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.25.0.7:44267 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:37:06.222+0000] {spark_submit.py:571} INFO - 24/05/20 15:37:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:37:16.241+0000] {spark_submit.py:571} INFO - 24/05/20 15:37:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:37:26.248+0000] {spark_submit.py:571} INFO - 24/05/20 15:37:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:37:36.268+0000] {spark_submit.py:571} INFO - 24/05/20 15:37:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:37:46.255+0000] {spark_submit.py:571} INFO - 24/05/20 15:37:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:37:56.274+0000] {spark_submit.py:571} INFO - 24/05/20 15:37:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:38:06.281+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:38:16.281+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:38:26.286+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:38:36.297+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:38:46.308+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:38:56.322+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:38:58.736+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/15 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.15.13cfa50e-9323-4360-8182-a379046a5929.tmp
[2024-05-20T15:38:58.772+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.15.13cfa50e-9323-4360-8182-a379046a5929.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/15
[2024-05-20T15:38:58.774+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:58 INFO MicroBatchExecution: Committed offsets for batch 15. Metadata OffsetSeqMetadata(0,1716219538703,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-20T15:38:58.845+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:38:58.849+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:38:58.866+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:38:58.867+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:38:58.908+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-20T15:38:58.947+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:58 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-20T15:38:58.949+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:58 INFO DAGScheduler: Got job 15 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-20T15:38:58.949+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:58 INFO DAGScheduler: Final stage: ResultStage 14 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-20T15:38:58.949+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:58 INFO DAGScheduler: Parents of final stage: List()
[2024-05-20T15:38:58.951+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:58 INFO DAGScheduler: Missing parents: List()
[2024-05-20T15:38:58.952+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:58 INFO DAGScheduler: Submitting ResultStage 14 (PythonRDD[159] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-20T15:38:58.953+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:58 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-20T15:38:58.959+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:58 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 434.3 MiB)
[2024-05-20T15:38:58.960+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:58 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on f39758f400c5:39575 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:38:58.961+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:58 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585
[2024-05-20T15:38:58.962+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (PythonRDD[159] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-20T15:38:58.963+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:58 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
[2024-05-20T15:38:58.967+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:58 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (172.25.0.7, executor 0, partition 0, PROCESS_LOCAL, 14270 bytes)
[2024-05-20T15:38:59.022+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:59 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.25.0.7:44267 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:38:59.847+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:59 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 878 ms on 172.25.0.7 (executor 0) (1/1)
[2024-05-20T15:38:59.857+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:59 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2024-05-20T15:38:59.859+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:59 INFO DAGScheduler: ResultStage 14 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.901 s
[2024-05-20T15:38:59.863+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:59 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-20T15:38:59.863+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
[2024-05-20T15:38:59.864+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:59 INFO DAGScheduler: Job 15 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.912514 s
[2024-05-20T15:38:59.898+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/15 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.15.eef2fc13-6b65-42d7-8227-2447c902a164.tmp
[2024-05-20T15:38:59.927+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.15.eef2fc13-6b65-42d7-8227-2447c902a164.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/15
[2024-05-20T15:38:59.941+0000] {spark_submit.py:571} INFO - 24/05/20 15:38:59 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-20T15:38:59.942+0000] {spark_submit.py:571} INFO - "id" : "5f42858b-b652-4f45-a8c9-eeb4fd889226",
[2024-05-20T15:38:59.942+0000] {spark_submit.py:571} INFO - "runId" : "9cc76651-4fe1-43e8-a73c-096daec0f336",
[2024-05-20T15:38:59.942+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-20T15:38:59.942+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-20T15:38:58.702Z",
[2024-05-20T15:38:59.942+0000] {spark_submit.py:571} INFO - "batchId" : 15,
[2024-05-20T15:38:59.942+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:38:59.943+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 90.90909090909092,
[2024-05-20T15:38:59.943+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8149959250203749,
[2024-05-20T15:38:59.943+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-20T15:38:59.943+0000] {spark_submit.py:571} INFO - "addBatch" : 1027,
[2024-05-20T15:38:59.943+0000] {spark_submit.py:571} INFO - "commitOffsets" : 46,
[2024-05-20T15:38:59.944+0000] {spark_submit.py:571} INFO - "getBatch" : 3,
[2024-05-20T15:38:59.944+0000] {spark_submit.py:571} INFO - "latestOffset" : 1,
[2024-05-20T15:38:59.944+0000] {spark_submit.py:571} INFO - "queryPlanning" : 63,
[2024-05-20T15:38:59.944+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1227,
[2024-05-20T15:38:59.944+0000] {spark_submit.py:571} INFO - "walCommit" : 74
[2024-05-20T15:38:59.944+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:38:59.944+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-20T15:38:59.945+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-20T15:38:59.945+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-20T15:38:59.945+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-20T15:38:59.945+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:38:59.945+0000] {spark_submit.py:571} INFO - "0" : 14
[2024-05-20T15:38:59.945+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:38:59.945+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:38:59.945+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-20T15:38:59.946+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:38:59.946+0000] {spark_submit.py:571} INFO - "0" : 15
[2024-05-20T15:38:59.946+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:38:59.946+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:38:59.946+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-20T15:38:59.946+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:38:59.946+0000] {spark_submit.py:571} INFO - "0" : 15
[2024-05-20T15:38:59.946+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:38:59.947+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:38:59.947+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:38:59.947+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 90.90909090909092,
[2024-05-20T15:38:59.947+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8149959250203749,
[2024-05-20T15:38:59.947+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-20T15:38:59.947+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-20T15:38:59.947+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-20T15:38:59.947+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-20T15:38:59.947+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:38:59.948+0000] {spark_submit.py:571} INFO - } ],
[2024-05-20T15:38:59.948+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-20T15:38:59.948+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-20T15:38:59.948+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-20T15:38:59.948+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:38:59.948+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:39:09.947+0000] {spark_submit.py:571} INFO - 24/05/20 15:39:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:39:13.598+0000] {spark_submit.py:571} INFO - 24/05/20 15:39:13 INFO BlockManagerInfo: Removed broadcast_14_piece0 on f39758f400c5:39575 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:39:13.622+0000] {spark_submit.py:571} INFO - 24/05/20 15:39:13 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.25.0.7:44267 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:39:19.954+0000] {spark_submit.py:571} INFO - 24/05/20 15:39:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:39:29.980+0000] {spark_submit.py:571} INFO - 24/05/20 15:39:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:39:39.966+0000] {spark_submit.py:571} INFO - 24/05/20 15:39:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:39:49.972+0000] {spark_submit.py:571} INFO - 24/05/20 15:39:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:39:59.976+0000] {spark_submit.py:571} INFO - 24/05/20 15:39:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:40:09.981+0000] {spark_submit.py:571} INFO - 24/05/20 15:40:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:40:19.989+0000] {spark_submit.py:571} INFO - 24/05/20 15:40:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:40:30.143+0000] {spark_submit.py:571} INFO - 24/05/20 15:40:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:40:40.146+0000] {spark_submit.py:571} INFO - 24/05/20 15:40:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:40:50.152+0000] {spark_submit.py:571} INFO - 24/05/20 15:40:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:41:00.163+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:41:10.163+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:41:14.886+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/16 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.16.5fba225e-5ba5-4063-a225-3eabd59dfda8.tmp
[2024-05-20T15:41:14.934+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.16.5fba225e-5ba5-4063-a225-3eabd59dfda8.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/16
[2024-05-20T15:41:14.939+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:14 INFO MicroBatchExecution: Committed offsets for batch 16. Metadata OffsetSeqMetadata(0,1716219674833,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-20T15:41:15.011+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:41:15.015+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:41:15.047+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:41:15.048+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:41:15.082+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-20T15:41:15.135+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:15 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-20T15:41:15.137+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:15 INFO DAGScheduler: Got job 16 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-20T15:41:15.138+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:15 INFO DAGScheduler: Final stage: ResultStage 15 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-20T15:41:15.138+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:15 INFO DAGScheduler: Parents of final stage: List()
[2024-05-20T15:41:15.138+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:15 INFO DAGScheduler: Missing parents: List()
[2024-05-20T15:41:15.139+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:15 INFO DAGScheduler: Submitting ResultStage 15 (PythonRDD[169] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-20T15:41:15.143+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:15 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-20T15:41:15.146+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:15 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 434.3 MiB)
[2024-05-20T15:41:15.146+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:15 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on f39758f400c5:39575 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:41:15.147+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:15 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585
[2024-05-20T15:41:15.147+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (PythonRDD[169] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-20T15:41:15.147+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:15 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2024-05-20T15:41:15.150+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:15 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (172.25.0.7, executor 0, partition 0, PROCESS_LOCAL, 14270 bytes)
[2024-05-20T15:41:15.239+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:15 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.25.0.7:44267 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:41:16.086+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:16 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 935 ms on 172.25.0.7 (executor 0) (1/1)
[2024-05-20T15:41:16.091+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:16 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2024-05-20T15:41:16.092+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:16 INFO DAGScheduler: ResultStage 15 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.948 s
[2024-05-20T15:41:16.093+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:16 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-20T15:41:16.096+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
[2024-05-20T15:41:16.096+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:16 INFO DAGScheduler: Job 16 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.958402 s
[2024-05-20T15:41:16.126+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/16 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.16.db5b9586-188d-4371-91ef-959f23c1cd46.tmp
[2024-05-20T15:41:16.170+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.16.db5b9586-188d-4371-91ef-959f23c1cd46.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/16
[2024-05-20T15:41:16.175+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:16 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-20T15:41:16.176+0000] {spark_submit.py:571} INFO - "id" : "5f42858b-b652-4f45-a8c9-eeb4fd889226",
[2024-05-20T15:41:16.176+0000] {spark_submit.py:571} INFO - "runId" : "9cc76651-4fe1-43e8-a73c-096daec0f336",
[2024-05-20T15:41:16.176+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-20T15:41:16.177+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-20T15:41:14.831Z",
[2024-05-20T15:41:16.177+0000] {spark_submit.py:571} INFO - "batchId" : 16,
[2024-05-20T15:41:16.177+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:41:16.178+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 62.5,
[2024-05-20T15:41:16.178+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7462686567164178,
[2024-05-20T15:41:16.178+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-20T15:41:16.178+0000] {spark_submit.py:571} INFO - "addBatch" : 1077,
[2024-05-20T15:41:16.179+0000] {spark_submit.py:571} INFO - "commitOffsets" : 67,
[2024-05-20T15:41:16.179+0000] {spark_submit.py:571} INFO - "getBatch" : 1,
[2024-05-20T15:41:16.179+0000] {spark_submit.py:571} INFO - "latestOffset" : 2,
[2024-05-20T15:41:16.179+0000] {spark_submit.py:571} INFO - "queryPlanning" : 74,
[2024-05-20T15:41:16.179+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1340,
[2024-05-20T15:41:16.179+0000] {spark_submit.py:571} INFO - "walCommit" : 107
[2024-05-20T15:41:16.179+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:41:16.179+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-20T15:41:16.180+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-20T15:41:16.180+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-20T15:41:16.180+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-20T15:41:16.180+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:41:16.180+0000] {spark_submit.py:571} INFO - "0" : 15
[2024-05-20T15:41:16.180+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:41:16.181+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:41:16.181+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-20T15:41:16.181+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:41:16.181+0000] {spark_submit.py:571} INFO - "0" : 16
[2024-05-20T15:41:16.181+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:41:16.181+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:41:16.181+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-20T15:41:16.181+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:41:16.182+0000] {spark_submit.py:571} INFO - "0" : 16
[2024-05-20T15:41:16.182+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:41:16.182+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:41:16.182+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:41:16.182+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 62.5,
[2024-05-20T15:41:16.182+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7462686567164178,
[2024-05-20T15:41:16.182+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-20T15:41:16.182+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-20T15:41:16.182+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-20T15:41:16.183+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-20T15:41:16.183+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:41:16.183+0000] {spark_submit.py:571} INFO - } ],
[2024-05-20T15:41:16.183+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-20T15:41:16.183+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-20T15:41:16.183+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-20T15:41:16.183+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:41:16.183+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:41:26.190+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:41:30.859+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:30 INFO BlockManagerInfo: Removed broadcast_15_piece0 on f39758f400c5:39575 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:41:30.881+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:30 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.25.0.7:44267 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:41:36.209+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:41:46.212+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:41:56.214+0000] {spark_submit.py:571} INFO - 24/05/20 15:41:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:42:06.212+0000] {spark_submit.py:571} INFO - 24/05/20 15:42:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:42:16.218+0000] {spark_submit.py:571} INFO - 24/05/20 15:42:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:42:26.232+0000] {spark_submit.py:571} INFO - 24/05/20 15:42:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:42:36.285+0000] {spark_submit.py:571} INFO - 24/05/20 15:42:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:42:46.250+0000] {spark_submit.py:571} INFO - 24/05/20 15:42:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:42:56.263+0000] {spark_submit.py:571} INFO - 24/05/20 15:42:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:43:06.258+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:43:16.267+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:43:26.286+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:43:29.756+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/17 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.17.f4720291-1234-4ab9-8d75-e10e11a17745.tmp
[2024-05-20T15:43:29.869+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.17.f4720291-1234-4ab9-8d75-e10e11a17745.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/17
[2024-05-20T15:43:29.870+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:29 INFO MicroBatchExecution: Committed offsets for batch 17. Metadata OffsetSeqMetadata(0,1716219809683,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-20T15:43:29.962+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:43:29.964+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:43:29.991+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:43:29.992+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:43:30.067+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-20T15:43:30.108+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:30 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-20T15:43:30.110+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:30 INFO DAGScheduler: Got job 17 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-20T15:43:30.111+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:30 INFO DAGScheduler: Final stage: ResultStage 16 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-20T15:43:30.112+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:30 INFO DAGScheduler: Parents of final stage: List()
[2024-05-20T15:43:30.113+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:30 INFO DAGScheduler: Missing parents: List()
[2024-05-20T15:43:30.113+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:30 INFO DAGScheduler: Submitting ResultStage 16 (PythonRDD[179] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-20T15:43:30.117+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:30 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-20T15:43:30.128+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:30 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 434.3 MiB)
[2024-05-20T15:43:30.131+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:30 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on f39758f400c5:39575 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:43:30.133+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:30 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585
[2024-05-20T15:43:30.135+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (PythonRDD[179] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-20T15:43:30.135+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:30 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
[2024-05-20T15:43:30.137+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:30 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (172.25.0.7, executor 0, partition 0, PROCESS_LOCAL, 14270 bytes)
[2024-05-20T15:43:30.189+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:30 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.25.0.7:44267 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:43:31.006+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:31 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 866 ms on 172.25.0.7 (executor 0) (1/1)
[2024-05-20T15:43:31.010+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:31 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2024-05-20T15:43:31.012+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:31 INFO DAGScheduler: ResultStage 16 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.896 s
[2024-05-20T15:43:31.014+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:31 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-20T15:43:31.019+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
[2024-05-20T15:43:31.020+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:31 INFO DAGScheduler: Job 17 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.906556 s
[2024-05-20T15:43:31.130+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:31 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/17 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.17.4960a8a0-894f-47eb-a704-b9a63468b318.tmp
[2024-05-20T15:43:31.187+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:31 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.17.4960a8a0-894f-47eb-a704-b9a63468b318.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/17
[2024-05-20T15:43:31.211+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:31 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-20T15:43:31.213+0000] {spark_submit.py:571} INFO - "id" : "5f42858b-b652-4f45-a8c9-eeb4fd889226",
[2024-05-20T15:43:31.214+0000] {spark_submit.py:571} INFO - "runId" : "9cc76651-4fe1-43e8-a73c-096daec0f336",
[2024-05-20T15:43:31.216+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-20T15:43:31.216+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-20T15:43:29.681Z",
[2024-05-20T15:43:31.218+0000] {spark_submit.py:571} INFO - "batchId" : 17,
[2024-05-20T15:43:31.219+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:43:31.220+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-20T15:43:31.221+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6640106241699867,
[2024-05-20T15:43:31.221+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-20T15:43:31.222+0000] {spark_submit.py:571} INFO - "addBatch" : 1080,
[2024-05-20T15:43:31.222+0000] {spark_submit.py:571} INFO - "commitOffsets" : 128,
[2024-05-20T15:43:31.222+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-20T15:43:31.222+0000] {spark_submit.py:571} INFO - "latestOffset" : 2,
[2024-05-20T15:43:31.223+0000] {spark_submit.py:571} INFO - "queryPlanning" : 93,
[2024-05-20T15:43:31.223+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1506,
[2024-05-20T15:43:31.223+0000] {spark_submit.py:571} INFO - "walCommit" : 188
[2024-05-20T15:43:31.223+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:43:31.224+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-20T15:43:31.224+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-20T15:43:31.224+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-20T15:43:31.224+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-20T15:43:31.225+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:43:31.225+0000] {spark_submit.py:571} INFO - "0" : 16
[2024-05-20T15:43:31.225+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:43:31.225+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:43:31.226+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-20T15:43:31.226+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:43:31.226+0000] {spark_submit.py:571} INFO - "0" : 17
[2024-05-20T15:43:31.226+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:43:31.227+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:43:31.227+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-20T15:43:31.227+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:43:31.227+0000] {spark_submit.py:571} INFO - "0" : 17
[2024-05-20T15:43:31.227+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:43:31.227+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:43:31.228+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:43:31.228+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-20T15:43:31.228+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6640106241699867,
[2024-05-20T15:43:31.228+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-20T15:43:31.228+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-20T15:43:31.229+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-20T15:43:31.229+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-20T15:43:31.229+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:43:31.230+0000] {spark_submit.py:571} INFO - } ],
[2024-05-20T15:43:31.230+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-20T15:43:31.230+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-20T15:43:31.230+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-20T15:43:31.231+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:43:31.231+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:43:41.295+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:43:48.479+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:48 INFO BlockManagerInfo: Removed broadcast_16_piece0 on f39758f400c5:39575 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:43:48.496+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:48 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.25.0.7:44267 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:43:51.270+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:43:52.770+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/18 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.18.9fbd8189-c10b-4204-bf2a-f04b9f56c59e.tmp
[2024-05-20T15:43:52.819+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.18.9fbd8189-c10b-4204-bf2a-f04b9f56c59e.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/18
[2024-05-20T15:43:52.819+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:52 INFO MicroBatchExecution: Committed offsets for batch 18. Metadata OffsetSeqMetadata(0,1716219832719,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-20T15:43:52.884+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:43:52.887+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:43:52.915+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:43:52.916+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:43:52.969+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-20T15:43:53.012+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:53 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-20T15:43:53.016+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:53 INFO DAGScheduler: Got job 18 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-20T15:43:53.017+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:53 INFO DAGScheduler: Final stage: ResultStage 17 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-20T15:43:53.017+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:53 INFO DAGScheduler: Parents of final stage: List()
[2024-05-20T15:43:53.017+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:53 INFO DAGScheduler: Missing parents: List()
[2024-05-20T15:43:53.020+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:53 INFO DAGScheduler: Submitting ResultStage 17 (PythonRDD[189] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-20T15:43:53.031+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:53 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-20T15:43:53.034+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:53 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 434.3 MiB)
[2024-05-20T15:43:53.034+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:53 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on f39758f400c5:39575 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:43:53.034+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:53 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1585
[2024-05-20T15:43:53.035+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (PythonRDD[189] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-20T15:43:53.035+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:53 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2024-05-20T15:43:53.037+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:53 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (172.25.0.7, executor 0, partition 0, PROCESS_LOCAL, 14270 bytes)
[2024-05-20T15:43:53.089+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:53 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.25.0.7:44267 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:43:53.971+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:53 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 928 ms on 172.25.0.7 (executor 0) (1/1)
[2024-05-20T15:43:53.975+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:53 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2024-05-20T15:43:53.976+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:53 INFO DAGScheduler: ResultStage 17 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.950 s
[2024-05-20T15:43:53.978+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:53 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-20T15:43:53.978+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
[2024-05-20T15:43:53.979+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:53 INFO DAGScheduler: Job 18 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.965630 s
[2024-05-20T15:43:54.024+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/18 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.18.52847312-db33-411f-a04c-ab46179d1afe.tmp
[2024-05-20T15:43:54.098+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.18.52847312-db33-411f-a04c-ab46179d1afe.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/18
[2024-05-20T15:43:54.105+0000] {spark_submit.py:571} INFO - 24/05/20 15:43:54 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-20T15:43:54.105+0000] {spark_submit.py:571} INFO - "id" : "5f42858b-b652-4f45-a8c9-eeb4fd889226",
[2024-05-20T15:43:54.105+0000] {spark_submit.py:571} INFO - "runId" : "9cc76651-4fe1-43e8-a73c-096daec0f336",
[2024-05-20T15:43:54.105+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-20T15:43:54.105+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-20T15:43:52.716Z",
[2024-05-20T15:43:54.106+0000] {spark_submit.py:571} INFO - "batchId" : 18,
[2024-05-20T15:43:54.106+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:43:54.106+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-20T15:43:54.106+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7230657989877078,
[2024-05-20T15:43:54.106+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-20T15:43:54.107+0000] {spark_submit.py:571} INFO - "addBatch" : 1109,
[2024-05-20T15:43:54.107+0000] {spark_submit.py:571} INFO - "commitOffsets" : 95,
[2024-05-20T15:43:54.107+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-20T15:43:54.108+0000] {spark_submit.py:571} INFO - "latestOffset" : 3,
[2024-05-20T15:43:54.108+0000] {spark_submit.py:571} INFO - "queryPlanning" : 67,
[2024-05-20T15:43:54.109+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1383,
[2024-05-20T15:43:54.110+0000] {spark_submit.py:571} INFO - "walCommit" : 100
[2024-05-20T15:43:54.111+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:43:54.111+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-20T15:43:54.112+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-20T15:43:54.112+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-20T15:43:54.113+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-20T15:43:54.115+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:43:54.116+0000] {spark_submit.py:571} INFO - "0" : 17
[2024-05-20T15:43:54.117+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:43:54.117+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:43:54.118+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-20T15:43:54.121+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:43:54.123+0000] {spark_submit.py:571} INFO - "0" : 18
[2024-05-20T15:43:54.124+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:43:54.125+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:43:54.127+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-20T15:43:54.127+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:43:54.128+0000] {spark_submit.py:571} INFO - "0" : 18
[2024-05-20T15:43:54.128+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:43:54.128+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:43:54.128+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:43:54.129+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-20T15:43:54.129+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7230657989877078,
[2024-05-20T15:43:54.129+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-20T15:43:54.129+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-20T15:43:54.129+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-20T15:43:54.130+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-20T15:43:54.130+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:43:54.136+0000] {spark_submit.py:571} INFO - } ],
[2024-05-20T15:43:54.137+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-20T15:43:54.137+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-20T15:43:54.137+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-20T15:43:54.138+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:43:54.138+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:44:04.123+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:44:07.158+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:07 INFO BlockManagerInfo: Removed broadcast_17_piece0 on f39758f400c5:39575 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:44:07.199+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:07 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.25.0.7:44267 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:44:14.125+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:44:24.141+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:44:34.148+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:44:34.882+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/19 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.19.1ecf8709-20c3-4418-803d-536ecd1c425c.tmp
[2024-05-20T15:44:34.915+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.19.1ecf8709-20c3-4418-803d-536ecd1c425c.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/19
[2024-05-20T15:44:34.916+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:34 INFO MicroBatchExecution: Committed offsets for batch 19. Metadata OffsetSeqMetadata(0,1716219874848,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-20T15:44:34.987+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:44:34.991+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:44:35.007+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:44:35.009+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:44:35.036+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-20T15:44:35.086+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-20T15:44:35.087+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO DAGScheduler: Got job 19 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-20T15:44:35.090+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO DAGScheduler: Final stage: ResultStage 18 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-20T15:44:35.091+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO DAGScheduler: Parents of final stage: List()
[2024-05-20T15:44:35.091+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO DAGScheduler: Missing parents: List()
[2024-05-20T15:44:35.096+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO DAGScheduler: Submitting ResultStage 18 (PythonRDD[199] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-20T15:44:35.097+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-20T15:44:35.098+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 434.3 MiB)
[2024-05-20T15:44:35.098+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on f39758f400c5:39575 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:44:35.098+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
[2024-05-20T15:44:35.098+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (PythonRDD[199] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-20T15:44:35.098+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2024-05-20T15:44:35.098+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (172.25.0.7, executor 0, partition 0, PROCESS_LOCAL, 14270 bytes)
[2024-05-20T15:44:35.158+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.25.0.7:44267 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:44:35.937+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 841 ms on 172.25.0.7 (executor 0) (1/1)
[2024-05-20T15:44:35.938+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2024-05-20T15:44:35.940+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO DAGScheduler: ResultStage 18 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.850 s
[2024-05-20T15:44:35.940+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-20T15:44:35.940+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
[2024-05-20T15:44:35.941+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO DAGScheduler: Job 19 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.854830 s
[2024-05-20T15:44:35.983+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/19 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.19.ef1be201-195d-41c5-9ddc-6ac77f68afa6.tmp
[2024-05-20T15:44:36.012+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.19.ef1be201-195d-41c5-9ddc-6ac77f68afa6.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/19
[2024-05-20T15:44:36.015+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:36 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-20T15:44:36.016+0000] {spark_submit.py:571} INFO - "id" : "5f42858b-b652-4f45-a8c9-eeb4fd889226",
[2024-05-20T15:44:36.016+0000] {spark_submit.py:571} INFO - "runId" : "9cc76651-4fe1-43e8-a73c-096daec0f336",
[2024-05-20T15:44:36.016+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-20T15:44:36.017+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-20T15:44:34.847Z",
[2024-05-20T15:44:36.018+0000] {spark_submit.py:571} INFO - "batchId" : 19,
[2024-05-20T15:44:36.018+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:44:36.019+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 90.90909090909092,
[2024-05-20T15:44:36.019+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8583690987124464,
[2024-05-20T15:44:36.019+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-20T15:44:36.019+0000] {spark_submit.py:571} INFO - "addBatch" : 959,
[2024-05-20T15:44:36.020+0000] {spark_submit.py:571} INFO - "commitOffsets" : 53,
[2024-05-20T15:44:36.020+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-20T15:44:36.020+0000] {spark_submit.py:571} INFO - "latestOffset" : 1,
[2024-05-20T15:44:36.020+0000] {spark_submit.py:571} INFO - "queryPlanning" : 74,
[2024-05-20T15:44:36.020+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1165,
[2024-05-20T15:44:36.020+0000] {spark_submit.py:571} INFO - "walCommit" : 68
[2024-05-20T15:44:36.021+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:44:36.021+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-20T15:44:36.021+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-20T15:44:36.021+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-20T15:44:36.021+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-20T15:44:36.021+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:44:36.021+0000] {spark_submit.py:571} INFO - "0" : 18
[2024-05-20T15:44:36.021+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:44:36.022+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:44:36.022+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-20T15:44:36.022+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:44:36.022+0000] {spark_submit.py:571} INFO - "0" : 19
[2024-05-20T15:44:36.022+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:44:36.022+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:44:36.022+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-20T15:44:36.022+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:44:36.022+0000] {spark_submit.py:571} INFO - "0" : 19
[2024-05-20T15:44:36.023+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:44:36.023+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:44:36.023+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:44:36.023+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 90.90909090909092,
[2024-05-20T15:44:36.023+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8583690987124464,
[2024-05-20T15:44:36.023+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-20T15:44:36.023+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-20T15:44:36.023+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-20T15:44:36.024+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-20T15:44:36.024+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:44:36.024+0000] {spark_submit.py:571} INFO - } ],
[2024-05-20T15:44:36.024+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-20T15:44:36.024+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-20T15:44:36.024+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-20T15:44:36.025+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:44:36.025+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:44:46.029+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:44:48.545+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:48 INFO BlockManagerInfo: Removed broadcast_18_piece0 on f39758f400c5:39575 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:44:48.585+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:48 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.25.0.7:44267 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:44:56.041+0000] {spark_submit.py:571} INFO - 24/05/20 15:44:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:45:06.035+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:45:16.043+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:45:26.063+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:45:29.292+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/20 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.20.be76f3e1-4bc9-4fca-9f94-f716c627e014.tmp
[2024-05-20T15:45:29.347+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/.20.be76f3e1-4bc9-4fca-9f94-f716c627e014.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/offsets/20
[2024-05-20T15:45:29.347+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:29 INFO MicroBatchExecution: Committed offsets for batch 20. Metadata OffsetSeqMetadata(0,1716219929235,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-20T15:45:29.415+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:45:29.436+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:45:29.462+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:45:29.463+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-20T15:45:29.489+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-20T15:45:29.543+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:29 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-20T15:45:29.545+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:29 INFO DAGScheduler: Got job 20 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-20T15:45:29.546+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:29 INFO DAGScheduler: Final stage: ResultStage 19 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-20T15:45:29.546+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:29 INFO DAGScheduler: Parents of final stage: List()
[2024-05-20T15:45:29.547+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:29 INFO DAGScheduler: Missing parents: List()
[2024-05-20T15:45:29.548+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:29 INFO DAGScheduler: Submitting ResultStage 19 (PythonRDD[209] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-20T15:45:29.551+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:29 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-20T15:45:29.568+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:29 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 434.3 MiB)
[2024-05-20T15:45:29.569+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:29 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on f39758f400c5:39575 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:45:29.570+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:29 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585
[2024-05-20T15:45:29.572+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (PythonRDD[209] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-20T15:45:29.572+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:29 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2024-05-20T15:45:29.574+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:29 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (172.25.0.7, executor 0, partition 0, PROCESS_LOCAL, 14270 bytes)
[2024-05-20T15:45:29.643+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:29 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.25.0.7:44267 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:45:30.418+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:30 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 842 ms on 172.25.0.7 (executor 0) (1/1)
[2024-05-20T15:45:30.423+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:30 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2024-05-20T15:45:30.427+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:30 INFO DAGScheduler: ResultStage 19 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.870 s
[2024-05-20T15:45:30.428+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:30 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-20T15:45:30.429+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
[2024-05-20T15:45:30.432+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:30 INFO DAGScheduler: Job 20 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.879517 s
[2024-05-20T15:45:30.481+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/20 using temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.20.72eec224-b0e9-4ae6-a7f5-7af23ba503e1.tmp
[2024-05-20T15:45:30.531+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/.20.72eec224-b0e9-4ae6-a7f5-7af23ba503e1.tmp to file:/tmp/temporary-bb087726-89e6-455c-8341-62eaf11aa694/commits/20
[2024-05-20T15:45:30.535+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:30 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-20T15:45:30.535+0000] {spark_submit.py:571} INFO - "id" : "5f42858b-b652-4f45-a8c9-eeb4fd889226",
[2024-05-20T15:45:30.535+0000] {spark_submit.py:571} INFO - "runId" : "9cc76651-4fe1-43e8-a73c-096daec0f336",
[2024-05-20T15:45:30.535+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-20T15:45:30.535+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-20T15:45:29.234Z",
[2024-05-20T15:45:30.536+0000] {spark_submit.py:571} INFO - "batchId" : 20,
[2024-05-20T15:45:30.536+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:45:30.536+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-20T15:45:30.536+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7710100231303008,
[2024-05-20T15:45:30.536+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-20T15:45:30.536+0000] {spark_submit.py:571} INFO - "addBatch" : 996,
[2024-05-20T15:45:30.536+0000] {spark_submit.py:571} INFO - "commitOffsets" : 81,
[2024-05-20T15:45:30.537+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-20T15:45:30.537+0000] {spark_submit.py:571} INFO - "latestOffset" : 1,
[2024-05-20T15:45:30.537+0000] {spark_submit.py:571} INFO - "queryPlanning" : 94,
[2024-05-20T15:45:30.537+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1297,
[2024-05-20T15:45:30.537+0000] {spark_submit.py:571} INFO - "walCommit" : 113
[2024-05-20T15:45:30.537+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:45:30.538+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-20T15:45:30.538+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-20T15:45:30.538+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-20T15:45:30.538+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-20T15:45:30.538+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:45:30.538+0000] {spark_submit.py:571} INFO - "0" : 19
[2024-05-20T15:45:30.538+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:45:30.539+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:45:30.539+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-20T15:45:30.539+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:45:30.539+0000] {spark_submit.py:571} INFO - "0" : 20
[2024-05-20T15:45:30.539+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:45:30.539+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:45:30.539+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-20T15:45:30.539+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-20T15:45:30.539+0000] {spark_submit.py:571} INFO - "0" : 20
[2024-05-20T15:45:30.539+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:45:30.540+0000] {spark_submit.py:571} INFO - },
[2024-05-20T15:45:30.540+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-20T15:45:30.540+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-20T15:45:30.540+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7710100231303008,
[2024-05-20T15:45:30.540+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-20T15:45:30.540+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-20T15:45:30.540+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-20T15:45:30.540+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-20T15:45:30.541+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:45:30.541+0000] {spark_submit.py:571} INFO - } ],
[2024-05-20T15:45:30.541+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-20T15:45:30.541+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-20T15:45:30.541+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-20T15:45:30.541+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:45:30.541+0000] {spark_submit.py:571} INFO - }
[2024-05-20T15:45:40.573+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:45:50.566+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:45:52.791+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:52 INFO BlockManagerInfo: Removed broadcast_19_piece0 on f39758f400c5:39575 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:45:52.810+0000] {spark_submit.py:571} INFO - 24/05/20 15:45:52 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.25.0.7:44267 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-20T15:46:00.578+0000] {spark_submit.py:571} INFO - 24/05/20 15:46:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:46:10.588+0000] {spark_submit.py:571} INFO - 24/05/20 15:46:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:46:20.588+0000] {spark_submit.py:571} INFO - 24/05/20 15:46:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:46:30.589+0000] {spark_submit.py:571} INFO - 24/05/20 15:46:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:46:40.594+0000] {spark_submit.py:571} INFO - 24/05/20 15:46:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:46:50.592+0000] {spark_submit.py:571} INFO - 24/05/20 15:46:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:47:00.603+0000] {spark_submit.py:571} INFO - 24/05/20 15:47:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:47:10.612+0000] {spark_submit.py:571} INFO - 24/05/20 15:47:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:47:20.626+0000] {spark_submit.py:571} INFO - 24/05/20 15:47:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:47:30.631+0000] {spark_submit.py:571} INFO - 24/05/20 15:47:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:47:40.630+0000] {spark_submit.py:571} INFO - 24/05/20 15:47:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:47:50.646+0000] {spark_submit.py:571} INFO - 24/05/20 15:47:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:48:00.656+0000] {spark_submit.py:571} INFO - 24/05/20 15:48:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:48:10.661+0000] {spark_submit.py:571} INFO - 24/05/20 15:48:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:48:20.661+0000] {spark_submit.py:571} INFO - 24/05/20 15:48:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:48:30.671+0000] {spark_submit.py:571} INFO - 24/05/20 15:48:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:48:40.674+0000] {spark_submit.py:571} INFO - 24/05/20 15:48:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:48:50.678+0000] {spark_submit.py:571} INFO - 24/05/20 15:48:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:49:00.684+0000] {spark_submit.py:571} INFO - 24/05/20 15:49:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:49:10.694+0000] {spark_submit.py:571} INFO - 24/05/20 15:49:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:49:20.696+0000] {spark_submit.py:571} INFO - 24/05/20 15:49:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:49:30.698+0000] {spark_submit.py:571} INFO - 24/05/20 15:49:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:49:40.711+0000] {spark_submit.py:571} INFO - 24/05/20 15:49:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:49:50.714+0000] {spark_submit.py:571} INFO - 24/05/20 15:49:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:50:00.717+0000] {spark_submit.py:571} INFO - 24/05/20 15:50:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:50:10.725+0000] {spark_submit.py:571} INFO - 24/05/20 15:50:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:50:20.743+0000] {spark_submit.py:571} INFO - 24/05/20 15:50:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:50:30.746+0000] {spark_submit.py:571} INFO - 24/05/20 15:50:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:50:40.747+0000] {spark_submit.py:571} INFO - 24/05/20 15:50:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:50:50.758+0000] {spark_submit.py:571} INFO - 24/05/20 15:50:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:51:00.766+0000] {spark_submit.py:571} INFO - 24/05/20 15:51:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:51:10.771+0000] {spark_submit.py:571} INFO - 24/05/20 15:51:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:51:20.777+0000] {spark_submit.py:571} INFO - 24/05/20 15:51:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:51:30.781+0000] {spark_submit.py:571} INFO - 24/05/20 15:51:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:51:40.785+0000] {spark_submit.py:571} INFO - 24/05/20 15:51:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:51:50.792+0000] {spark_submit.py:571} INFO - 24/05/20 15:51:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:52:00.803+0000] {spark_submit.py:571} INFO - 24/05/20 15:52:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:52:10.808+0000] {spark_submit.py:571} INFO - 24/05/20 15:52:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:52:20.810+0000] {spark_submit.py:571} INFO - 24/05/20 15:52:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:52:30.824+0000] {spark_submit.py:571} INFO - 24/05/20 15:52:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:52:40.818+0000] {spark_submit.py:571} INFO - 24/05/20 15:52:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:52:50.821+0000] {spark_submit.py:571} INFO - 24/05/20 15:52:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:53:00.827+0000] {spark_submit.py:571} INFO - 24/05/20 15:53:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:53:10.830+0000] {spark_submit.py:571} INFO - 24/05/20 15:53:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:53:20.841+0000] {spark_submit.py:571} INFO - 24/05/20 15:53:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:53:30.849+0000] {spark_submit.py:571} INFO - 24/05/20 15:53:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:53:40.856+0000] {spark_submit.py:571} INFO - 24/05/20 15:53:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:53:50.870+0000] {spark_submit.py:571} INFO - 24/05/20 15:53:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:54:00.869+0000] {spark_submit.py:571} INFO - 24/05/20 15:54:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:54:10.884+0000] {spark_submit.py:571} INFO - 24/05/20 15:54:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:54:20.885+0000] {spark_submit.py:571} INFO - 24/05/20 15:54:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:54:30.892+0000] {spark_submit.py:571} INFO - 24/05/20 15:54:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:54:40.896+0000] {spark_submit.py:571} INFO - 24/05/20 15:54:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:54:50.902+0000] {spark_submit.py:571} INFO - 24/05/20 15:54:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:55:00.917+0000] {spark_submit.py:571} INFO - 24/05/20 15:55:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:55:10.915+0000] {spark_submit.py:571} INFO - 24/05/20 15:55:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:55:20.931+0000] {spark_submit.py:571} INFO - 24/05/20 15:55:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:55:30.936+0000] {spark_submit.py:571} INFO - 24/05/20 15:55:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:55:40.944+0000] {spark_submit.py:571} INFO - 24/05/20 15:55:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:55:50.945+0000] {spark_submit.py:571} INFO - 24/05/20 15:55:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:56:00.952+0000] {spark_submit.py:571} INFO - 24/05/20 15:56:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:56:10.953+0000] {spark_submit.py:571} INFO - 24/05/20 15:56:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:56:20.958+0000] {spark_submit.py:571} INFO - 24/05/20 15:56:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:56:30.957+0000] {spark_submit.py:571} INFO - 24/05/20 15:56:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:56:40.956+0000] {spark_submit.py:571} INFO - 24/05/20 15:56:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:56:50.969+0000] {spark_submit.py:571} INFO - 24/05/20 15:56:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:57:00.975+0000] {spark_submit.py:571} INFO - 24/05/20 15:57:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:57:10.995+0000] {spark_submit.py:571} INFO - 24/05/20 15:57:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:57:20.988+0000] {spark_submit.py:571} INFO - 24/05/20 15:57:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:57:31.003+0000] {spark_submit.py:571} INFO - 24/05/20 15:57:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:57:41.004+0000] {spark_submit.py:571} INFO - 24/05/20 15:57:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:57:51.030+0000] {spark_submit.py:571} INFO - 24/05/20 15:57:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:58:01.019+0000] {spark_submit.py:571} INFO - 24/05/20 15:58:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:58:11.018+0000] {spark_submit.py:571} INFO - 24/05/20 15:58:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:58:21.020+0000] {spark_submit.py:571} INFO - 24/05/20 15:58:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:58:31.018+0000] {spark_submit.py:571} INFO - 24/05/20 15:58:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:58:41.031+0000] {spark_submit.py:571} INFO - 24/05/20 15:58:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:58:51.035+0000] {spark_submit.py:571} INFO - 24/05/20 15:58:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:59:01.040+0000] {spark_submit.py:571} INFO - 24/05/20 15:59:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:59:11.050+0000] {spark_submit.py:571} INFO - 24/05/20 15:59:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:59:21.061+0000] {spark_submit.py:571} INFO - 24/05/20 15:59:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:59:31.067+0000] {spark_submit.py:571} INFO - 24/05/20 15:59:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:59:41.087+0000] {spark_submit.py:571} INFO - 24/05/20 15:59:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T15:59:51.094+0000] {spark_submit.py:571} INFO - 24/05/20 15:59:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:00:01.101+0000] {spark_submit.py:571} INFO - 24/05/20 16:00:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:00:11.121+0000] {spark_submit.py:571} INFO - 24/05/20 16:00:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:00:21.114+0000] {spark_submit.py:571} INFO - 24/05/20 16:00:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:00:31.121+0000] {spark_submit.py:571} INFO - 24/05/20 16:00:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:00:41.168+0000] {spark_submit.py:571} INFO - 24/05/20 16:00:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:00:51.154+0000] {spark_submit.py:571} INFO - 24/05/20 16:00:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:01:01.148+0000] {spark_submit.py:571} INFO - 24/05/20 16:01:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:01:11.167+0000] {spark_submit.py:571} INFO - 24/05/20 16:01:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:01:21.184+0000] {spark_submit.py:571} INFO - 24/05/20 16:01:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:01:31.205+0000] {spark_submit.py:571} INFO - 24/05/20 16:01:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:01:41.194+0000] {spark_submit.py:571} INFO - 24/05/20 16:01:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:01:51.201+0000] {spark_submit.py:571} INFO - 24/05/20 16:01:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:02:01.210+0000] {spark_submit.py:571} INFO - 24/05/20 16:02:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:02:11.226+0000] {spark_submit.py:571} INFO - 24/05/20 16:02:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:02:21.235+0000] {spark_submit.py:571} INFO - 24/05/20 16:02:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:02:31.239+0000] {spark_submit.py:571} INFO - 24/05/20 16:02:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:02:41.248+0000] {spark_submit.py:571} INFO - 24/05/20 16:02:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:02:51.250+0000] {spark_submit.py:571} INFO - 24/05/20 16:02:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:03:01.263+0000] {spark_submit.py:571} INFO - 24/05/20 16:03:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:03:11.420+0000] {spark_submit.py:571} INFO - 24/05/20 16:03:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:03:21.279+0000] {spark_submit.py:571} INFO - 24/05/20 16:03:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:03:31.286+0000] {spark_submit.py:571} INFO - 24/05/20 16:03:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:03:41.288+0000] {spark_submit.py:571} INFO - 24/05/20 16:03:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:03:51.306+0000] {spark_submit.py:571} INFO - 24/05/20 16:03:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:04:01.292+0000] {spark_submit.py:571} INFO - 24/05/20 16:04:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:04:11.403+0000] {spark_submit.py:571} INFO - 24/05/20 16:04:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:04:21.346+0000] {spark_submit.py:571} INFO - 24/05/20 16:04:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:04:31.347+0000] {spark_submit.py:571} INFO - 24/05/20 16:04:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:04:41.375+0000] {spark_submit.py:571} INFO - 24/05/20 16:04:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:04:51.364+0000] {spark_submit.py:571} INFO - 24/05/20 16:04:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:05:01.373+0000] {spark_submit.py:571} INFO - 24/05/20 16:05:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:05:11.488+0000] {spark_submit.py:571} INFO - 24/05/20 16:05:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:05:21.440+0000] {spark_submit.py:571} INFO - 24/05/20 16:05:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:05:31.447+0000] {spark_submit.py:571} INFO - 24/05/20 16:05:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:05:41.447+0000] {spark_submit.py:571} INFO - 24/05/20 16:05:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:05:51.461+0000] {spark_submit.py:571} INFO - 24/05/20 16:05:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:06:01.462+0000] {spark_submit.py:571} INFO - 24/05/20 16:06:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:06:11.463+0000] {spark_submit.py:571} INFO - 24/05/20 16:06:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:06:21.472+0000] {spark_submit.py:571} INFO - 24/05/20 16:06:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:06:31.474+0000] {spark_submit.py:571} INFO - 24/05/20 16:06:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:06:41.482+0000] {spark_submit.py:571} INFO - 24/05/20 16:06:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:06:51.501+0000] {spark_submit.py:571} INFO - 24/05/20 16:06:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:07:01.486+0000] {spark_submit.py:571} INFO - 24/05/20 16:07:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:07:11.496+0000] {spark_submit.py:571} INFO - 24/05/20 16:07:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:07:21.510+0000] {spark_submit.py:571} INFO - 24/05/20 16:07:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:07:31.510+0000] {spark_submit.py:571} INFO - 24/05/20 16:07:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:07:41.515+0000] {spark_submit.py:571} INFO - 24/05/20 16:07:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:07:51.511+0000] {spark_submit.py:571} INFO - 24/05/20 16:07:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:08:01.515+0000] {spark_submit.py:571} INFO - 24/05/20 16:08:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:08:11.516+0000] {spark_submit.py:571} INFO - 24/05/20 16:08:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:08:21.527+0000] {spark_submit.py:571} INFO - 24/05/20 16:08:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:08:31.538+0000] {spark_submit.py:571} INFO - 24/05/20 16:08:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:08:41.538+0000] {spark_submit.py:571} INFO - 24/05/20 16:08:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:08:51.546+0000] {spark_submit.py:571} INFO - 24/05/20 16:08:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:09:01.548+0000] {spark_submit.py:571} INFO - 24/05/20 16:09:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:09:11.549+0000] {spark_submit.py:571} INFO - 24/05/20 16:09:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:09:21.552+0000] {spark_submit.py:571} INFO - 24/05/20 16:09:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:09:31.565+0000] {spark_submit.py:571} INFO - 24/05/20 16:09:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:09:41.574+0000] {spark_submit.py:571} INFO - 24/05/20 16:09:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:09:51.589+0000] {spark_submit.py:571} INFO - 24/05/20 16:09:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:10:01.589+0000] {spark_submit.py:571} INFO - 24/05/20 16:10:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:10:11.595+0000] {spark_submit.py:571} INFO - 24/05/20 16:10:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:10:21.603+0000] {spark_submit.py:571} INFO - 24/05/20 16:10:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:10:31.611+0000] {spark_submit.py:571} INFO - 24/05/20 16:10:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:10:41.621+0000] {spark_submit.py:571} INFO - 24/05/20 16:10:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:10:51.623+0000] {spark_submit.py:571} INFO - 24/05/20 16:10:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:11:01.625+0000] {spark_submit.py:571} INFO - 24/05/20 16:11:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:11:11.635+0000] {spark_submit.py:571} INFO - 24/05/20 16:11:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:11:21.646+0000] {spark_submit.py:571} INFO - 24/05/20 16:11:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:11:31.647+0000] {spark_submit.py:571} INFO - 24/05/20 16:11:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:11:41.657+0000] {spark_submit.py:571} INFO - 24/05/20 16:11:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:11:51.658+0000] {spark_submit.py:571} INFO - 24/05/20 16:11:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:12:01.663+0000] {spark_submit.py:571} INFO - 24/05/20 16:12:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:12:11.667+0000] {spark_submit.py:571} INFO - 24/05/20 16:12:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:12:21.676+0000] {spark_submit.py:571} INFO - 24/05/20 16:12:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:12:31.688+0000] {spark_submit.py:571} INFO - 24/05/20 16:12:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:12:41.685+0000] {spark_submit.py:571} INFO - 24/05/20 16:12:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:12:51.691+0000] {spark_submit.py:571} INFO - 24/05/20 16:12:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:13:01.732+0000] {spark_submit.py:571} INFO - 24/05/20 16:13:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:13:11.723+0000] {spark_submit.py:571} INFO - 24/05/20 16:13:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:13:21.736+0000] {spark_submit.py:571} INFO - 24/05/20 16:13:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:13:31.736+0000] {spark_submit.py:571} INFO - 24/05/20 16:13:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:13:41.744+0000] {spark_submit.py:571} INFO - 24/05/20 16:13:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:13:51.748+0000] {spark_submit.py:571} INFO - 24/05/20 16:13:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:14:01.761+0000] {spark_submit.py:571} INFO - 24/05/20 16:14:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:14:11.771+0000] {spark_submit.py:571} INFO - 24/05/20 16:14:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:14:21.768+0000] {spark_submit.py:571} INFO - 24/05/20 16:14:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:14:31.778+0000] {spark_submit.py:571} INFO - 24/05/20 16:14:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:14:41.788+0000] {spark_submit.py:571} INFO - 24/05/20 16:14:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:14:51.790+0000] {spark_submit.py:571} INFO - 24/05/20 16:14:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:15:01.800+0000] {spark_submit.py:571} INFO - 24/05/20 16:15:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:15:11.807+0000] {spark_submit.py:571} INFO - 24/05/20 16:15:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:15:21.816+0000] {spark_submit.py:571} INFO - 24/05/20 16:15:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:15:31.832+0000] {spark_submit.py:571} INFO - 24/05/20 16:15:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:15:41.838+0000] {spark_submit.py:571} INFO - 24/05/20 16:15:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:15:51.842+0000] {spark_submit.py:571} INFO - 24/05/20 16:15:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:16:01.852+0000] {spark_submit.py:571} INFO - 24/05/20 16:16:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:16:11.864+0000] {spark_submit.py:571} INFO - 24/05/20 16:16:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:16:21.871+0000] {spark_submit.py:571} INFO - 24/05/20 16:16:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:16:31.873+0000] {spark_submit.py:571} INFO - 24/05/20 16:16:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:16:41.877+0000] {spark_submit.py:571} INFO - 24/05/20 16:16:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:16:51.888+0000] {spark_submit.py:571} INFO - 24/05/20 16:16:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:17:01.891+0000] {spark_submit.py:571} INFO - 24/05/20 16:17:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:17:11.899+0000] {spark_submit.py:571} INFO - 24/05/20 16:17:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:17:21.904+0000] {spark_submit.py:571} INFO - 24/05/20 16:17:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:17:31.917+0000] {spark_submit.py:571} INFO - 24/05/20 16:17:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:17:41.953+0000] {spark_submit.py:571} INFO - 24/05/20 16:17:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:17:51.953+0000] {spark_submit.py:571} INFO - 24/05/20 16:17:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:18:01.956+0000] {spark_submit.py:571} INFO - 24/05/20 16:18:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:18:11.969+0000] {spark_submit.py:571} INFO - 24/05/20 16:18:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:18:21.984+0000] {spark_submit.py:571} INFO - 24/05/20 16:18:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:18:31.986+0000] {spark_submit.py:571} INFO - 24/05/20 16:18:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:18:41.988+0000] {spark_submit.py:571} INFO - 24/05/20 16:18:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:18:52.009+0000] {spark_submit.py:571} INFO - 24/05/20 16:18:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:19:02.001+0000] {spark_submit.py:571} INFO - 24/05/20 16:19:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:19:12.014+0000] {spark_submit.py:571} INFO - 24/05/20 16:19:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:19:22.029+0000] {spark_submit.py:571} INFO - 24/05/20 16:19:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:19:32.026+0000] {spark_submit.py:571} INFO - 24/05/20 16:19:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:19:42.037+0000] {spark_submit.py:571} INFO - 24/05/20 16:19:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:19:52.049+0000] {spark_submit.py:571} INFO - 24/05/20 16:19:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:20:02.060+0000] {spark_submit.py:571} INFO - 24/05/20 16:20:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:20:12.070+0000] {spark_submit.py:571} INFO - 24/05/20 16:20:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:20:22.086+0000] {spark_submit.py:571} INFO - 24/05/20 16:20:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:20:32.088+0000] {spark_submit.py:571} INFO - 24/05/20 16:20:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:20:42.092+0000] {spark_submit.py:571} INFO - 24/05/20 16:20:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:20:52.091+0000] {spark_submit.py:571} INFO - 24/05/20 16:20:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:21:02.095+0000] {spark_submit.py:571} INFO - 24/05/20 16:21:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:21:12.105+0000] {spark_submit.py:571} INFO - 24/05/20 16:21:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:21:22.116+0000] {spark_submit.py:571} INFO - 24/05/20 16:21:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:21:32.124+0000] {spark_submit.py:571} INFO - 24/05/20 16:21:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:21:42.126+0000] {spark_submit.py:571} INFO - 24/05/20 16:21:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:21:52.127+0000] {spark_submit.py:571} INFO - 24/05/20 16:21:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:22:02.138+0000] {spark_submit.py:571} INFO - 24/05/20 16:22:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:22:12.144+0000] {spark_submit.py:571} INFO - 24/05/20 16:22:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:22:22.150+0000] {spark_submit.py:571} INFO - 24/05/20 16:22:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:22:32.149+0000] {spark_submit.py:571} INFO - 24/05/20 16:22:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:22:42.154+0000] {spark_submit.py:571} INFO - 24/05/20 16:22:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:22:52.161+0000] {spark_submit.py:571} INFO - 24/05/20 16:22:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:23:02.164+0000] {spark_submit.py:571} INFO - 24/05/20 16:23:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:23:12.189+0000] {spark_submit.py:571} INFO - 24/05/20 16:23:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:23:22.184+0000] {spark_submit.py:571} INFO - 24/05/20 16:23:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:23:32.184+0000] {spark_submit.py:571} INFO - 24/05/20 16:23:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:23:42.191+0000] {spark_submit.py:571} INFO - 24/05/20 16:23:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:23:52.202+0000] {spark_submit.py:571} INFO - 24/05/20 16:23:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:24:02.201+0000] {spark_submit.py:571} INFO - 24/05/20 16:24:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:24:12.206+0000] {spark_submit.py:571} INFO - 24/05/20 16:24:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:24:22.212+0000] {spark_submit.py:571} INFO - 24/05/20 16:24:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:24:32.222+0000] {spark_submit.py:571} INFO - 24/05/20 16:24:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:24:42.224+0000] {spark_submit.py:571} INFO - 24/05/20 16:24:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:24:52.235+0000] {spark_submit.py:571} INFO - 24/05/20 16:24:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:25:02.242+0000] {spark_submit.py:571} INFO - 24/05/20 16:25:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:25:12.332+0000] {spark_submit.py:571} INFO - 24/05/20 16:25:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:25:22.335+0000] {spark_submit.py:571} INFO - 24/05/20 16:25:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:25:32.339+0000] {spark_submit.py:571} INFO - 24/05/20 16:25:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:25:42.342+0000] {spark_submit.py:571} INFO - 24/05/20 16:25:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:25:52.354+0000] {spark_submit.py:571} INFO - 24/05/20 16:25:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:26:02.362+0000] {spark_submit.py:571} INFO - 24/05/20 16:26:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:26:12.376+0000] {spark_submit.py:571} INFO - 24/05/20 16:26:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:26:22.393+0000] {spark_submit.py:571} INFO - 24/05/20 16:26:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:26:32.393+0000] {spark_submit.py:571} INFO - 24/05/20 16:26:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:26:42.406+0000] {spark_submit.py:571} INFO - 24/05/20 16:26:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:26:52.405+0000] {spark_submit.py:571} INFO - 24/05/20 16:26:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:27:02.418+0000] {spark_submit.py:571} INFO - 24/05/20 16:27:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:27:12.412+0000] {spark_submit.py:571} INFO - 24/05/20 16:27:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:27:22.413+0000] {spark_submit.py:571} INFO - 24/05/20 16:27:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:27:32.420+0000] {spark_submit.py:571} INFO - 24/05/20 16:27:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:27:42.415+0000] {spark_submit.py:571} INFO - 24/05/20 16:27:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:27:52.419+0000] {spark_submit.py:571} INFO - 24/05/20 16:27:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:28:02.428+0000] {spark_submit.py:571} INFO - 24/05/20 16:28:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:28:12.433+0000] {spark_submit.py:571} INFO - 24/05/20 16:28:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:28:22.442+0000] {spark_submit.py:571} INFO - 24/05/20 16:28:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:28:32.439+0000] {spark_submit.py:571} INFO - 24/05/20 16:28:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:28:42.444+0000] {spark_submit.py:571} INFO - 24/05/20 16:28:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:28:52.459+0000] {spark_submit.py:571} INFO - 24/05/20 16:28:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:29:02.462+0000] {spark_submit.py:571} INFO - 24/05/20 16:29:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:29:12.463+0000] {spark_submit.py:571} INFO - 24/05/20 16:29:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:29:22.486+0000] {spark_submit.py:571} INFO - 24/05/20 16:29:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:29:32.489+0000] {spark_submit.py:571} INFO - 24/05/20 16:29:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:29:42.490+0000] {spark_submit.py:571} INFO - 24/05/20 16:29:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:29:52.497+0000] {spark_submit.py:571} INFO - 24/05/20 16:29:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:30:02.496+0000] {spark_submit.py:571} INFO - 24/05/20 16:30:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:30:12.520+0000] {spark_submit.py:571} INFO - 24/05/20 16:30:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:30:22.506+0000] {spark_submit.py:571} INFO - 24/05/20 16:30:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:30:32.518+0000] {spark_submit.py:571} INFO - 24/05/20 16:30:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:30:42.523+0000] {spark_submit.py:571} INFO - 24/05/20 16:30:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:30:52.530+0000] {spark_submit.py:571} INFO - 24/05/20 16:30:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:31:02.556+0000] {spark_submit.py:571} INFO - 24/05/20 16:31:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:31:12.540+0000] {spark_submit.py:571} INFO - 24/05/20 16:31:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:31:22.539+0000] {spark_submit.py:571} INFO - 24/05/20 16:31:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:31:32.544+0000] {spark_submit.py:571} INFO - 24/05/20 16:31:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:31:42.554+0000] {spark_submit.py:571} INFO - 24/05/20 16:31:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:31:52.557+0000] {spark_submit.py:571} INFO - 24/05/20 16:31:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:32:02.561+0000] {spark_submit.py:571} INFO - 24/05/20 16:32:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:32:12.596+0000] {spark_submit.py:571} INFO - 24/05/20 16:32:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:32:22.567+0000] {spark_submit.py:571} INFO - 24/05/20 16:32:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:32:32.574+0000] {spark_submit.py:571} INFO - 24/05/20 16:32:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:32:42.581+0000] {spark_submit.py:571} INFO - 24/05/20 16:32:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:32:52.610+0000] {spark_submit.py:571} INFO - 24/05/20 16:32:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:33:02.591+0000] {spark_submit.py:571} INFO - 24/05/20 16:33:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:33:12.603+0000] {spark_submit.py:571} INFO - 24/05/20 16:33:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:33:22.607+0000] {spark_submit.py:571} INFO - 24/05/20 16:33:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:33:32.605+0000] {spark_submit.py:571} INFO - 24/05/20 16:33:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:33:42.610+0000] {spark_submit.py:571} INFO - 24/05/20 16:33:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:33:52.616+0000] {spark_submit.py:571} INFO - 24/05/20 16:33:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:34:02.622+0000] {spark_submit.py:571} INFO - 24/05/20 16:34:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:34:12.634+0000] {spark_submit.py:571} INFO - 24/05/20 16:34:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:34:22.638+0000] {spark_submit.py:571} INFO - 24/05/20 16:34:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:34:32.648+0000] {spark_submit.py:571} INFO - 24/05/20 16:34:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:34:42.661+0000] {spark_submit.py:571} INFO - 24/05/20 16:34:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:34:52.664+0000] {spark_submit.py:571} INFO - 24/05/20 16:34:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:35:02.686+0000] {spark_submit.py:571} INFO - 24/05/20 16:35:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:35:12.703+0000] {spark_submit.py:571} INFO - 24/05/20 16:35:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:35:22.707+0000] {spark_submit.py:571} INFO - 24/05/20 16:35:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:35:32.711+0000] {spark_submit.py:571} INFO - 24/05/20 16:35:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:35:42.720+0000] {spark_submit.py:571} INFO - 24/05/20 16:35:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:35:52.727+0000] {spark_submit.py:571} INFO - 24/05/20 16:35:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:36:02.732+0000] {spark_submit.py:571} INFO - 24/05/20 16:36:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:36:12.742+0000] {spark_submit.py:571} INFO - 24/05/20 16:36:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:36:22.754+0000] {spark_submit.py:571} INFO - 24/05/20 16:36:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:36:32.775+0000] {spark_submit.py:571} INFO - 24/05/20 16:36:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:36:42.766+0000] {spark_submit.py:571} INFO - 24/05/20 16:36:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:36:52.796+0000] {spark_submit.py:571} INFO - 24/05/20 16:36:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:37:02.787+0000] {spark_submit.py:571} INFO - 24/05/20 16:37:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:37:12.810+0000] {spark_submit.py:571} INFO - 24/05/20 16:37:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:37:22.808+0000] {spark_submit.py:571} INFO - 24/05/20 16:37:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:37:32.808+0000] {spark_submit.py:571} INFO - 24/05/20 16:37:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:37:42.816+0000] {spark_submit.py:571} INFO - 24/05/20 16:37:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:37:52.815+0000] {spark_submit.py:571} INFO - 24/05/20 16:37:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:38:02.820+0000] {spark_submit.py:571} INFO - 24/05/20 16:38:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:38:12.830+0000] {spark_submit.py:571} INFO - 24/05/20 16:38:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:38:22.836+0000] {spark_submit.py:571} INFO - 24/05/20 16:38:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:38:32.840+0000] {spark_submit.py:571} INFO - 24/05/20 16:38:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:38:42.841+0000] {spark_submit.py:571} INFO - 24/05/20 16:38:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:38:52.850+0000] {spark_submit.py:571} INFO - 24/05/20 16:38:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:39:02.853+0000] {spark_submit.py:571} INFO - 24/05/20 16:39:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:39:12.862+0000] {spark_submit.py:571} INFO - 24/05/20 16:39:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:39:22.861+0000] {spark_submit.py:571} INFO - 24/05/20 16:39:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:39:32.865+0000] {spark_submit.py:571} INFO - 24/05/20 16:39:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:39:42.873+0000] {spark_submit.py:571} INFO - 24/05/20 16:39:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:39:52.873+0000] {spark_submit.py:571} INFO - 24/05/20 16:39:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:40:02.884+0000] {spark_submit.py:571} INFO - 24/05/20 16:40:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:40:12.892+0000] {spark_submit.py:571} INFO - 24/05/20 16:40:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:40:22.896+0000] {spark_submit.py:571} INFO - 24/05/20 16:40:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:40:32.931+0000] {spark_submit.py:571} INFO - 24/05/20 16:40:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:40:42.921+0000] {spark_submit.py:571} INFO - 24/05/20 16:40:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:40:52.924+0000] {spark_submit.py:571} INFO - 24/05/20 16:40:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:41:02.951+0000] {spark_submit.py:571} INFO - 24/05/20 16:41:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:41:12.951+0000] {spark_submit.py:571} INFO - 24/05/20 16:41:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:41:22.961+0000] {spark_submit.py:571} INFO - 24/05/20 16:41:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:41:32.963+0000] {spark_submit.py:571} INFO - 24/05/20 16:41:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:41:42.972+0000] {spark_submit.py:571} INFO - 24/05/20 16:41:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:41:52.973+0000] {spark_submit.py:571} INFO - 24/05/20 16:41:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:42:02.982+0000] {spark_submit.py:571} INFO - 24/05/20 16:42:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:42:12.985+0000] {spark_submit.py:571} INFO - 24/05/20 16:42:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:42:23.002+0000] {spark_submit.py:571} INFO - 24/05/20 16:42:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:42:33.010+0000] {spark_submit.py:571} INFO - 24/05/20 16:42:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:42:43.013+0000] {spark_submit.py:571} INFO - 24/05/20 16:42:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:42:53.023+0000] {spark_submit.py:571} INFO - 24/05/20 16:42:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:43:03.038+0000] {spark_submit.py:571} INFO - 24/05/20 16:43:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:43:13.038+0000] {spark_submit.py:571} INFO - 24/05/20 16:43:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:43:23.043+0000] {spark_submit.py:571} INFO - 24/05/20 16:43:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:43:33.055+0000] {spark_submit.py:571} INFO - 24/05/20 16:43:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:43:43.065+0000] {spark_submit.py:571} INFO - 24/05/20 16:43:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:43:53.068+0000] {spark_submit.py:571} INFO - 24/05/20 16:43:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:44:03.073+0000] {spark_submit.py:571} INFO - 24/05/20 16:44:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:44:13.086+0000] {spark_submit.py:571} INFO - 24/05/20 16:44:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:44:23.083+0000] {spark_submit.py:571} INFO - 24/05/20 16:44:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:44:33.087+0000] {spark_submit.py:571} INFO - 24/05/20 16:44:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:44:43.093+0000] {spark_submit.py:571} INFO - 24/05/20 16:44:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:44:53.101+0000] {spark_submit.py:571} INFO - 24/05/20 16:44:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:45:03.110+0000] {spark_submit.py:571} INFO - 24/05/20 16:45:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:45:13.119+0000] {spark_submit.py:571} INFO - 24/05/20 16:45:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:45:23.116+0000] {spark_submit.py:571} INFO - 24/05/20 16:45:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:45:33.138+0000] {spark_submit.py:571} INFO - 24/05/20 16:45:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:45:43.133+0000] {spark_submit.py:571} INFO - 24/05/20 16:45:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:45:53.129+0000] {spark_submit.py:571} INFO - 24/05/20 16:45:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:46:03.139+0000] {spark_submit.py:571} INFO - 24/05/20 16:46:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:46:13.138+0000] {spark_submit.py:571} INFO - 24/05/20 16:46:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:46:23.149+0000] {spark_submit.py:571} INFO - 24/05/20 16:46:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:46:33.160+0000] {spark_submit.py:571} INFO - 24/05/20 16:46:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:46:43.167+0000] {spark_submit.py:571} INFO - 24/05/20 16:46:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:46:53.177+0000] {spark_submit.py:571} INFO - 24/05/20 16:46:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:47:03.181+0000] {spark_submit.py:571} INFO - 24/05/20 16:47:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:47:13.197+0000] {spark_submit.py:571} INFO - 24/05/20 16:47:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:47:23.204+0000] {spark_submit.py:571} INFO - 24/05/20 16:47:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:47:33.203+0000] {spark_submit.py:571} INFO - 24/05/20 16:47:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:47:43.208+0000] {spark_submit.py:571} INFO - 24/05/20 16:47:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:47:53.217+0000] {spark_submit.py:571} INFO - 24/05/20 16:47:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:48:03.227+0000] {spark_submit.py:571} INFO - 24/05/20 16:48:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:48:13.232+0000] {spark_submit.py:571} INFO - 24/05/20 16:48:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:48:23.236+0000] {spark_submit.py:571} INFO - 24/05/20 16:48:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:48:33.257+0000] {spark_submit.py:571} INFO - 24/05/20 16:48:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:48:43.253+0000] {spark_submit.py:571} INFO - 24/05/20 16:48:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:48:53.270+0000] {spark_submit.py:571} INFO - 24/05/20 16:48:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:49:03.276+0000] {spark_submit.py:571} INFO - 24/05/20 16:49:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:49:13.286+0000] {spark_submit.py:571} INFO - 24/05/20 16:49:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:49:23.285+0000] {spark_submit.py:571} INFO - 24/05/20 16:49:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:49:33.312+0000] {spark_submit.py:571} INFO - 24/05/20 16:49:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:49:43.296+0000] {spark_submit.py:571} INFO - 24/05/20 16:49:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:49:53.300+0000] {spark_submit.py:571} INFO - 24/05/20 16:49:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:50:03.309+0000] {spark_submit.py:571} INFO - 24/05/20 16:50:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:50:13.316+0000] {spark_submit.py:571} INFO - 24/05/20 16:50:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:50:23.318+0000] {spark_submit.py:571} INFO - 24/05/20 16:50:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:50:33.318+0000] {spark_submit.py:571} INFO - 24/05/20 16:50:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:50:43.321+0000] {spark_submit.py:571} INFO - 24/05/20 16:50:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:50:53.324+0000] {spark_submit.py:571} INFO - 24/05/20 16:50:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:51:03.335+0000] {spark_submit.py:571} INFO - 24/05/20 16:51:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:51:13.340+0000] {spark_submit.py:571} INFO - 24/05/20 16:51:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:51:23.343+0000] {spark_submit.py:571} INFO - 24/05/20 16:51:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:51:33.361+0000] {spark_submit.py:571} INFO - 24/05/20 16:51:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:51:43.362+0000] {spark_submit.py:571} INFO - 24/05/20 16:51:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:51:53.359+0000] {spark_submit.py:571} INFO - 24/05/20 16:51:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:52:03.367+0000] {spark_submit.py:571} INFO - 24/05/20 16:52:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:52:13.374+0000] {spark_submit.py:571} INFO - 24/05/20 16:52:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:52:23.383+0000] {spark_submit.py:571} INFO - 24/05/20 16:52:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:52:33.395+0000] {spark_submit.py:571} INFO - 24/05/20 16:52:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:52:43.406+0000] {spark_submit.py:571} INFO - 24/05/20 16:52:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:52:53.416+0000] {spark_submit.py:571} INFO - 24/05/20 16:52:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:53:03.416+0000] {spark_submit.py:571} INFO - 24/05/20 16:53:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:53:13.418+0000] {spark_submit.py:571} INFO - 24/05/20 16:53:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:53:23.423+0000] {spark_submit.py:571} INFO - 24/05/20 16:53:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:53:33.424+0000] {spark_submit.py:571} INFO - 24/05/20 16:53:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:53:43.425+0000] {spark_submit.py:571} INFO - 24/05/20 16:53:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:53:53.430+0000] {spark_submit.py:571} INFO - 24/05/20 16:53:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:54:03.431+0000] {spark_submit.py:571} INFO - 24/05/20 16:54:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:54:13.435+0000] {spark_submit.py:571} INFO - 24/05/20 16:54:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:54:23.438+0000] {spark_submit.py:571} INFO - 24/05/20 16:54:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:54:33.450+0000] {spark_submit.py:571} INFO - 24/05/20 16:54:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:54:43.457+0000] {spark_submit.py:571} INFO - 24/05/20 16:54:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:54:53.454+0000] {spark_submit.py:571} INFO - 24/05/20 16:54:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:55:03.470+0000] {spark_submit.py:571} INFO - 24/05/20 16:55:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:55:13.472+0000] {spark_submit.py:571} INFO - 24/05/20 16:55:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:55:23.490+0000] {spark_submit.py:571} INFO - 24/05/20 16:55:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:55:33.488+0000] {spark_submit.py:571} INFO - 24/05/20 16:55:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:55:43.505+0000] {spark_submit.py:571} INFO - 24/05/20 16:55:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:55:53.516+0000] {spark_submit.py:571} INFO - 24/05/20 16:55:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:56:03.536+0000] {spark_submit.py:571} INFO - 24/05/20 16:56:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:56:13.521+0000] {spark_submit.py:571} INFO - 24/05/20 16:56:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:56:23.522+0000] {spark_submit.py:571} INFO - 24/05/20 16:56:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:56:33.529+0000] {spark_submit.py:571} INFO - 24/05/20 16:56:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:56:43.534+0000] {spark_submit.py:571} INFO - 24/05/20 16:56:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:56:53.534+0000] {spark_submit.py:571} INFO - 24/05/20 16:56:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:57:03.538+0000] {spark_submit.py:571} INFO - 24/05/20 16:57:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:57:13.544+0000] {spark_submit.py:571} INFO - 24/05/20 16:57:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:57:23.555+0000] {spark_submit.py:571} INFO - 24/05/20 16:57:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:57:33.564+0000] {spark_submit.py:571} INFO - 24/05/20 16:57:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:57:43.570+0000] {spark_submit.py:571} INFO - 24/05/20 16:57:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:57:53.568+0000] {spark_submit.py:571} INFO - 24/05/20 16:57:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:58:03.600+0000] {spark_submit.py:571} INFO - 24/05/20 16:58:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:58:13.595+0000] {spark_submit.py:571} INFO - 24/05/20 16:58:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:58:23.600+0000] {spark_submit.py:571} INFO - 24/05/20 16:58:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:58:33.608+0000] {spark_submit.py:571} INFO - 24/05/20 16:58:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:58:43.608+0000] {spark_submit.py:571} INFO - 24/05/20 16:58:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:58:53.636+0000] {spark_submit.py:571} INFO - 24/05/20 16:58:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:59:03.642+0000] {spark_submit.py:571} INFO - 24/05/20 16:59:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:59:13.644+0000] {spark_submit.py:571} INFO - 24/05/20 16:59:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:59:23.651+0000] {spark_submit.py:571} INFO - 24/05/20 16:59:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:59:33.652+0000] {spark_submit.py:571} INFO - 24/05/20 16:59:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:59:43.664+0000] {spark_submit.py:571} INFO - 24/05/20 16:59:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T16:59:53.676+0000] {spark_submit.py:571} INFO - 24/05/20 16:59:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:00:03.667+0000] {spark_submit.py:571} INFO - 24/05/20 17:00:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:00:13.677+0000] {spark_submit.py:571} INFO - 24/05/20 17:00:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:00:23.682+0000] {spark_submit.py:571} INFO - 24/05/20 17:00:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:00:33.691+0000] {spark_submit.py:571} INFO - 24/05/20 17:00:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:00:43.704+0000] {spark_submit.py:571} INFO - 24/05/20 17:00:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:00:53.712+0000] {spark_submit.py:571} INFO - 24/05/20 17:00:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:01:03.712+0000] {spark_submit.py:571} INFO - 24/05/20 17:01:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:01:13.726+0000] {spark_submit.py:571} INFO - 24/05/20 17:01:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:01:23.733+0000] {spark_submit.py:571} INFO - 24/05/20 17:01:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:01:33.738+0000] {spark_submit.py:571} INFO - 24/05/20 17:01:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:01:43.743+0000] {spark_submit.py:571} INFO - 24/05/20 17:01:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:01:53.750+0000] {spark_submit.py:571} INFO - 24/05/20 17:01:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:02:03.756+0000] {spark_submit.py:571} INFO - 24/05/20 17:02:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:02:13.760+0000] {spark_submit.py:571} INFO - 24/05/20 17:02:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:02:23.766+0000] {spark_submit.py:571} INFO - 24/05/20 17:02:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:02:33.786+0000] {spark_submit.py:571} INFO - 24/05/20 17:02:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:02:43.786+0000] {spark_submit.py:571} INFO - 24/05/20 17:02:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:02:53.791+0000] {spark_submit.py:571} INFO - 24/05/20 17:02:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:03:03.795+0000] {spark_submit.py:571} INFO - 24/05/20 17:03:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:03:13.814+0000] {spark_submit.py:571} INFO - 24/05/20 17:03:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:03:23.815+0000] {spark_submit.py:571} INFO - 24/05/20 17:03:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:03:33.818+0000] {spark_submit.py:571} INFO - 24/05/20 17:03:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:03:43.830+0000] {spark_submit.py:571} INFO - 24/05/20 17:03:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:03:53.834+0000] {spark_submit.py:571} INFO - 24/05/20 17:03:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:04:03.842+0000] {spark_submit.py:571} INFO - 24/05/20 17:04:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:04:13.844+0000] {spark_submit.py:571} INFO - 24/05/20 17:04:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:04:23.855+0000] {spark_submit.py:571} INFO - 24/05/20 17:04:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:04:33.858+0000] {spark_submit.py:571} INFO - 24/05/20 17:04:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:04:43.864+0000] {spark_submit.py:571} INFO - 24/05/20 17:04:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:04:53.862+0000] {spark_submit.py:571} INFO - 24/05/20 17:04:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:05:03.889+0000] {spark_submit.py:571} INFO - 24/05/20 17:05:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:05:13.879+0000] {spark_submit.py:571} INFO - 24/05/20 17:05:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:05:23.881+0000] {spark_submit.py:571} INFO - 24/05/20 17:05:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:05:33.890+0000] {spark_submit.py:571} INFO - 24/05/20 17:05:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:05:43.898+0000] {spark_submit.py:571} INFO - 24/05/20 17:05:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:05:53.909+0000] {spark_submit.py:571} INFO - 24/05/20 17:05:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:06:03.922+0000] {spark_submit.py:571} INFO - 24/05/20 17:06:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:06:13.930+0000] {spark_submit.py:571} INFO - 24/05/20 17:06:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:06:23.961+0000] {spark_submit.py:571} INFO - 24/05/20 17:06:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:06:33.962+0000] {spark_submit.py:571} INFO - 24/05/20 17:06:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:06:43.976+0000] {spark_submit.py:571} INFO - 24/05/20 17:06:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:06:53.977+0000] {spark_submit.py:571} INFO - 24/05/20 17:06:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:07:03.987+0000] {spark_submit.py:571} INFO - 24/05/20 17:07:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:07:13.998+0000] {spark_submit.py:571} INFO - 24/05/20 17:07:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:07:24.002+0000] {spark_submit.py:571} INFO - 24/05/20 17:07:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:07:34.005+0000] {spark_submit.py:571} INFO - 24/05/20 17:07:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:07:44.019+0000] {spark_submit.py:571} INFO - 24/05/20 17:07:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:07:54.028+0000] {spark_submit.py:571} INFO - 24/05/20 17:07:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:08:04.034+0000] {spark_submit.py:571} INFO - 24/05/20 17:08:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:08:14.048+0000] {spark_submit.py:571} INFO - 24/05/20 17:08:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:08:24.056+0000] {spark_submit.py:571} INFO - 24/05/20 17:08:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:08:34.060+0000] {spark_submit.py:571} INFO - 24/05/20 17:08:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:08:44.063+0000] {spark_submit.py:571} INFO - 24/05/20 17:08:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:08:54.064+0000] {spark_submit.py:571} INFO - 24/05/20 17:08:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:09:04.071+0000] {spark_submit.py:571} INFO - 24/05/20 17:09:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:09:14.082+0000] {spark_submit.py:571} INFO - 24/05/20 17:09:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:09:24.096+0000] {spark_submit.py:571} INFO - 24/05/20 17:09:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:09:34.096+0000] {spark_submit.py:571} INFO - 24/05/20 17:09:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:09:44.102+0000] {spark_submit.py:571} INFO - 24/05/20 17:09:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:09:54.131+0000] {spark_submit.py:571} INFO - 24/05/20 17:09:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:10:04.120+0000] {spark_submit.py:571} INFO - 24/05/20 17:10:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:10:14.127+0000] {spark_submit.py:571} INFO - 24/05/20 17:10:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:10:24.122+0000] {spark_submit.py:571} INFO - 24/05/20 17:10:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:10:34.126+0000] {spark_submit.py:571} INFO - 24/05/20 17:10:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:10:44.140+0000] {spark_submit.py:571} INFO - 24/05/20 17:10:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:10:54.158+0000] {spark_submit.py:571} INFO - 24/05/20 17:10:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:11:04.173+0000] {spark_submit.py:571} INFO - 24/05/20 17:11:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:11:14.184+0000] {spark_submit.py:571} INFO - 24/05/20 17:11:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:11:24.180+0000] {spark_submit.py:571} INFO - 24/05/20 17:11:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:11:34.192+0000] {spark_submit.py:571} INFO - 24/05/20 17:11:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:11:44.198+0000] {spark_submit.py:571} INFO - 24/05/20 17:11:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:11:54.210+0000] {spark_submit.py:571} INFO - 24/05/20 17:11:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:12:04.219+0000] {spark_submit.py:571} INFO - 24/05/20 17:12:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:12:14.225+0000] {spark_submit.py:571} INFO - 24/05/20 17:12:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:12:24.226+0000] {spark_submit.py:571} INFO - 24/05/20 17:12:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:12:34.229+0000] {spark_submit.py:571} INFO - 24/05/20 17:12:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:12:44.244+0000] {spark_submit.py:571} INFO - 24/05/20 17:12:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:12:54.247+0000] {spark_submit.py:571} INFO - 24/05/20 17:12:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:13:04.260+0000] {spark_submit.py:571} INFO - 24/05/20 17:13:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:13:14.266+0000] {spark_submit.py:571} INFO - 24/05/20 17:13:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:13:24.273+0000] {spark_submit.py:571} INFO - 24/05/20 17:13:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:13:34.278+0000] {spark_submit.py:571} INFO - 24/05/20 17:13:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:13:44.281+0000] {spark_submit.py:571} INFO - 24/05/20 17:13:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:13:54.302+0000] {spark_submit.py:571} INFO - 24/05/20 17:13:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:14:04.309+0000] {spark_submit.py:571} INFO - 24/05/20 17:14:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:14:14.311+0000] {spark_submit.py:571} INFO - 24/05/20 17:14:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:14:24.311+0000] {spark_submit.py:571} INFO - 24/05/20 17:14:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:14:34.319+0000] {spark_submit.py:571} INFO - 24/05/20 17:14:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:14:44.327+0000] {spark_submit.py:571} INFO - 24/05/20 17:14:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:14:54.346+0000] {spark_submit.py:571} INFO - 24/05/20 17:14:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:15:04.360+0000] {spark_submit.py:571} INFO - 24/05/20 17:15:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:15:14.373+0000] {spark_submit.py:571} INFO - 24/05/20 17:15:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:15:24.371+0000] {spark_submit.py:571} INFO - 24/05/20 17:15:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:15:34.369+0000] {spark_submit.py:571} INFO - 24/05/20 17:15:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:15:44.376+0000] {spark_submit.py:571} INFO - 24/05/20 17:15:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:15:54.378+0000] {spark_submit.py:571} INFO - 24/05/20 17:15:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:16:04.390+0000] {spark_submit.py:571} INFO - 24/05/20 17:16:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:16:14.389+0000] {spark_submit.py:571} INFO - 24/05/20 17:16:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:16:24.400+0000] {spark_submit.py:571} INFO - 24/05/20 17:16:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:16:34.409+0000] {spark_submit.py:571} INFO - 24/05/20 17:16:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:16:44.419+0000] {spark_submit.py:571} INFO - 24/05/20 17:16:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:16:54.420+0000] {spark_submit.py:571} INFO - 24/05/20 17:16:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-20T17:17:04.050+0000] {local_task_job_runner.py:310} WARNING - State of this instance has been externally set to success. Terminating instance.
[2024-05-20T17:17:04.087+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-05-20T17:17:04.109+0000] {process_utils.py:132} INFO - Sending 15 to group 345. PIDs of all processes in the group: [347, 432, 345]
[2024-05-20T17:17:04.111+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 345
[2024-05-20T17:17:04.118+0000] {taskinstance.py:2611} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-05-20T17:17:04.132+0000] {spark_submit.py:697} INFO - Sending kill signal to spark-submit
[2024-05-20T17:17:04.188+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-05-20T17:17:04.296+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=432, status='terminated', started='15:25:07') (432) terminated with exit code None
[2024-05-20T17:17:04.574+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=347, status='terminated', started='15:24:41') (347) terminated with exit code None
[2024-05-20T17:17:04.575+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=345, status='terminated', exitcode=0, started='15:24:41') (345) terminated with exit code 0
