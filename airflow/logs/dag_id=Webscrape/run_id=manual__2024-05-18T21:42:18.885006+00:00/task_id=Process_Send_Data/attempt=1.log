[2024-05-18T21:42:22.417+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-05-18T21:42:22.473+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: Webscrape.Process_Send_Data manual__2024-05-18T21:42:18.885006+00:00 [queued]>
[2024-05-18T21:42:22.492+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: Webscrape.Process_Send_Data manual__2024-05-18T21:42:18.885006+00:00 [queued]>
[2024-05-18T21:42:22.493+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 6
[2024-05-18T21:42:22.517+0000] {taskinstance.py:2330} INFO - Executing <Task(SparkSubmitOperator): Process_Send_Data> on 2024-05-18 21:42:18.885006+00:00
[2024-05-18T21:42:22.560+0000] {standard_task_runner.py:63} INFO - Started process 5543 to run task
[2024-05-18T21:42:22.569+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'Webscrape', 'Process_Send_Data', 'manual__2024-05-18T21:42:18.885006+00:00', '--job-id', '295', '--raw', '--subdir', 'DAGS_FOLDER/webscrapping.py', '--cfg-path', '/tmp/tmptq55zopy']
[2024-05-18T21:42:22.616+0000] {standard_task_runner.py:91} INFO - Job 295: Subtask Process_Send_Data
[2024-05-18T21:42:22.799+0000] {task_command.py:426} INFO - Running <TaskInstance: Webscrape.Process_Send_Data manual__2024-05-18T21:42:18.885006+00:00 [running]> on host f3c105d69d83
[2024-05-18T21:42:22.963+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='svc' AIRFLOW_CTX_DAG_ID='Webscrape' AIRFLOW_CTX_TASK_ID='Process_Send_Data' AIRFLOW_CTX_EXECUTION_DATE='2024-05-18T21:42:18.885006+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-05-18T21:42:18.885006+00:00'
[2024-05-18T21:42:22.964+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-05-18T21:42:23.003+0000] {base.py:84} INFO - Using connection ID 'spark_default' for task execution.
[2024-05-18T21:42:23.006+0000] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,com.datastax.spark:spark-cassandra-connector_2.12:3.5.0,org.apache.kafka:kafka-clients:3.5.1,commons-logging:commons-logging:1.2 --name arrow-spark --deploy-mode client dags/includes/jobs/spark_consumer.py
[2024-05-18T21:42:23.245+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.11/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-05-18T21:42:25.342+0000] {spark_submit.py:571} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-05-18T21:42:25.474+0000] {spark_submit.py:571} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2024-05-18T21:42:25.476+0000] {spark_submit.py:571} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2024-05-18T21:42:25.481+0000] {spark_submit.py:571} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2024-05-18T21:42:25.482+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency
[2024-05-18T21:42:25.483+0000] {spark_submit.py:571} INFO - org.apache.kafka#kafka-clients added as a dependency
[2024-05-18T21:42:25.484+0000] {spark_submit.py:571} INFO - commons-logging#commons-logging added as a dependency
[2024-05-18T21:42:25.485+0000] {spark_submit.py:571} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-124a9213-32f1-47c4-a388-7ec0e0848d28;1.0
[2024-05-18T21:42:25.486+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-05-18T21:42:25.686+0000] {spark_submit.py:571} INFO - found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
[2024-05-18T21:42:25.765+0000] {spark_submit.py:571} INFO - found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
[2024-05-18T21:42:25.863+0000] {spark_submit.py:571} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2024-05-18T21:42:25.931+0000] {spark_submit.py:571} INFO - found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2024-05-18T21:42:25.977+0000] {spark_submit.py:571} INFO - found org.xerial.snappy#snappy-java;1.1.10.3 in central
[2024-05-18T21:42:26.022+0000] {spark_submit.py:571} INFO - found org.slf4j#slf4j-api;2.0.7 in central
[2024-05-18T21:42:26.060+0000] {spark_submit.py:571} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2024-05-18T21:42:26.080+0000] {spark_submit.py:571} INFO - found org.apache.commons#commons-pool2;2.11.1 in central
[2024-05-18T21:42:26.102+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector_2.12;3.5.0 in central
[2024-05-18T21:42:26.122+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0 in central
[2024-05-18T21:42:26.161+0000] {spark_submit.py:571} INFO - found org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central
[2024-05-18T21:42:26.193+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-core-shaded;4.13.0 in central
[2024-05-18T21:42:26.230+0000] {spark_submit.py:571} INFO - found com.datastax.oss#native-protocol;1.5.0 in central
[2024-05-18T21:42:26.251+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central
[2024-05-18T21:42:26.268+0000] {spark_submit.py:571} INFO - found com.typesafe#config;1.4.1 in central
[2024-05-18T21:42:26.282+0000] {spark_submit.py:571} INFO - found io.dropwizard.metrics#metrics-core;4.1.18 in central
[2024-05-18T21:42:26.294+0000] {spark_submit.py:571} INFO - found org.hdrhistogram#HdrHistogram;2.1.12 in central
[2024-05-18T21:42:26.304+0000] {spark_submit.py:571} INFO - found org.reactivestreams#reactive-streams;1.0.3 in central
[2024-05-18T21:42:26.314+0000] {spark_submit.py:571} INFO - found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2024-05-18T21:42:26.328+0000] {spark_submit.py:571} INFO - found com.github.spotbugs#spotbugs-annotations;3.1.12 in central
[2024-05-18T21:42:26.339+0000] {spark_submit.py:571} INFO - found com.google.code.findbugs#jsr305;3.0.2 in central
[2024-05-18T21:42:26.352+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central
[2024-05-18T21:42:26.361+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-query-builder;4.13.0 in central
[2024-05-18T21:42:26.373+0000] {spark_submit.py:571} INFO - found org.apache.commons#commons-lang3;3.10 in central
[2024-05-18T21:42:26.421+0000] {spark_submit.py:571} INFO - found com.thoughtworks.paranamer#paranamer;2.8 in central
[2024-05-18T21:42:26.438+0000] {spark_submit.py:571} INFO - found org.scala-lang#scala-reflect;2.12.11 in central
[2024-05-18T21:42:26.460+0000] {spark_submit.py:571} INFO - found org.apache.kafka#kafka-clients;3.5.1 in central
[2024-05-18T21:42:26.473+0000] {spark_submit.py:571} INFO - found com.github.luben#zstd-jni;1.5.5-1 in central
[2024-05-18T21:42:26.488+0000] {spark_submit.py:571} INFO - found org.lz4#lz4-java;1.8.0 in central
[2024-05-18T21:42:26.507+0000] {spark_submit.py:571} INFO - found commons-logging#commons-logging;1.2 in central
[2024-05-18T21:42:26.607+0000] {spark_submit.py:571} INFO - :: resolution report :: resolve 1075ms :: artifacts dl 50ms
[2024-05-18T21:42:26.608+0000] {spark_submit.py:571} INFO - :: modules in use:
[2024-05-18T21:42:26.608+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]
[2024-05-18T21:42:26.609+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]
[2024-05-18T21:42:26.609+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]
[2024-05-18T21:42:26.610+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]
[2024-05-18T21:42:26.610+0000] {spark_submit.py:571} INFO - com.datastax.oss#native-protocol;1.5.0 from central in [default]
[2024-05-18T21:42:26.611+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0 from central in [default]
[2024-05-18T21:42:26.612+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12;3.5.0 from central in [default]
[2024-05-18T21:42:26.612+0000] {spark_submit.py:571} INFO - com.github.luben#zstd-jni;1.5.5-1 from central in [default]
[2024-05-18T21:42:26.612+0000] {spark_submit.py:571} INFO - com.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]
[2024-05-18T21:42:26.613+0000] {spark_submit.py:571} INFO - com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2024-05-18T21:42:26.613+0000] {spark_submit.py:571} INFO - com.google.code.findbugs#jsr305;3.0.2 from central in [default]
[2024-05-18T21:42:26.614+0000] {spark_submit.py:571} INFO - com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
[2024-05-18T21:42:26.614+0000] {spark_submit.py:571} INFO - com.typesafe#config;1.4.1 from central in [default]
[2024-05-18T21:42:26.615+0000] {spark_submit.py:571} INFO - commons-logging#commons-logging;1.2 from central in [default]
[2024-05-18T21:42:26.616+0000] {spark_submit.py:571} INFO - io.dropwizard.metrics#metrics-core;4.1.18 from central in [default]
[2024-05-18T21:42:26.617+0000] {spark_submit.py:571} INFO - org.apache.commons#commons-lang3;3.10 from central in [default]
[2024-05-18T21:42:26.617+0000] {spark_submit.py:571} INFO - org.apache.commons#commons-pool2;2.11.1 from central in [default]
[2024-05-18T21:42:26.618+0000] {spark_submit.py:571} INFO - org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
[2024-05-18T21:42:26.618+0000] {spark_submit.py:571} INFO - org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
[2024-05-18T21:42:26.619+0000] {spark_submit.py:571} INFO - org.apache.kafka#kafka-clients;3.5.1 from central in [default]
[2024-05-18T21:42:26.619+0000] {spark_submit.py:571} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
[2024-05-18T21:42:26.621+0000] {spark_submit.py:571} INFO - org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
[2024-05-18T21:42:26.621+0000] {spark_submit.py:571} INFO - org.hdrhistogram#HdrHistogram;2.1.12 from central in [default]
[2024-05-18T21:42:26.622+0000] {spark_submit.py:571} INFO - org.lz4#lz4-java;1.8.0 from central in [default]
[2024-05-18T21:42:26.622+0000] {spark_submit.py:571} INFO - org.reactivestreams#reactive-streams;1.0.3 from central in [default]
[2024-05-18T21:42:26.622+0000] {spark_submit.py:571} INFO - org.scala-lang#scala-reflect;2.12.11 from central in [default]
[2024-05-18T21:42:26.623+0000] {spark_submit.py:571} INFO - org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]
[2024-05-18T21:42:26.623+0000] {spark_submit.py:571} INFO - org.slf4j#slf4j-api;2.0.7 from central in [default]
[2024-05-18T21:42:26.624+0000] {spark_submit.py:571} INFO - org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
[2024-05-18T21:42:26.624+0000] {spark_submit.py:571} INFO - :: evicted modules:
[2024-05-18T21:42:26.625+0000] {spark_submit.py:571} INFO - org.apache.kafka#kafka-clients;3.4.1 by [org.apache.kafka#kafka-clients;3.5.1] in [default]
[2024-05-18T21:42:26.625+0000] {spark_submit.py:571} INFO - commons-logging#commons-logging;1.1.3 by [commons-logging#commons-logging;1.2] in [default]
[2024-05-18T21:42:26.626+0000] {spark_submit.py:571} INFO - com.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]
[2024-05-18T21:42:26.626+0000] {spark_submit.py:571} INFO - org.slf4j#slf4j-api;1.7.26 by [org.slf4j#slf4j-api;2.0.7] in [default]
[2024-05-18T21:42:26.627+0000] {spark_submit.py:571} INFO - org.xerial.snappy#snappy-java;1.1.10.1 by [org.xerial.snappy#snappy-java;1.1.10.3] in [default]
[2024-05-18T21:42:26.627+0000] {spark_submit.py:571} INFO - org.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.7] in [default]
[2024-05-18T21:42:26.627+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-05-18T21:42:26.628+0000] {spark_submit.py:571} INFO - |                  |            modules            ||   artifacts   |
[2024-05-18T21:42:26.628+0000] {spark_submit.py:571} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-05-18T21:42:26.630+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-05-18T21:42:26.630+0000] {spark_submit.py:571} INFO - |      default     |   35  |   0   |   0   |   6   ||   29  |   0   |
[2024-05-18T21:42:26.631+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-05-18T21:42:26.631+0000] {spark_submit.py:571} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-124a9213-32f1-47c4-a388-7ec0e0848d28
[2024-05-18T21:42:26.632+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-05-18T21:42:26.636+0000] {spark_submit.py:571} INFO - 0 artifacts copied, 29 already retrieved (0kB/12ms)
[2024-05-18T21:42:27.073+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-05-18T21:42:31.102+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:31 INFO SparkContext: Running Spark version 3.5.1
[2024-05-18T21:42:31.134+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:31 INFO SparkContext: OS info Linux, 6.6.26-linuxkit, aarch64
[2024-05-18T21:42:31.135+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:31 INFO SparkContext: Java version 11.0.23
[2024-05-18T21:42:31.191+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:31 INFO ResourceUtils: ==============================================================
[2024-05-18T21:42:31.191+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:31 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-05-18T21:42:31.191+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:31 INFO ResourceUtils: ==============================================================
[2024-05-18T21:42:31.192+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:31 INFO SparkContext: Submitted application: Redfin_Properties_Consumer
[2024-05-18T21:42:31.262+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-05-18T21:42:31.315+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:31 INFO ResourceProfile: Limiting resource is cpu
[2024-05-18T21:42:31.315+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:31 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-05-18T21:42:31.465+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:31 INFO SecurityManager: Changing view acls to: ***
[2024-05-18T21:42:31.467+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:31 INFO SecurityManager: Changing modify acls to: ***
[2024-05-18T21:42:31.468+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:31 INFO SecurityManager: Changing view acls groups to:
[2024-05-18T21:42:31.468+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:31 INFO SecurityManager: Changing modify acls groups to:
[2024-05-18T21:42:31.468+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-05-18T21:42:31.902+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:31 INFO Utils: Successfully started service 'sparkDriver' on port 33587.
[2024-05-18T21:42:31.964+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:31 INFO SparkEnv: Registering MapOutputTracker
[2024-05-18T21:42:32.004+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkEnv: Registering BlockManagerMaster
[2024-05-18T21:42:32.042+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-05-18T21:42:32.043+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-05-18T21:42:32.048+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-05-18T21:42:32.079+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-802e041b-c8ee-4620-b5dd-bb45cad83a62
[2024-05-18T21:42:32.096+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-05-18T21:42:32.112+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-05-18T21:42:32.295+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-05-18T21:42:32.376+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-05-18T21:42:32.416+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://f3c105d69d83:33587/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1716068551058
[2024-05-18T21:42:32.416+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar at spark://f3c105d69d83:33587/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1716068551058
[2024-05-18T21:42:32.417+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.5.1.jar at spark://f3c105d69d83:33587/jars/org.apache.kafka_kafka-clients-3.5.1.jar with timestamp 1716068551058
[2024-05-18T21:42:32.417+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.2.jar at spark://f3c105d69d83:33587/jars/commons-logging_commons-logging-1.2.jar with timestamp 1716068551058
[2024-05-18T21:42:32.417+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://f3c105d69d83:33587/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1716068551058
[2024-05-18T21:42:32.418+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://f3c105d69d83:33587/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1716068551058
[2024-05-18T21:42:32.418+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://f3c105d69d83:33587/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1716068551058
[2024-05-18T21:42:32.418+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://f3c105d69d83:33587/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1716068551058
[2024-05-18T21:42:32.418+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://f3c105d69d83:33587/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1716068551058
[2024-05-18T21:42:32.418+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://f3c105d69d83:33587/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1716068551058
[2024-05-18T21:42:32.418+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar at spark://f3c105d69d83:33587/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1716068551058
[2024-05-18T21:42:32.419+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://f3c105d69d83:33587/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1716068551058
[2024-05-18T21:42:32.419+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://f3c105d69d83:33587/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1716068551058
[2024-05-18T21:42:32.419+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://f3c105d69d83:33587/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1716068551058
[2024-05-18T21:42:32.420+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://f3c105d69d83:33587/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1716068551058
[2024-05-18T21:42:32.420+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://f3c105d69d83:33587/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1716068551058
[2024-05-18T21:42:32.420+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://f3c105d69d83:33587/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1716068551058
[2024-05-18T21:42:32.420+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://f3c105d69d83:33587/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1716068551058
[2024-05-18T21:42:32.420+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://f3c105d69d83:33587/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1716068551058
[2024-05-18T21:42:32.421+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://f3c105d69d83:33587/jars/com.typesafe_config-1.4.1.jar with timestamp 1716068551058
[2024-05-18T21:42:32.421+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://f3c105d69d83:33587/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1716068551058
[2024-05-18T21:42:32.421+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://f3c105d69d83:33587/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1716068551058
[2024-05-18T21:42:32.421+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://f3c105d69d83:33587/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1716068551058
[2024-05-18T21:42:32.421+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://f3c105d69d83:33587/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1716068551058
[2024-05-18T21:42:32.422+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://f3c105d69d83:33587/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1716068551058
[2024-05-18T21:42:32.422+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://f3c105d69d83:33587/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1716068551058
[2024-05-18T21:42:32.422+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://f3c105d69d83:33587/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1716068551058
[2024-05-18T21:42:32.422+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.luben_zstd-jni-1.5.5-1.jar at spark://f3c105d69d83:33587/jars/com.github.luben_zstd-jni-1.5.5-1.jar with timestamp 1716068551058
[2024-05-18T21:42:32.422+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://f3c105d69d83:33587/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1716068551058
[2024-05-18T21:42:32.424+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://f3c105d69d83:33587/files/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1716068551058
[2024-05-18T21:42:32.425+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
[2024-05-18T21:42:32.449+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar at spark://f3c105d69d83:33587/files/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar with timestamp 1716068551058
[2024-05-18T21:42:32.450+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/com.datastax.spark_spark-cassandra-connector_2.12-3.5.0.jar
[2024-05-18T21:42:32.490+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.5.1.jar at spark://f3c105d69d83:33587/files/org.apache.kafka_kafka-clients-3.5.1.jar with timestamp 1716068551058
[2024-05-18T21:42:32.491+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.5.1.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/org.apache.kafka_kafka-clients-3.5.1.jar
[2024-05-18T21:42:32.554+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.2.jar at spark://f3c105d69d83:33587/files/commons-logging_commons-logging-1.2.jar with timestamp 1716068551058
[2024-05-18T21:42:32.555+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO Utils: Copying /home/***/.ivy2/jars/commons-logging_commons-logging-1.2.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/commons-logging_commons-logging-1.2.jar
[2024-05-18T21:42:32.561+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://f3c105d69d83:33587/files/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1716068551058
[2024-05-18T21:42:32.561+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
[2024-05-18T21:42:32.568+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://f3c105d69d83:33587/files/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1716068551058
[2024-05-18T21:42:32.568+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/org.apache.commons_commons-pool2-2.11.1.jar
[2024-05-18T21:42:32.577+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://f3c105d69d83:33587/files/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1716068551058
[2024-05-18T21:42:32.577+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:32 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2024-05-18T21:42:33.033+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://f3c105d69d83:33587/files/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1716068551058
[2024-05-18T21:42:33.034+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2024-05-18T21:42:33.292+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://f3c105d69d83:33587/files/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1716068551058
[2024-05-18T21:42:33.292+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO Utils: Copying /home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2024-05-18T21:42:33.315+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://f3c105d69d83:33587/files/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1716068551058
[2024-05-18T21:42:33.316+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO Utils: Copying /home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/org.slf4j_slf4j-api-2.0.7.jar
[2024-05-18T21:42:33.319+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar at spark://f3c105d69d83:33587/files/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar with timestamp 1716068551058
[2024-05-18T21:42:33.323+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.5.0.jar
[2024-05-18T21:42:33.332+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://f3c105d69d83:33587/files/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1716068551058
[2024-05-18T21:42:33.333+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2024-05-18T21:42:33.343+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://f3c105d69d83:33587/files/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1716068551058
[2024-05-18T21:42:33.344+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2024-05-18T21:42:33.431+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://f3c105d69d83:33587/files/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1716068551058
[2024-05-18T21:42:33.432+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2024-05-18T21:42:33.435+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://f3c105d69d83:33587/files/org.apache.commons_commons-lang3-3.10.jar with timestamp 1716068551058
[2024-05-18T21:42:33.479+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/org.apache.commons_commons-lang3-3.10.jar
[2024-05-18T21:42:33.489+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://f3c105d69d83:33587/files/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1716068551058
[2024-05-18T21:42:33.492+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO Utils: Copying /home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/com.thoughtworks.paranamer_paranamer-2.8.jar
[2024-05-18T21:42:33.495+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://f3c105d69d83:33587/files/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1716068551058
[2024-05-18T21:42:33.495+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/org.scala-lang_scala-reflect-2.12.11.jar
[2024-05-18T21:42:33.541+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://f3c105d69d83:33587/files/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1716068551058
[2024-05-18T21:42:33.542+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/com.datastax.oss_native-protocol-1.5.0.jar
[2024-05-18T21:42:33.548+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://f3c105d69d83:33587/files/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1716068551058
[2024-05-18T21:42:33.548+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2024-05-18T21:42:33.584+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://f3c105d69d83:33587/files/com.typesafe_config-1.4.1.jar with timestamp 1716068551058
[2024-05-18T21:42:33.584+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO Utils: Copying /home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/com.typesafe_config-1.4.1.jar
[2024-05-18T21:42:33.592+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://f3c105d69d83:33587/files/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1716068551058
[2024-05-18T21:42:33.592+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO Utils: Copying /home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2024-05-18T21:42:33.597+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://f3c105d69d83:33587/files/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1716068551058
[2024-05-18T21:42:33.597+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO Utils: Copying /home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2024-05-18T21:42:33.602+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://f3c105d69d83:33587/files/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1716068551058
[2024-05-18T21:42:33.602+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO Utils: Copying /home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/org.reactivestreams_reactive-streams-1.0.3.jar
[2024-05-18T21:42:33.607+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://f3c105d69d83:33587/files/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1716068551058
[2024-05-18T21:42:33.607+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO Utils: Copying /home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2024-05-18T21:42:33.610+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://f3c105d69d83:33587/files/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1716068551058
[2024-05-18T21:42:33.611+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO Utils: Copying /home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2024-05-18T21:42:33.615+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://f3c105d69d83:33587/files/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1716068551058
[2024-05-18T21:42:33.617+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO Utils: Copying /home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/com.google.code.findbugs_jsr305-3.0.2.jar
[2024-05-18T21:42:33.620+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://f3c105d69d83:33587/files/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1716068551058
[2024-05-18T21:42:33.620+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2024-05-18T21:42:33.630+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.luben_zstd-jni-1.5.5-1.jar at spark://f3c105d69d83:33587/files/com.github.luben_zstd-jni-1.5.5-1.jar with timestamp 1716068551058
[2024-05-18T21:42:33.632+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO Utils: Copying /home/***/.ivy2/jars/com.github.luben_zstd-jni-1.5.5-1.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/com.github.luben_zstd-jni-1.5.5-1.jar
[2024-05-18T21:42:33.714+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://f3c105d69d83:33587/files/org.lz4_lz4-java-1.8.0.jar with timestamp 1716068551058
[2024-05-18T21:42:33.714+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO Utils: Copying /home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-19241334-03b6-4645-9413-6cb0f711ec5a/userFiles-ba8baa41-0b02-496e-9c58-8d18e971a87e/org.lz4_lz4-java-1.8.0.jar
[2024-05-18T21:42:33.865+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2024-05-18T21:42:33.914+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:33 INFO TransportClientFactory: Successfully created connection to spark-master/172.25.0.4:7077 after 25 ms (0 ms spent in bootstraps)
[2024-05-18T21:42:34.308+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:34 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240518214234-0014
[2024-05-18T21:42:34.322+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:34 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240518214234-0014/0 on worker-20240518201224-172.25.0.6-33123 (172.25.0.6:33123) with 1 core(s)
[2024-05-18T21:42:34.323+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38673.
[2024-05-18T21:42:34.325+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:34 INFO NettyBlockTransferService: Server created on f3c105d69d83:38673
[2024-05-18T21:42:34.326+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:34 INFO StandaloneSchedulerBackend: Granted executor ID app-20240518214234-0014/0 on hostPort 172.25.0.6:33123 with 1 core(s), 1024.0 MiB RAM
[2024-05-18T21:42:34.330+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-05-18T21:42:34.353+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f3c105d69d83, 38673, None)
[2024-05-18T21:42:34.358+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:34 INFO BlockManagerMasterEndpoint: Registering block manager f3c105d69d83:38673 with 434.4 MiB RAM, BlockManagerId(driver, f3c105d69d83, 38673, None)
[2024-05-18T21:42:34.363+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f3c105d69d83, 38673, None)
[2024-05-18T21:42:34.365+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f3c105d69d83, 38673, None)
[2024-05-18T21:42:34.725+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:34 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240518214234-0014/0 is now RUNNING
[2024-05-18T21:42:34.856+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:34 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-05-18T21:42:35.283+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:35 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-05-18T21:42:35.287+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:35 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2024-05-18T21:42:42.183+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:42 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.25.0.6:38554) with ID 0,  ResourceProfileId 0
[2024-05-18T21:42:42.300+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:42 INFO BlockManagerMasterEndpoint: Registering block manager 172.25.0.6:45173 with 434.4 MiB RAM, BlockManagerId(0, 172.25.0.6, 45173, None)
[2024-05-18T21:42:42.374+0000] {spark_submit.py:571} INFO - INFO:py4j.java_gateway:Callback Server Starting
[2024-05-18T21:42:42.375+0000] {spark_submit.py:571} INFO - INFO:py4j.java_gateway:Socket listening on ('127.0.0.1', 36107)
[2024-05-18T21:42:42.425+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:42 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2024-05-18T21:42:42.470+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:42 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
[2024-05-18T21:42:42.498+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:42 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693 resolved to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693.
[2024-05-18T21:42:42.498+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:42 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2024-05-18T21:42:42.634+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/metadata using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/.metadata.9d50b8c5-c1c5-467b-ad34-7e0d203148f7.tmp
[2024-05-18T21:42:42.784+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/.metadata.9d50b8c5-c1c5-467b-ad34-7e0d203148f7.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/metadata
[2024-05-18T21:42:42.844+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:42 INFO MicroBatchExecution: Starting [id = 7c05ded5-8d4c-4f2c-ac3c-99865592e6d1, runId = bced2123-102e-4a99-bbe9-1d6dba8f5896]. Use file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693 to store the query checkpoint.
[2024-05-18T21:42:42.855+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:42 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@5ed5f678] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@397dcbe8]
[2024-05-18T21:42:42.899+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:42 INFO OffsetSeqLog: BatchIds found from listing:
[2024-05-18T21:42:42.901+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:42 INFO OffsetSeqLog: BatchIds found from listing:
[2024-05-18T21:42:42.902+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:42 INFO MicroBatchExecution: Starting new streaming query.
[2024-05-18T21:42:42.928+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:42 INFO MicroBatchExecution: Stream started from {}
[2024-05-18T21:42:43.561+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:43 INFO AdminClientConfig: AdminClientConfig values:
[2024-05-18T21:42:43.565+0000] {spark_submit.py:571} INFO - auto.include.jmx.reporter = true
[2024-05-18T21:42:43.566+0000] {spark_submit.py:571} INFO - bootstrap.servers = [kafka:9092]
[2024-05-18T21:42:43.566+0000] {spark_submit.py:571} INFO - client.dns.lookup = use_all_dns_ips
[2024-05-18T21:42:43.566+0000] {spark_submit.py:571} INFO - client.id =
[2024-05-18T21:42:43.566+0000] {spark_submit.py:571} INFO - connections.max.idle.ms = 300000
[2024-05-18T21:42:43.566+0000] {spark_submit.py:571} INFO - default.api.timeout.ms = 60000
[2024-05-18T21:42:43.567+0000] {spark_submit.py:571} INFO - metadata.max.age.ms = 300000
[2024-05-18T21:42:43.567+0000] {spark_submit.py:571} INFO - metric.reporters = []
[2024-05-18T21:42:43.567+0000] {spark_submit.py:571} INFO - metrics.num.samples = 2
[2024-05-18T21:42:43.567+0000] {spark_submit.py:571} INFO - metrics.recording.level = INFO
[2024-05-18T21:42:43.567+0000] {spark_submit.py:571} INFO - metrics.sample.window.ms = 30000
[2024-05-18T21:42:43.568+0000] {spark_submit.py:571} INFO - receive.buffer.bytes = 65536
[2024-05-18T21:42:43.568+0000] {spark_submit.py:571} INFO - reconnect.backoff.max.ms = 1000
[2024-05-18T21:42:43.568+0000] {spark_submit.py:571} INFO - reconnect.backoff.ms = 50
[2024-05-18T21:42:43.568+0000] {spark_submit.py:571} INFO - request.timeout.ms = 30000
[2024-05-18T21:42:43.568+0000] {spark_submit.py:571} INFO - retries = 2147483647
[2024-05-18T21:42:43.569+0000] {spark_submit.py:571} INFO - retry.backoff.ms = 100
[2024-05-18T21:42:43.569+0000] {spark_submit.py:571} INFO - sasl.client.callback.handler.class = null
[2024-05-18T21:42:43.569+0000] {spark_submit.py:571} INFO - sasl.jaas.config = null
[2024-05-18T21:42:43.569+0000] {spark_submit.py:571} INFO - sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2024-05-18T21:42:43.569+0000] {spark_submit.py:571} INFO - sasl.kerberos.min.time.before.relogin = 60000
[2024-05-18T21:42:43.569+0000] {spark_submit.py:571} INFO - sasl.kerberos.service.name = null
[2024-05-18T21:42:43.570+0000] {spark_submit.py:571} INFO - sasl.kerberos.ticket.renew.jitter = 0.05
[2024-05-18T21:42:43.570+0000] {spark_submit.py:571} INFO - sasl.kerberos.ticket.renew.window.factor = 0.8
[2024-05-18T21:42:43.570+0000] {spark_submit.py:571} INFO - sasl.login.callback.handler.class = null
[2024-05-18T21:42:43.570+0000] {spark_submit.py:571} INFO - sasl.login.class = null
[2024-05-18T21:42:43.570+0000] {spark_submit.py:571} INFO - sasl.login.connect.timeout.ms = null
[2024-05-18T21:42:43.570+0000] {spark_submit.py:571} INFO - sasl.login.read.timeout.ms = null
[2024-05-18T21:42:43.571+0000] {spark_submit.py:571} INFO - sasl.login.refresh.buffer.seconds = 300
[2024-05-18T21:42:43.571+0000] {spark_submit.py:571} INFO - sasl.login.refresh.min.period.seconds = 60
[2024-05-18T21:42:43.572+0000] {spark_submit.py:571} INFO - sasl.login.refresh.window.factor = 0.8
[2024-05-18T21:42:43.572+0000] {spark_submit.py:571} INFO - sasl.login.refresh.window.jitter = 0.05
[2024-05-18T21:42:43.572+0000] {spark_submit.py:571} INFO - sasl.login.retry.backoff.max.ms = 10000
[2024-05-18T21:42:43.572+0000] {spark_submit.py:571} INFO - sasl.login.retry.backoff.ms = 100
[2024-05-18T21:42:43.572+0000] {spark_submit.py:571} INFO - sasl.mechanism = GSSAPI
[2024-05-18T21:42:43.572+0000] {spark_submit.py:571} INFO - sasl.oauthbearer.clock.skew.seconds = 30
[2024-05-18T21:42:43.573+0000] {spark_submit.py:571} INFO - sasl.oauthbearer.expected.audience = null
[2024-05-18T21:42:43.573+0000] {spark_submit.py:571} INFO - sasl.oauthbearer.expected.issuer = null
[2024-05-18T21:42:43.573+0000] {spark_submit.py:571} INFO - sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2024-05-18T21:42:43.574+0000] {spark_submit.py:571} INFO - sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2024-05-18T21:42:43.574+0000] {spark_submit.py:571} INFO - sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2024-05-18T21:42:43.574+0000] {spark_submit.py:571} INFO - sasl.oauthbearer.jwks.endpoint.url = null
[2024-05-18T21:42:43.574+0000] {spark_submit.py:571} INFO - sasl.oauthbearer.scope.claim.name = scope
[2024-05-18T21:42:43.574+0000] {spark_submit.py:571} INFO - sasl.oauthbearer.sub.claim.name = sub
[2024-05-18T21:42:43.574+0000] {spark_submit.py:571} INFO - sasl.oauthbearer.token.endpoint.url = null
[2024-05-18T21:42:43.574+0000] {spark_submit.py:571} INFO - security.protocol = PLAINTEXT
[2024-05-18T21:42:43.575+0000] {spark_submit.py:571} INFO - security.providers = null
[2024-05-18T21:42:43.575+0000] {spark_submit.py:571} INFO - send.buffer.bytes = 131072
[2024-05-18T21:42:43.575+0000] {spark_submit.py:571} INFO - socket.connection.setup.timeout.max.ms = 30000
[2024-05-18T21:42:43.575+0000] {spark_submit.py:571} INFO - socket.connection.setup.timeout.ms = 10000
[2024-05-18T21:42:43.575+0000] {spark_submit.py:571} INFO - ssl.cipher.suites = null
[2024-05-18T21:42:43.575+0000] {spark_submit.py:571} INFO - ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2024-05-18T21:42:43.576+0000] {spark_submit.py:571} INFO - ssl.endpoint.identification.algorithm = https
[2024-05-18T21:42:43.576+0000] {spark_submit.py:571} INFO - ssl.engine.factory.class = null
[2024-05-18T21:42:43.577+0000] {spark_submit.py:571} INFO - ssl.key.password = null
[2024-05-18T21:42:43.577+0000] {spark_submit.py:571} INFO - ssl.keymanager.algorithm = SunX509
[2024-05-18T21:42:43.578+0000] {spark_submit.py:571} INFO - ssl.keystore.certificate.chain = null
[2024-05-18T21:42:43.578+0000] {spark_submit.py:571} INFO - ssl.keystore.key = null
[2024-05-18T21:42:43.578+0000] {spark_submit.py:571} INFO - ssl.keystore.location = null
[2024-05-18T21:42:43.579+0000] {spark_submit.py:571} INFO - ssl.keystore.password = null
[2024-05-18T21:42:43.579+0000] {spark_submit.py:571} INFO - ssl.keystore.type = JKS
[2024-05-18T21:42:43.579+0000] {spark_submit.py:571} INFO - ssl.protocol = TLSv1.3
[2024-05-18T21:42:43.580+0000] {spark_submit.py:571} INFO - ssl.provider = null
[2024-05-18T21:42:43.580+0000] {spark_submit.py:571} INFO - ssl.secure.random.implementation = null
[2024-05-18T21:42:43.580+0000] {spark_submit.py:571} INFO - ssl.trustmanager.algorithm = PKIX
[2024-05-18T21:42:43.581+0000] {spark_submit.py:571} INFO - ssl.truststore.certificates = null
[2024-05-18T21:42:43.581+0000] {spark_submit.py:571} INFO - ssl.truststore.location = null
[2024-05-18T21:42:43.582+0000] {spark_submit.py:571} INFO - ssl.truststore.password = null
[2024-05-18T21:42:43.582+0000] {spark_submit.py:571} INFO - ssl.truststore.type = JKS
[2024-05-18T21:42:43.582+0000] {spark_submit.py:571} INFO - 
[2024-05-18T21:42:43.724+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:43 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2024-05-18T21:42:43.728+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:43 INFO AppInfoParser: Kafka version: 3.5.1
[2024-05-18T21:42:43.729+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:43 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2024-05-18T21:42:43.729+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:43 INFO AppInfoParser: Kafka startTimeMs: 1716068563723
[2024-05-18T21:42:44.729+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/sources/0/0 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/sources/0/.0.a82789b9-4898-443f-a328-a1110c4cda2f.tmp
[2024-05-18T21:42:44.782+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/sources/0/.0.a82789b9-4898-443f-a328-a1110c4cda2f.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/sources/0/0
[2024-05-18T21:42:44.783+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:44 INFO KafkaMicroBatchStream: Initial offsets: {"redfin_properties":{"0":0}}
[2024-05-18T21:42:44.823+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/0 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.0.517bdbdf-99a7-4055-99ce-d1b5536ba198.tmp
[2024-05-18T21:42:44.893+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.0.517bdbdf-99a7-4055-99ce-d1b5536ba198.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/0
[2024-05-18T21:42:44.896+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:44 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1716068564811,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:42:45.877+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:42:45.971+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:42:46.051+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:42:46.052+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:42:46.461+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:46 INFO CodeGenerator: Code generated in 247.410208 ms
[2024-05-18T21:42:46.577+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Python Server ready to receive messages
[2024-05-18T21:42:46.579+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:42:46.620+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:46 INFO CodeGenerator: Code generated in 21.505083 ms
[2024-05-18T21:42:46.805+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:46 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:42:46.817+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:46 INFO DAGScheduler: Got job 0 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:42:46.818+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:46 INFO DAGScheduler: Final stage: ResultStage 0 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:42:46.818+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:46 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:42:46.819+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:46 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:42:46.822+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:46 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[9] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:42:46.897+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:42:46.939+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:42:46.941+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:42:46.946+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:46 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:42:46.971+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[9] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:42:46.973+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-05-18T21:42:46.999+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:42:47.363+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:42:53.149+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:53 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 6153 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:42:53.153+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:53 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-05-18T21:42:53.159+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:53 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 49361
[2024-05-18T21:42:53.168+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:53 INFO DAGScheduler: ResultStage 0 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 6.334 s
[2024-05-18T21:42:53.176+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:53 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:42:53.178+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-05-18T21:42:53.191+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:53 INFO DAGScheduler: Job 0 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 6.385216 s
[2024-05-18T21:42:53.263+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/0 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.0.46aca26b-4e8c-445d-be36-8e045b1a0b5b.tmp
[2024-05-18T21:42:53.411+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.0.46aca26b-4e8c-445d-be36-8e045b1a0b5b.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/0
[2024-05-18T21:42:53.416+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:53 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:42:53.424+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:53 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:42:53.483+0000] {spark_submit.py:571} INFO - 24/05/18 21:42:53 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:42:53.485+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:42:53.486+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:42:53.487+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:42:53.487+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:42:42.891Z",
[2024-05-18T21:42:53.488+0000] {spark_submit.py:571} INFO - "batchId" : 0,
[2024-05-18T21:42:53.488+0000] {spark_submit.py:571} INFO - "numInputRows" : 3,
[2024-05-18T21:42:53.491+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 0.0,
[2024-05-18T21:42:53.491+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.285143997718848,
[2024-05-18T21:42:53.492+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:42:53.492+0000] {spark_submit.py:571} INFO - "addBatch" : 7227,
[2024-05-18T21:42:53.492+0000] {spark_submit.py:571} INFO - "commitOffsets" : 178,
[2024-05-18T21:42:53.493+0000] {spark_submit.py:571} INFO - "getBatch" : 26,
[2024-05-18T21:42:53.494+0000] {spark_submit.py:571} INFO - "latestOffset" : 1875,
[2024-05-18T21:42:53.495+0000] {spark_submit.py:571} INFO - "queryPlanning" : 1071,
[2024-05-18T21:42:53.496+0000] {spark_submit.py:571} INFO - "triggerExecution" : 10520,
[2024-05-18T21:42:53.496+0000] {spark_submit.py:571} INFO - "walCommit" : 79
[2024-05-18T21:42:53.497+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:42:53.498+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:42:53.499+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:42:53.500+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:42:53.501+0000] {spark_submit.py:571} INFO - "startOffset" : null,
[2024-05-18T21:42:53.501+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:42:53.502+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:42:53.503+0000] {spark_submit.py:571} INFO - "0" : 3
[2024-05-18T21:42:53.503+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:42:53.504+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:42:53.504+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:42:53.504+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:42:53.504+0000] {spark_submit.py:571} INFO - "0" : 3
[2024-05-18T21:42:53.505+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:42:53.506+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:42:53.506+0000] {spark_submit.py:571} INFO - "numInputRows" : 3,
[2024-05-18T21:42:53.506+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 0.0,
[2024-05-18T21:42:53.506+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.285143997718848,
[2024-05-18T21:42:53.506+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:42:53.506+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:42:53.507+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:42:53.507+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:42:53.507+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:42:53.507+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:42:53.508+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:42:53.508+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:42:53.508+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:42:53.508+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:42:53.508+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:43:03.464+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:43:10.762+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/1 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.1.f0702bf8-52e3-41cc-90f4-beec1b959d5f.tmp
[2024-05-18T21:43:10.819+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.1.f0702bf8-52e3-41cc-90f4-beec1b959d5f.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/1
[2024-05-18T21:43:10.820+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:10 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1716068590720,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:43:11.017+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:43:11.021+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:43:11.092+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:43:11.095+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:43:11.193+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:43:11.290+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:11 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:43:11.295+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:11 INFO DAGScheduler: Got job 1 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:43:11.296+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:11 INFO DAGScheduler: Final stage: ResultStage 1 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:43:11.296+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:11 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:43:11.297+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:11 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:43:11.298+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:11 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[19] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:43:11.303+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:11 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:43:11.307+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:11 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:43:11.308+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:43:11.309+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:11 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:43:11.309+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[19] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:43:11.309+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:11 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-05-18T21:43:11.312+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:11 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:43:11.410+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:43:12.205+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 894 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:43:12.207+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-05-18T21:43:12.211+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:12 INFO DAGScheduler: ResultStage 1 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.911 s
[2024-05-18T21:43:12.212+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:12 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:43:12.213+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-05-18T21:43:12.214+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:12 INFO DAGScheduler: Job 1 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.924234 s
[2024-05-18T21:43:12.242+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/1 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.1.89f4c6ef-f98b-4970-9b37-efd91d672087.tmp
[2024-05-18T21:43:12.298+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.1.89f4c6ef-f98b-4970-9b37-efd91d672087.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/1
[2024-05-18T21:43:12.305+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:12 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:43:12.305+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:43:12.305+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:43:12.305+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:43:12.306+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:43:10.711Z",
[2024-05-18T21:43:12.306+0000] {spark_submit.py:571} INFO - "batchId" : 1,
[2024-05-18T21:43:12.307+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:43:12.307+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 29.41176470588235,
[2024-05-18T21:43:12.307+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6297229219143576,
[2024-05-18T21:43:12.308+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:43:12.308+0000] {spark_submit.py:571} INFO - "addBatch" : 1189,
[2024-05-18T21:43:12.308+0000] {spark_submit.py:571} INFO - "commitOffsets" : 74,
[2024-05-18T21:43:12.308+0000] {spark_submit.py:571} INFO - "getBatch" : 2,
[2024-05-18T21:43:12.309+0000] {spark_submit.py:571} INFO - "latestOffset" : 8,
[2024-05-18T21:43:12.309+0000] {spark_submit.py:571} INFO - "queryPlanning" : 167,
[2024-05-18T21:43:12.309+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1588,
[2024-05-18T21:43:12.309+0000] {spark_submit.py:571} INFO - "walCommit" : 131
[2024-05-18T21:43:12.309+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:43:12.310+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:43:12.310+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:43:12.310+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:43:12.310+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:43:12.310+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:43:12.311+0000] {spark_submit.py:571} INFO - "0" : 3
[2024-05-18T21:43:12.311+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:43:12.311+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:43:12.311+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:43:12.311+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:43:12.311+0000] {spark_submit.py:571} INFO - "0" : 4
[2024-05-18T21:43:12.312+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:43:12.312+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:43:12.312+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:43:12.312+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:43:12.312+0000] {spark_submit.py:571} INFO - "0" : 4
[2024-05-18T21:43:12.312+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:43:12.313+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:43:12.313+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:43:12.313+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 29.41176470588235,
[2024-05-18T21:43:12.313+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6297229219143576,
[2024-05-18T21:43:12.313+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:43:12.313+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:43:12.313+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:43:12.314+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:43:12.314+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:43:12.314+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:43:12.314+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:43:12.314+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:43:12.314+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:43:12.314+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:43:12.315+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:43:18.289+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:18 INFO BlockManagerInfo: Removed broadcast_1_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:43:18.296+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:18 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:43:22.318+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:43:32.324+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:43:32.561+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/2 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.2.2bb8b34b-075c-43d0-b388-975058b4867d.tmp
[2024-05-18T21:43:32.601+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.2.2bb8b34b-075c-43d0-b388-975058b4867d.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/2
[2024-05-18T21:43:32.602+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:32 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1716068612525,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:43:32.669+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:43:32.672+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:43:32.700+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:43:32.701+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:43:32.735+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:43:32.862+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:32 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:43:32.865+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:32 INFO DAGScheduler: Got job 2 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:43:32.865+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:32 INFO DAGScheduler: Final stage: ResultStage 2 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:43:32.867+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:32 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:43:32.867+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:32 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:43:32.868+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:32 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[29] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:43:32.870+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:32 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:43:32.875+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:32 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:43:32.878+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:32 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:43:32.879+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:32 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:43:32.881+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (PythonRDD[29] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:43:32.883+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:32 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-05-18T21:43:32.890+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:32 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:43:32.990+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:32 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:43:33.788+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:33 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 898 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:43:33.793+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:33 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-05-18T21:43:33.796+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:33 INFO DAGScheduler: ResultStage 2 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.923 s
[2024-05-18T21:43:33.799+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:33 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:43:33.801+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-05-18T21:43:33.802+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:33 INFO DAGScheduler: Job 2 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.930029 s
[2024-05-18T21:43:33.846+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/2 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.2.7879a3b8-941c-4168-94eb-e0c38da19435.tmp
[2024-05-18T21:43:33.896+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.2.7879a3b8-941c-4168-94eb-e0c38da19435.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/2
[2024-05-18T21:43:33.900+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:33 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:43:33.901+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:43:33.901+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:43:33.901+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:43:33.901+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:43:32.523Z",
[2024-05-18T21:43:33.901+0000] {spark_submit.py:571} INFO - "batchId" : 2,
[2024-05-18T21:43:33.901+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:43:33.902+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:43:33.902+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7283321194464676,
[2024-05-18T21:43:33.902+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:43:33.902+0000] {spark_submit.py:571} INFO - "addBatch" : 1123,
[2024-05-18T21:43:33.902+0000] {spark_submit.py:571} INFO - "commitOffsets" : 90,
[2024-05-18T21:43:33.903+0000] {spark_submit.py:571} INFO - "getBatch" : 1,
[2024-05-18T21:43:33.903+0000] {spark_submit.py:571} INFO - "latestOffset" : 2,
[2024-05-18T21:43:33.903+0000] {spark_submit.py:571} INFO - "queryPlanning" : 72,
[2024-05-18T21:43:33.903+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1373,
[2024-05-18T21:43:33.903+0000] {spark_submit.py:571} INFO - "walCommit" : 77
[2024-05-18T21:43:33.903+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:43:33.903+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:43:33.904+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:43:33.904+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:43:33.904+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:43:33.905+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:43:33.905+0000] {spark_submit.py:571} INFO - "0" : 4
[2024-05-18T21:43:33.905+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:43:33.905+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:43:33.905+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:43:33.905+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:43:33.905+0000] {spark_submit.py:571} INFO - "0" : 5
[2024-05-18T21:43:33.906+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:43:33.906+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:43:33.906+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:43:33.906+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:43:33.906+0000] {spark_submit.py:571} INFO - "0" : 5
[2024-05-18T21:43:33.906+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:43:33.906+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:43:33.907+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:43:33.907+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:43:33.907+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7283321194464676,
[2024-05-18T21:43:33.907+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:43:33.907+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:43:33.907+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:43:33.907+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:43:33.907+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:43:33.907+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:43:33.908+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:43:33.908+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:43:33.908+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:43:33.908+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:43:33.908+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:43:43.915+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:43:46.188+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:46 INFO BlockManagerInfo: Removed broadcast_2_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:43:46.226+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:46 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:43:53.917+0000] {spark_submit.py:571} INFO - 24/05/18 21:43:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:44:00.934+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/3 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.3.e9ad9f0c-4369-4e4b-95ed-432384df336b.tmp
[2024-05-18T21:44:00.965+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.3.e9ad9f0c-4369-4e4b-95ed-432384df336b.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/3
[2024-05-18T21:44:00.966+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:00 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1716068640892,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:44:01.049+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:44:01.053+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:44:01.092+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:44:01.094+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:44:01.158+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:44:01.287+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:01 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:44:01.290+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:01 INFO DAGScheduler: Got job 3 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:44:01.291+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:01 INFO DAGScheduler: Final stage: ResultStage 3 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:44:01.292+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:01 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:44:01.293+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:01 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:44:01.298+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:01 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[39] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:44:01.304+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:01 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:44:01.306+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:01 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:44:01.307+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:01 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:44:01.308+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:01 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:44:01.309+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (PythonRDD[39] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:44:01.309+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:01 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2024-05-18T21:44:01.311+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:01 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:44:01.475+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:01 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:44:02.359+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:02 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1047 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:44:02.361+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:02 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2024-05-18T21:44:02.364+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:02 INFO DAGScheduler: ResultStage 3 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 1.067 s
[2024-05-18T21:44:02.367+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:02 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:44:02.368+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2024-05-18T21:44:02.368+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:02 INFO DAGScheduler: Job 3 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 1.077909 s
[2024-05-18T21:44:02.417+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/3 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.3.da203376-0332-44e9-89aa-3ac906629db2.tmp
[2024-05-18T21:44:02.446+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.3.da203376-0332-44e9-89aa-3ac906629db2.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/3
[2024-05-18T21:44:02.462+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:02 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:44:02.462+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:44:02.462+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:44:02.463+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:44:02.463+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:44:00.890Z",
[2024-05-18T21:44:02.463+0000] {spark_submit.py:571} INFO - "batchId" : 3,
[2024-05-18T21:44:02.463+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:44:02.464+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-18T21:44:02.464+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6422607578676943,
[2024-05-18T21:44:02.464+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:44:02.464+0000] {spark_submit.py:571} INFO - "addBatch" : 1321,
[2024-05-18T21:44:02.464+0000] {spark_submit.py:571} INFO - "commitOffsets" : 64,
[2024-05-18T21:44:02.464+0000] {spark_submit.py:571} INFO - "getBatch" : 1,
[2024-05-18T21:44:02.465+0000] {spark_submit.py:571} INFO - "latestOffset" : 2,
[2024-05-18T21:44:02.465+0000] {spark_submit.py:571} INFO - "queryPlanning" : 87,
[2024-05-18T21:44:02.465+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1557,
[2024-05-18T21:44:02.465+0000] {spark_submit.py:571} INFO - "walCommit" : 74
[2024-05-18T21:44:02.465+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:44:02.465+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:44:02.465+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:44:02.466+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:44:02.466+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:44:02.466+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:44:02.466+0000] {spark_submit.py:571} INFO - "0" : 5
[2024-05-18T21:44:02.466+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:44:02.466+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:44:02.466+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:44:02.466+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:44:02.467+0000] {spark_submit.py:571} INFO - "0" : 6
[2024-05-18T21:44:02.467+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:44:02.467+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:44:02.467+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:44:02.467+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:44:02.467+0000] {spark_submit.py:571} INFO - "0" : 6
[2024-05-18T21:44:02.467+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:44:02.467+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:44:02.468+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:44:02.468+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-18T21:44:02.468+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6422607578676943,
[2024-05-18T21:44:02.468+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:44:02.468+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:44:02.469+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:44:02.469+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:44:02.469+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:44:02.469+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:44:02.469+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:44:02.469+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:44:02.469+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:44:02.469+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:44:02.469+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:44:12.469+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:44:16.110+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:16 INFO BlockManagerInfo: Removed broadcast_3_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:44:16.136+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:16 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:44:22.473+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:44:32.482+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:44:42.515+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:44:50.424+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/4 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.4.5693dfdf-4f49-4a06-aa83-79664e0ae13b.tmp
[2024-05-18T21:44:50.514+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.4.5693dfdf-4f49-4a06-aa83-79664e0ae13b.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/4
[2024-05-18T21:44:50.515+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:50 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1716068690360,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:44:50.585+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:44:50.588+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:44:50.612+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:44:50.613+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:44:50.656+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:44:50.717+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:50 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:44:50.718+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:50 INFO DAGScheduler: Got job 4 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:44:50.719+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:50 INFO DAGScheduler: Final stage: ResultStage 4 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:44:50.720+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:50 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:44:50.721+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:50 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:44:50.721+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:50 INFO DAGScheduler: Submitting ResultStage 4 (PythonRDD[49] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:44:50.722+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:50 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:44:50.725+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:50 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:44:50.726+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:50 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:44:50.726+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:50 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:44:50.727+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (PythonRDD[49] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:44:50.727+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:50 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2024-05-18T21:44:50.729+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:50 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:44:50.790+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:50 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:44:51.609+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:51 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 878 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:44:51.612+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:51 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2024-05-18T21:44:51.613+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:51 INFO DAGScheduler: ResultStage 4 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.891 s
[2024-05-18T21:44:51.614+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:51 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:44:51.614+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2024-05-18T21:44:51.615+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:51 INFO DAGScheduler: Job 4 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.898043 s
[2024-05-18T21:44:51.734+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/4 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.4.7d80f25b-1199-4845-9afb-bf8796fc8a35.tmp
[2024-05-18T21:44:51.818+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.4.7d80f25b-1199-4845-9afb-bf8796fc8a35.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/4
[2024-05-18T21:44:51.822+0000] {spark_submit.py:571} INFO - 24/05/18 21:44:51 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:44:51.822+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:44:51.823+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:44:51.824+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:44:51.824+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:44:50.358Z",
[2024-05-18T21:44:51.825+0000] {spark_submit.py:571} INFO - "batchId" : 4,
[2024-05-18T21:44:51.825+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:44:51.826+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:44:51.827+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.684931506849315,
[2024-05-18T21:44:51.827+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:44:51.827+0000] {spark_submit.py:571} INFO - "addBatch" : 1032,
[2024-05-18T21:44:51.827+0000] {spark_submit.py:571} INFO - "commitOffsets" : 191,
[2024-05-18T21:44:51.828+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-18T21:44:51.828+0000] {spark_submit.py:571} INFO - "latestOffset" : 2,
[2024-05-18T21:44:51.828+0000] {spark_submit.py:571} INFO - "queryPlanning" : 71,
[2024-05-18T21:44:51.828+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1460,
[2024-05-18T21:44:51.828+0000] {spark_submit.py:571} INFO - "walCommit" : 154
[2024-05-18T21:44:51.829+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:44:51.829+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:44:51.829+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:44:51.829+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:44:51.829+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:44:51.829+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:44:51.830+0000] {spark_submit.py:571} INFO - "0" : 6
[2024-05-18T21:44:51.830+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:44:51.830+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:44:51.830+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:44:51.831+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:44:51.831+0000] {spark_submit.py:571} INFO - "0" : 7
[2024-05-18T21:44:51.831+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:44:51.832+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:44:51.832+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:44:51.832+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:44:51.832+0000] {spark_submit.py:571} INFO - "0" : 7
[2024-05-18T21:44:51.832+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:44:51.833+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:44:51.833+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:44:51.833+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:44:51.833+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.684931506849315,
[2024-05-18T21:44:51.833+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:44:51.834+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:44:51.834+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:44:51.834+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:44:51.834+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:44:51.835+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:44:51.835+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:44:51.835+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:44:51.835+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:44:51.835+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:44:51.836+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:45:01.828+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:45:11.832+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:45:17.034+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/5 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.5.62acc1e3-d7c6-4c3d-9a74-3fab790bd7b3.tmp
[2024-05-18T21:45:17.082+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.5.62acc1e3-d7c6-4c3d-9a74-3fab790bd7b3.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/5
[2024-05-18T21:45:17.083+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1716068717007,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:45:17.143+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:45:17.146+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:45:17.168+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:45:17.170+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:45:17.234+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:45:17.325+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO BlockManagerInfo: Removed broadcast_4_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:45:17.346+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:45:17.484+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:45:17.486+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO DAGScheduler: Got job 5 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:45:17.487+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO DAGScheduler: Final stage: ResultStage 5 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:45:17.488+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:45:17.492+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:45:17.493+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO DAGScheduler: Submitting ResultStage 5 (PythonRDD[59] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:45:17.512+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:45:17.521+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:45:17.526+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:45:17.527+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:45:17.528+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (PythonRDD[59] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:45:17.530+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2024-05-18T21:45:17.534+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:45:17.620+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:17 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:45:18.395+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:18 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 862 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:45:18.402+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:18 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2024-05-18T21:45:18.414+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:18 INFO DAGScheduler: ResultStage 5 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.915 s
[2024-05-18T21:45:18.416+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:18 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:45:18.417+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2024-05-18T21:45:18.421+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:18 INFO DAGScheduler: Job 5 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.932304 s
[2024-05-18T21:45:18.444+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/5 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.5.fc104996-b8a5-45b3-b413-e2459d666347.tmp
[2024-05-18T21:45:18.475+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.5.fc104996-b8a5-45b3-b413-e2459d666347.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/5
[2024-05-18T21:45:18.492+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:18 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:45:18.494+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:45:18.495+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:45:18.495+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:45:18.496+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:45:17.004Z",
[2024-05-18T21:45:18.496+0000] {spark_submit.py:571} INFO - "batchId" : 5,
[2024-05-18T21:45:18.496+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:45:18.497+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-18T21:45:18.497+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6798096532970768,
[2024-05-18T21:45:18.497+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:45:18.497+0000] {spark_submit.py:571} INFO - "addBatch" : 1276,
[2024-05-18T21:45:18.498+0000] {spark_submit.py:571} INFO - "commitOffsets" : 46,
[2024-05-18T21:45:18.498+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-18T21:45:18.498+0000] {spark_submit.py:571} INFO - "latestOffset" : 3,
[2024-05-18T21:45:18.498+0000] {spark_submit.py:571} INFO - "queryPlanning" : 63,
[2024-05-18T21:45:18.498+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1471,
[2024-05-18T21:45:18.498+0000] {spark_submit.py:571} INFO - "walCommit" : 76
[2024-05-18T21:45:18.498+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:45:18.498+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:45:18.499+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:45:18.499+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:45:18.499+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:45:18.499+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:45:18.499+0000] {spark_submit.py:571} INFO - "0" : 7
[2024-05-18T21:45:18.499+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:45:18.500+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:45:18.500+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:45:18.500+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:45:18.500+0000] {spark_submit.py:571} INFO - "0" : 8
[2024-05-18T21:45:18.500+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:45:18.500+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:45:18.500+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:45:18.501+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:45:18.501+0000] {spark_submit.py:571} INFO - "0" : 8
[2024-05-18T21:45:18.501+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:45:18.501+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:45:18.501+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:45:18.501+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-18T21:45:18.501+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6798096532970768,
[2024-05-18T21:45:18.501+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:45:18.501+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:45:18.502+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:45:18.502+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:45:18.502+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:45:18.502+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:45:18.502+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:45:18.502+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:45:18.502+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:45:18.503+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:45:18.503+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:45:28.490+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:45:38.507+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:45:40.344+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/6 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.6.dd527938-fe43-45bf-893f-79b93a9130c1.tmp
[2024-05-18T21:45:40.396+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.6.dd527938-fe43-45bf-893f-79b93a9130c1.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/6
[2024-05-18T21:45:40.397+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:40 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1716068740286,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:45:40.451+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:45:40.457+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:45:40.486+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:45:40.489+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:45:40.532+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:45:40.577+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:40 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:45:40.583+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:40 INFO DAGScheduler: Got job 6 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:45:40.585+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:40 INFO DAGScheduler: Final stage: ResultStage 6 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:45:40.586+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:40 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:45:40.588+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:40 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:45:40.588+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:40 INFO DAGScheduler: Submitting ResultStage 6 (PythonRDD[69] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:45:40.590+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:40 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 69.8 KiB, free 434.2 MiB)
[2024-05-18T21:45:40.604+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:40 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.2 MiB)
[2024-05-18T21:45:40.605+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:40 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:45:40.606+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:40 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:45:40.606+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (PythonRDD[69] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:45:40.607+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:40 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2024-05-18T21:45:40.609+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:40 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:45:40.670+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:40 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:45:41.389+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:41 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 780 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:45:41.391+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:41 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2024-05-18T21:45:41.393+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:41 INFO DAGScheduler: ResultStage 6 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.804 s
[2024-05-18T21:45:41.394+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:41 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:45:41.396+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2024-05-18T21:45:41.397+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:41 INFO DAGScheduler: Job 6 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.817303 s
[2024-05-18T21:45:41.418+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/6 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.6.12bae85a-5d02-46f4-a0a8-8f2f89202e7f.tmp
[2024-05-18T21:45:41.457+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.6.12bae85a-5d02-46f4-a0a8-8f2f89202e7f.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/6
[2024-05-18T21:45:41.460+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:41 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:45:41.460+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:45:41.460+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:45:41.460+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:45:41.461+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:45:40.284Z",
[2024-05-18T21:45:41.461+0000] {spark_submit.py:571} INFO - "batchId" : 6,
[2024-05-18T21:45:41.461+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:45:41.461+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 66.66666666666667,
[2024-05-18T21:45:41.461+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8525149190110827,
[2024-05-18T21:45:41.462+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:45:41.462+0000] {spark_submit.py:571} INFO - "addBatch" : 938,
[2024-05-18T21:45:41.462+0000] {spark_submit.py:571} INFO - "commitOffsets" : 53,
[2024-05-18T21:45:41.462+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-18T21:45:41.462+0000] {spark_submit.py:571} INFO - "latestOffset" : 2,
[2024-05-18T21:45:41.463+0000] {spark_submit.py:571} INFO - "queryPlanning" : 62,
[2024-05-18T21:45:41.463+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1173,
[2024-05-18T21:45:41.463+0000] {spark_submit.py:571} INFO - "walCommit" : 111
[2024-05-18T21:45:41.463+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:45:41.463+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:45:41.463+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:45:41.463+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:45:41.463+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:45:41.464+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:45:41.464+0000] {spark_submit.py:571} INFO - "0" : 8
[2024-05-18T21:45:41.464+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:45:41.464+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:45:41.464+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:45:41.464+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:45:41.464+0000] {spark_submit.py:571} INFO - "0" : 9
[2024-05-18T21:45:41.464+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:45:41.464+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:45:41.465+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:45:41.465+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:45:41.465+0000] {spark_submit.py:571} INFO - "0" : 9
[2024-05-18T21:45:41.465+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:45:41.465+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:45:41.465+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:45:41.465+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 66.66666666666667,
[2024-05-18T21:45:41.466+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8525149190110827,
[2024-05-18T21:45:41.466+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:45:41.466+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:45:41.466+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:45:41.466+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:45:41.466+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:45:41.466+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:45:41.466+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:45:41.466+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:45:41.467+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:45:41.467+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:45:41.467+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:45:46.726+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:46 INFO BlockManagerInfo: Removed broadcast_6_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:45:46.733+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:46 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:45:46.739+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:46 INFO BlockManagerInfo: Removed broadcast_5_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:45:46.741+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:46 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:45:51.470+0000] {spark_submit.py:571} INFO - 24/05/18 21:45:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:46:01.477+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:46:11.488+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/7 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.7.03e86af5-e0f8-4975-9668-e7a57d978f09.tmp
[2024-05-18T21:46:11.565+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.7.03e86af5-e0f8-4975-9668-e7a57d978f09.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/7
[2024-05-18T21:46:11.572+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:11 INFO MicroBatchExecution: Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1716068771429,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:46:11.675+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:46:11.679+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:46:11.746+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:46:11.769+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:46:11.835+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:46:11.924+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:11 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:46:11.935+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:11 INFO DAGScheduler: Got job 7 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:46:11.938+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:11 INFO DAGScheduler: Final stage: ResultStage 7 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:46:11.941+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:11 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:46:11.942+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:11 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:46:11.943+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:11 INFO DAGScheduler: Submitting ResultStage 7 (PythonRDD[79] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:46:11.953+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:11 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:46:11.957+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:11 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:46:11.958+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:11 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:46:11.958+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:11 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:46:11.959+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (PythonRDD[79] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:46:11.960+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:11 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2024-05-18T21:46:11.963+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:11 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:46:12.074+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:46:12.938+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:12 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 974 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:46:12.941+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:12 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2024-05-18T21:46:12.943+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:12 INFO DAGScheduler: ResultStage 7 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.994 s
[2024-05-18T21:46:12.944+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:12 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:46:12.945+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2024-05-18T21:46:12.945+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:12 INFO DAGScheduler: Job 7 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 1.020214 s
[2024-05-18T21:46:13.024+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:13 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/7 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.7.038fd010-b3a8-4405-848f-6713861aa774.tmp
[2024-05-18T21:46:13.076+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:13 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.7.038fd010-b3a8-4405-848f-6713861aa774.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/7
[2024-05-18T21:46:13.082+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:13 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:46:13.083+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:46:13.084+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:46:13.085+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:46:13.086+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:46:11.426Z",
[2024-05-18T21:46:13.086+0000] {spark_submit.py:571} INFO - "batchId" : 7,
[2024-05-18T21:46:13.086+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:46:13.086+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 62.5,
[2024-05-18T21:46:13.086+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6060606060606061,
[2024-05-18T21:46:13.087+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:46:13.087+0000] {spark_submit.py:571} INFO - "addBatch" : 1277,
[2024-05-18T21:46:13.087+0000] {spark_submit.py:571} INFO - "commitOffsets" : 98,
[2024-05-18T21:46:13.087+0000] {spark_submit.py:571} INFO - "getBatch" : 3,
[2024-05-18T21:46:13.088+0000] {spark_submit.py:571} INFO - "latestOffset" : 3,
[2024-05-18T21:46:13.088+0000] {spark_submit.py:571} INFO - "queryPlanning" : 103,
[2024-05-18T21:46:13.088+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1650,
[2024-05-18T21:46:13.088+0000] {spark_submit.py:571} INFO - "walCommit" : 143
[2024-05-18T21:46:13.088+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:46:13.088+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:46:13.089+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:46:13.089+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:46:13.089+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:46:13.089+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:46:13.089+0000] {spark_submit.py:571} INFO - "0" : 9
[2024-05-18T21:46:13.089+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:46:13.089+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:46:13.089+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:46:13.089+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:46:13.090+0000] {spark_submit.py:571} INFO - "0" : 10
[2024-05-18T21:46:13.090+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:46:13.090+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:46:13.090+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:46:13.090+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:46:13.090+0000] {spark_submit.py:571} INFO - "0" : 10
[2024-05-18T21:46:13.090+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:46:13.090+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:46:13.090+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:46:13.091+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 62.5,
[2024-05-18T21:46:13.091+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6060606060606061,
[2024-05-18T21:46:13.091+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:46:13.091+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:46:13.091+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:46:13.091+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:46:13.091+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:46:13.091+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:46:13.091+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:46:13.091+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:46:13.092+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:46:13.092+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:46:13.092+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:46:17.116+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:17 INFO BlockManagerInfo: Removed broadcast_7_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:46:17.121+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:17 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:46:23.085+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:46:33.093+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:46:40.122+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/8 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.8.04f45125-d756-4f65-8873-2de92b88815f.tmp
[2024-05-18T21:46:40.192+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.8.04f45125-d756-4f65-8873-2de92b88815f.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/8
[2024-05-18T21:46:40.195+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:40 INFO MicroBatchExecution: Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1716068800067,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:46:40.322+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:46:40.326+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:46:40.395+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:46:40.400+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:46:40.465+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:46:40.546+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:40 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:46:40.548+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:40 INFO DAGScheduler: Got job 8 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:46:40.549+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:40 INFO DAGScheduler: Final stage: ResultStage 8 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:46:40.549+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:40 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:46:40.554+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:40 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:46:40.555+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:40 INFO DAGScheduler: Submitting ResultStage 8 (PythonRDD[89] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:46:40.556+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:40 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:46:40.559+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:40 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 434.3 MiB)
[2024-05-18T21:46:40.560+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:40 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on f3c105d69d83:38673 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-18T21:46:40.561+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:40 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:46:40.562+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (PythonRDD[89] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:46:40.562+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:40 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2024-05-18T21:46:40.565+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:40 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:46:40.677+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:40 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.25.0.6:45173 (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-18T21:46:41.494+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:41 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 929 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:46:41.497+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:41 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2024-05-18T21:46:41.510+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:41 INFO DAGScheduler: ResultStage 8 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.953 s
[2024-05-18T21:46:41.515+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:41 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:46:41.516+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2024-05-18T21:46:41.519+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:41 INFO DAGScheduler: Job 8 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.971809 s
[2024-05-18T21:46:41.631+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/8 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.8.a60287ba-182b-472d-8182-9c0b8754ab09.tmp
[2024-05-18T21:46:41.700+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.8.a60287ba-182b-472d-8182-9c0b8754ab09.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/8
[2024-05-18T21:46:41.706+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:41 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:46:41.706+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:46:41.707+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:46:41.708+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:46:41.709+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:46:40.064Z",
[2024-05-18T21:46:41.709+0000] {spark_submit.py:571} INFO - "batchId" : 8,
[2024-05-18T21:46:41.710+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:46:41.710+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-18T21:46:41.711+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6112469437652812,
[2024-05-18T21:46:41.711+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:46:41.711+0000] {spark_submit.py:571} INFO - "addBatch" : 1232,
[2024-05-18T21:46:41.711+0000] {spark_submit.py:571} INFO - "commitOffsets" : 129,
[2024-05-18T21:46:41.712+0000] {spark_submit.py:571} INFO - "getBatch" : 1,
[2024-05-18T21:46:41.712+0000] {spark_submit.py:571} INFO - "latestOffset" : 3,
[2024-05-18T21:46:41.713+0000] {spark_submit.py:571} INFO - "queryPlanning" : 130,
[2024-05-18T21:46:41.713+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1636,
[2024-05-18T21:46:41.713+0000] {spark_submit.py:571} INFO - "walCommit" : 128
[2024-05-18T21:46:41.713+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:46:41.713+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:46:41.714+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:46:41.714+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:46:41.714+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:46:41.714+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:46:41.714+0000] {spark_submit.py:571} INFO - "0" : 10
[2024-05-18T21:46:41.715+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:46:41.715+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:46:41.715+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:46:41.716+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:46:41.716+0000] {spark_submit.py:571} INFO - "0" : 11
[2024-05-18T21:46:41.716+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:46:41.716+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:46:41.717+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:46:41.717+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:46:41.717+0000] {spark_submit.py:571} INFO - "0" : 11
[2024-05-18T21:46:41.718+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:46:41.718+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:46:41.719+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:46:41.719+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-18T21:46:41.719+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6112469437652812,
[2024-05-18T21:46:41.719+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:46:41.720+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:46:41.720+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:46:41.720+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:46:41.720+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:46:41.720+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:46:41.721+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:46:41.721+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:46:41.721+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:46:41.722+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:46:41.722+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:46:47.432+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:47 INFO BlockManagerInfo: Removed broadcast_8_piece0 on f3c105d69d83:38673 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-18T21:46:47.436+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:47 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.25.0.6:45173 in memory (size: 25.5 KiB, free: 434.4 MiB)
[2024-05-18T21:46:51.731+0000] {spark_submit.py:571} INFO - 24/05/18 21:46:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:47:01.734+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:47:11.747+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:47:19.427+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/9 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.9.04fb8810-6052-406b-a1ff-4ccd132a59b8.tmp
[2024-05-18T21:47:19.466+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.9.04fb8810-6052-406b-a1ff-4ccd132a59b8.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/9
[2024-05-18T21:47:19.468+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:19 INFO MicroBatchExecution: Committed offsets for batch 9. Metadata OffsetSeqMetadata(0,1716068839373,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:47:19.535+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:47:19.539+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:47:19.562+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:47:19.563+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:47:19.651+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:47:19.711+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:19 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:47:19.713+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:19 INFO DAGScheduler: Got job 9 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:47:19.713+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:19 INFO DAGScheduler: Final stage: ResultStage 9 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:47:19.713+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:19 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:47:19.714+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:19 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:47:19.714+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:19 INFO DAGScheduler: Submitting ResultStage 9 (PythonRDD[99] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:47:19.716+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:19 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:47:19.719+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:19 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:47:19.719+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:19 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:47:19.720+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:19 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:47:19.720+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (PythonRDD[99] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:47:19.721+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:19 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2024-05-18T21:47:19.724+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:19 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:47:19.809+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:19 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:47:20.530+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:20 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 803 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:47:20.540+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:20 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2024-05-18T21:47:20.542+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:20 INFO DAGScheduler: ResultStage 9 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.824 s
[2024-05-18T21:47:20.544+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:20 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:47:20.545+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2024-05-18T21:47:20.546+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:20 INFO DAGScheduler: Job 9 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.833707 s
[2024-05-18T21:47:20.578+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/9 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.9.d592ceaf-ceec-41f6-a7d6-484733f4bc38.tmp
[2024-05-18T21:47:20.635+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.9.d592ceaf-ceec-41f6-a7d6-484733f4bc38.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/9
[2024-05-18T21:47:20.638+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:20 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:47:20.638+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:47:20.639+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:47:20.639+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:47:20.639+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:47:19.372Z",
[2024-05-18T21:47:20.639+0000] {spark_submit.py:571} INFO - "batchId" : 9,
[2024-05-18T21:47:20.639+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:47:20.640+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-18T21:47:20.640+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7917656373713381,
[2024-05-18T21:47:20.640+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:47:20.641+0000] {spark_submit.py:571} INFO - "addBatch" : 1007,
[2024-05-18T21:47:20.641+0000] {spark_submit.py:571} INFO - "commitOffsets" : 81,
[2024-05-18T21:47:20.641+0000] {spark_submit.py:571} INFO - "getBatch" : 1,
[2024-05-18T21:47:20.641+0000] {spark_submit.py:571} INFO - "latestOffset" : 1,
[2024-05-18T21:47:20.641+0000] {spark_submit.py:571} INFO - "queryPlanning" : 68,
[2024-05-18T21:47:20.641+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1263,
[2024-05-18T21:47:20.641+0000] {spark_submit.py:571} INFO - "walCommit" : 97
[2024-05-18T21:47:20.641+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:47:20.642+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:47:20.642+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:47:20.642+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:47:20.642+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:47:20.642+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:47:20.642+0000] {spark_submit.py:571} INFO - "0" : 11
[2024-05-18T21:47:20.642+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:47:20.642+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:47:20.642+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:47:20.643+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:47:20.643+0000] {spark_submit.py:571} INFO - "0" : 12
[2024-05-18T21:47:20.643+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:47:20.643+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:47:20.643+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:47:20.643+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:47:20.643+0000] {spark_submit.py:571} INFO - "0" : 12
[2024-05-18T21:47:20.643+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:47:20.644+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:47:20.644+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:47:20.644+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-18T21:47:20.644+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7917656373713381,
[2024-05-18T21:47:20.644+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:47:20.644+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:47:20.644+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:47:20.644+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:47:20.644+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:47:20.645+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:47:20.645+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:47:20.645+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:47:20.645+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:47:20.645+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:47:20.645+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:47:30.648+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:47:40.666+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:47:44.632+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:44 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
[2024-05-18T21:47:47.743+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/10 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.10.fc5c3f1b-c99b-48ec-b22b-8f1b2d09ee58.tmp
[2024-05-18T21:47:47.785+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.10.fc5c3f1b-c99b-48ec-b22b-8f1b2d09ee58.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/10
[2024-05-18T21:47:47.786+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:47 INFO MicroBatchExecution: Committed offsets for batch 10. Metadata OffsetSeqMetadata(0,1716068867695,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:47:47.844+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:47:47.848+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:47:47.867+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:47:47.868+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:47:47.905+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:47:47.940+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:47 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:47:47.941+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:47 INFO DAGScheduler: Got job 10 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:47:47.941+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:47 INFO DAGScheduler: Final stage: ResultStage 10 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:47:47.942+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:47 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:47:47.942+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:47 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:47:47.943+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:47 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[109] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:47:48.002+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:48 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 69.8 KiB, free 434.2 MiB)
[2024-05-18T21:47:48.005+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:48 INFO BlockManagerInfo: Removed broadcast_9_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:47:48.006+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:48 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:47:48.008+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:48 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:47:48.009+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:48 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:47:48.010+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (PythonRDD[109] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:47:48.010+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:48 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2024-05-18T21:47:48.013+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:48 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:47:48.028+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:48 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:47:48.055+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:48 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:47:48.802+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:48 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 786 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:47:48.805+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:48 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2024-05-18T21:47:48.808+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:48 INFO DAGScheduler: ResultStage 10 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.806 s
[2024-05-18T21:47:48.809+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:48 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:47:48.810+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
[2024-05-18T21:47:48.811+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:48 INFO DAGScheduler: Job 10 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.866732 s
[2024-05-18T21:47:48.836+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/10 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.10.41d2eafa-db7e-4d8c-9b30-766709c0499a.tmp
[2024-05-18T21:47:48.866+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.10.41d2eafa-db7e-4d8c-9b30-766709c0499a.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/10
[2024-05-18T21:47:48.869+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:48 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:47:48.869+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:47:48.870+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:47:48.870+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:47:48.874+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:47:47.693Z",
[2024-05-18T21:47:48.876+0000] {spark_submit.py:571} INFO - "batchId" : 10,
[2024-05-18T21:47:48.877+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:47:48.878+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:47:48.878+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8525149190110827,
[2024-05-18T21:47:48.880+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:47:48.881+0000] {spark_submit.py:571} INFO - "addBatch" : 962,
[2024-05-18T21:47:48.881+0000] {spark_submit.py:571} INFO - "commitOffsets" : 50,
[2024-05-18T21:47:48.881+0000] {spark_submit.py:571} INFO - "getBatch" : 1,
[2024-05-18T21:47:48.882+0000] {spark_submit.py:571} INFO - "latestOffset" : 2,
[2024-05-18T21:47:48.882+0000] {spark_submit.py:571} INFO - "queryPlanning" : 62,
[2024-05-18T21:47:48.883+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1173,
[2024-05-18T21:47:48.883+0000] {spark_submit.py:571} INFO - "walCommit" : 91
[2024-05-18T21:47:48.884+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:47:48.884+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:47:48.885+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:47:48.886+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:47:48.886+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:47:48.887+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:47:48.887+0000] {spark_submit.py:571} INFO - "0" : 12
[2024-05-18T21:47:48.887+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:47:48.888+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:47:48.888+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:47:48.889+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:47:48.889+0000] {spark_submit.py:571} INFO - "0" : 13
[2024-05-18T21:47:48.891+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:47:48.892+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:47:48.893+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:47:48.893+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:47:48.894+0000] {spark_submit.py:571} INFO - "0" : 13
[2024-05-18T21:47:48.894+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:47:48.894+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:47:48.896+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:47:48.897+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:47:48.898+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8525149190110827,
[2024-05-18T21:47:48.899+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:47:48.900+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:47:48.900+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:47:48.901+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:47:48.901+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:47:48.901+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:47:48.902+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:47:48.902+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:47:48.902+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:47:48.902+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:47:48.903+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:47:58.882+0000] {spark_submit.py:571} INFO - 24/05/18 21:47:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:48:08.879+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:48:18.884+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:48:25.476+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:25 INFO BlockManagerInfo: Removed broadcast_10_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:48:25.596+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:25 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:48:29.561+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/11 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.11.93489738-0f06-4d24-aeae-8807f6221283.tmp
[2024-05-18T21:48:30.216+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.11.93489738-0f06-4d24-aeae-8807f6221283.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/11
[2024-05-18T21:48:30.229+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:30 INFO MicroBatchExecution: Committed offsets for batch 11. Metadata OffsetSeqMetadata(0,1716068907929,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:48:31.547+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:48:31.560+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:48:31.839+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:48:31.864+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:48:32.375+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:48:33.185+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:33 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:48:33.217+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:33 INFO DAGScheduler: Got job 11 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:48:33.218+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:33 INFO DAGScheduler: Final stage: ResultStage 11 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:48:33.218+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:33 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:48:33.218+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:33 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:48:33.220+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:33 INFO DAGScheduler: Submitting ResultStage 11 (PythonRDD[119] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:48:33.249+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:33 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:48:33.275+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:33 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:48:33.277+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:33 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:48:33.280+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:33 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:48:33.280+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (PythonRDD[119] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:48:33.281+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:33 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2024-05-18T21:48:33.295+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:33 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:48:34.137+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:34 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:48:35.440+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:35 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 2131 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:48:35.457+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:35 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2024-05-18T21:48:35.459+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:35 INFO DAGScheduler: ResultStage 11 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 2.217 s
[2024-05-18T21:48:35.462+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:35 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:48:35.464+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2024-05-18T21:48:35.465+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:35 INFO DAGScheduler: Job 11 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 2.275889 s
[2024-05-18T21:48:35.585+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/11 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.11.3c755df7-3470-470a-83c3-9329ebc928b6.tmp
[2024-05-18T21:48:35.652+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.11.3c755df7-3470-470a-83c3-9329ebc928b6.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/11
[2024-05-18T21:48:35.660+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:35 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:48:35.661+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:48:35.661+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:48:35.661+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:48:35.662+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:48:27.737Z",
[2024-05-18T21:48:35.662+0000] {spark_submit.py:571} INFO - "batchId" : 11,
[2024-05-18T21:48:35.663+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:48:35.663+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 24.390243902439025,
[2024-05-18T21:48:35.664+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.12632642748863063,
[2024-05-18T21:48:35.664+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:48:35.664+0000] {spark_submit.py:571} INFO - "addBatch" : 3849,
[2024-05-18T21:48:35.665+0000] {spark_submit.py:571} INFO - "commitOffsets" : 148,
[2024-05-18T21:48:35.665+0000] {spark_submit.py:571} INFO - "getBatch" : 6,
[2024-05-18T21:48:35.665+0000] {spark_submit.py:571} INFO - "latestOffset" : 192,
[2024-05-18T21:48:35.666+0000] {spark_submit.py:571} INFO - "queryPlanning" : 1289,
[2024-05-18T21:48:35.666+0000] {spark_submit.py:571} INFO - "triggerExecution" : 7916,
[2024-05-18T21:48:35.666+0000] {spark_submit.py:571} INFO - "walCommit" : 2326
[2024-05-18T21:48:35.666+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:48:35.666+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:48:35.667+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:48:35.667+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:48:35.667+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:48:35.667+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:48:35.667+0000] {spark_submit.py:571} INFO - "0" : 13
[2024-05-18T21:48:35.667+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:48:35.668+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:48:35.668+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:48:35.668+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:48:35.668+0000] {spark_submit.py:571} INFO - "0" : 14
[2024-05-18T21:48:35.668+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:48:35.668+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:48:35.668+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:48:35.669+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:48:35.669+0000] {spark_submit.py:571} INFO - "0" : 14
[2024-05-18T21:48:35.669+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:48:35.669+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:48:35.669+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:48:35.669+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 24.390243902439025,
[2024-05-18T21:48:35.669+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.12632642748863063,
[2024-05-18T21:48:35.670+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:48:35.670+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:48:35.670+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:48:35.670+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:48:35.670+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:48:35.670+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:48:35.670+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:48:35.670+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:48:35.671+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:48:35.671+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:48:35.671+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:48:45.661+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:48:55.664+0000] {spark_submit.py:571} INFO - 24/05/18 21:48:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:49:00.832+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/12 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.12.5191ae9a-69a7-49c9-9d69-dcb23d474158.tmp
[2024-05-18T21:49:00.892+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.12.5191ae9a-69a7-49c9-9d69-dcb23d474158.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/12
[2024-05-18T21:49:00.893+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:00 INFO MicroBatchExecution: Committed offsets for batch 12. Metadata OffsetSeqMetadata(0,1716068940781,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:49:01.032+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:49:01.036+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:49:01.101+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:49:01.103+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:49:01.215+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:01 INFO BlockManagerInfo: Removed broadcast_11_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:49:01.240+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:49:01.255+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:01 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:49:01.336+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:01 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:49:01.341+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:01 INFO DAGScheduler: Got job 12 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:49:01.342+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:01 INFO DAGScheduler: Final stage: ResultStage 12 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:49:01.343+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:01 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:49:01.343+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:01 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:49:01.343+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:01 INFO DAGScheduler: Submitting ResultStage 12 (PythonRDD[129] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:49:01.346+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:01 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:49:01.350+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:01 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:49:01.350+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:01 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:49:01.351+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:01 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:49:01.351+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (PythonRDD[129] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:49:01.351+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:01 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2024-05-18T21:49:01.354+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:01 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:49:01.430+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:01 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:49:03.027+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:03 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 1671 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:49:03.032+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:03 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2024-05-18T21:49:03.033+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:03 INFO DAGScheduler: ResultStage 12 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 1.688 s
[2024-05-18T21:49:03.034+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:03 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:49:03.034+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
[2024-05-18T21:49:03.035+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:03 INFO DAGScheduler: Job 12 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 1.698462 s
[2024-05-18T21:49:03.151+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/12 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.12.27eba628-7da9-4b46-b9dd-8d235b78f59e.tmp
[2024-05-18T21:49:03.350+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.12.27eba628-7da9-4b46-b9dd-8d235b78f59e.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/12
[2024-05-18T21:49:03.355+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:03 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:49:03.356+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:49:03.356+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:49:03.356+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:49:03.356+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:49:00.775Z",
[2024-05-18T21:49:03.357+0000] {spark_submit.py:571} INFO - "batchId" : 12,
[2024-05-18T21:49:03.357+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:49:03.357+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-05-18T21:49:03.357+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.3883495145631068,
[2024-05-18T21:49:03.357+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:49:03.358+0000] {spark_submit.py:571} INFO - "addBatch" : 1987,
[2024-05-18T21:49:03.358+0000] {spark_submit.py:571} INFO - "commitOffsets" : 281,
[2024-05-18T21:49:03.358+0000] {spark_submit.py:571} INFO - "getBatch" : 1,
[2024-05-18T21:49:03.358+0000] {spark_submit.py:571} INFO - "latestOffset" : 6,
[2024-05-18T21:49:03.358+0000] {spark_submit.py:571} INFO - "queryPlanning" : 146,
[2024-05-18T21:49:03.359+0000] {spark_submit.py:571} INFO - "triggerExecution" : 2575,
[2024-05-18T21:49:03.359+0000] {spark_submit.py:571} INFO - "walCommit" : 114
[2024-05-18T21:49:03.359+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:49:03.359+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:49:03.359+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:49:03.359+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:49:03.359+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:49:03.360+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:49:03.360+0000] {spark_submit.py:571} INFO - "0" : 14
[2024-05-18T21:49:03.360+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:49:03.360+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:49:03.360+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:49:03.360+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:49:03.360+0000] {spark_submit.py:571} INFO - "0" : 15
[2024-05-18T21:49:03.361+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:49:03.361+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:49:03.361+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:49:03.361+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:49:03.361+0000] {spark_submit.py:571} INFO - "0" : 15
[2024-05-18T21:49:03.361+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:49:03.361+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:49:03.362+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:49:03.362+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-05-18T21:49:03.362+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.3883495145631068,
[2024-05-18T21:49:03.362+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:49:03.362+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:49:03.362+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:49:03.362+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:49:03.363+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:49:03.363+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:49:03.363+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:49:03.363+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:49:03.363+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:49:03.363+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:49:03.364+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:49:13.373+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:49:23.370+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:49:33.372+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:49:34.689+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:34 INFO BlockManagerInfo: Removed broadcast_12_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:49:34.729+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:34 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:49:35.474+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/13 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.13.e8f524ea-a67f-4db3-8959-c2a2c5f40832.tmp
[2024-05-18T21:49:35.514+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.13.e8f524ea-a67f-4db3-8959-c2a2c5f40832.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/13
[2024-05-18T21:49:35.514+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:35 INFO MicroBatchExecution: Committed offsets for batch 13. Metadata OffsetSeqMetadata(0,1716068975422,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:49:35.564+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:49:35.568+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:49:35.597+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:49:35.599+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:49:35.634+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:49:35.706+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:35 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:49:35.707+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:35 INFO DAGScheduler: Got job 13 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:49:35.707+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:35 INFO DAGScheduler: Final stage: ResultStage 13 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:49:35.707+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:35 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:49:35.708+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:35 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:49:35.708+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:35 INFO DAGScheduler: Submitting ResultStage 13 (PythonRDD[139] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:49:35.710+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:35 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:49:35.712+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:35 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:49:35.713+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:35 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:49:35.713+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:35 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:49:35.713+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (PythonRDD[139] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:49:35.713+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:35 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2024-05-18T21:49:35.715+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:35 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:49:35.755+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:35 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:49:36.547+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:36 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 830 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:49:36.555+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:36 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2024-05-18T21:49:36.556+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:36 INFO DAGScheduler: ResultStage 13 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.843 s
[2024-05-18T21:49:36.557+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:36 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:49:36.558+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2024-05-18T21:49:36.558+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:36 INFO DAGScheduler: Job 13 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.847950 s
[2024-05-18T21:49:36.615+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/13 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.13.65534e9a-5bab-4076-9352-c3e358037a7e.tmp
[2024-05-18T21:49:36.671+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.13.65534e9a-5bab-4076-9352-c3e358037a7e.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/13
[2024-05-18T21:49:36.683+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:36 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:49:36.684+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:49:36.685+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:49:36.686+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:49:36.686+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:49:35.419Z",
[2024-05-18T21:49:36.687+0000] {spark_submit.py:571} INFO - "batchId" : 13,
[2024-05-18T21:49:36.687+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:49:36.688+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-05-18T21:49:36.688+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7980845969672786,
[2024-05-18T21:49:36.688+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:49:36.689+0000] {spark_submit.py:571} INFO - "addBatch" : 988,
[2024-05-18T21:49:36.689+0000] {spark_submit.py:571} INFO - "commitOffsets" : 103,
[2024-05-18T21:49:36.689+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-18T21:49:36.690+0000] {spark_submit.py:571} INFO - "latestOffset" : 3,
[2024-05-18T21:49:36.690+0000] {spark_submit.py:571} INFO - "queryPlanning" : 56,
[2024-05-18T21:49:36.691+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1253,
[2024-05-18T21:49:36.691+0000] {spark_submit.py:571} INFO - "walCommit" : 93
[2024-05-18T21:49:36.692+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:49:36.692+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:49:36.692+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:49:36.692+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:49:36.692+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:49:36.692+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:49:36.692+0000] {spark_submit.py:571} INFO - "0" : 15
[2024-05-18T21:49:36.693+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:49:36.693+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:49:36.693+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:49:36.693+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:49:36.693+0000] {spark_submit.py:571} INFO - "0" : 16
[2024-05-18T21:49:36.693+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:49:36.693+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:49:36.694+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:49:36.694+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:49:36.694+0000] {spark_submit.py:571} INFO - "0" : 16
[2024-05-18T21:49:36.694+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:49:36.694+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:49:36.694+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:49:36.694+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-05-18T21:49:36.695+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7980845969672786,
[2024-05-18T21:49:36.695+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:49:36.695+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:49:36.695+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:49:36.695+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:49:36.695+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:49:36.695+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:49:36.696+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:49:36.696+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:49:36.696+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:49:36.696+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:49:36.696+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:49:46.689+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:49:56.696+0000] {spark_submit.py:571} INFO - 24/05/18 21:49:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:50:05.761+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:05 INFO BlockManagerInfo: Removed broadcast_13_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:50:05.800+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:05 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:50:06.464+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/14 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.14.974aa748-d3f6-4215-94ac-4c82b90db600.tmp
[2024-05-18T21:50:06.528+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.14.974aa748-d3f6-4215-94ac-4c82b90db600.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/14
[2024-05-18T21:50:06.529+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:06 INFO MicroBatchExecution: Committed offsets for batch 14. Metadata OffsetSeqMetadata(0,1716069006374,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:50:06.570+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:50:06.574+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:50:06.603+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:50:06.605+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:50:06.676+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:50:06.730+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:06 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:50:06.732+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:06 INFO DAGScheduler: Got job 14 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:50:06.732+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:06 INFO DAGScheduler: Final stage: ResultStage 14 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:50:06.732+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:06 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:50:06.733+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:06 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:50:06.734+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:06 INFO DAGScheduler: Submitting ResultStage 14 (PythonRDD[149] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:50:06.737+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:06 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:50:06.739+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:06 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:50:06.740+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:06 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:50:06.740+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:06 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:50:06.741+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (PythonRDD[149] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:50:06.741+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:06 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
[2024-05-18T21:50:06.743+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:06 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:50:06.799+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:06 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:50:07.571+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:07 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 829 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:50:07.573+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:07 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2024-05-18T21:50:07.573+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:07 INFO DAGScheduler: ResultStage 14 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.838 s
[2024-05-18T21:50:07.574+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:07 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:50:07.574+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
[2024-05-18T21:50:07.575+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:07 INFO DAGScheduler: Job 14 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.843761 s
[2024-05-18T21:50:07.603+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/14 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.14.755b40b9-2658-412d-8022-0f9c5052b800.tmp
[2024-05-18T21:50:07.650+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.14.755b40b9-2658-412d-8022-0f9c5052b800.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/14
[2024-05-18T21:50:07.660+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:07 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:50:07.661+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:50:07.661+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:50:07.661+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:50:07.662+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:50:06.367Z",
[2024-05-18T21:50:07.662+0000] {spark_submit.py:571} INFO - "batchId" : 14,
[2024-05-18T21:50:07.662+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:50:07.662+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:50:07.663+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.779423226812159,
[2024-05-18T21:50:07.663+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:50:07.663+0000] {spark_submit.py:571} INFO - "addBatch" : 996,
[2024-05-18T21:50:07.663+0000] {spark_submit.py:571} INFO - "commitOffsets" : 69,
[2024-05-18T21:50:07.664+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-18T21:50:07.664+0000] {spark_submit.py:571} INFO - "latestOffset" : 7,
[2024-05-18T21:50:07.664+0000] {spark_submit.py:571} INFO - "queryPlanning" : 47,
[2024-05-18T21:50:07.665+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1283,
[2024-05-18T21:50:07.665+0000] {spark_submit.py:571} INFO - "walCommit" : 155
[2024-05-18T21:50:07.665+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:50:07.665+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:50:07.665+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:50:07.665+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:50:07.665+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:50:07.666+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:50:07.666+0000] {spark_submit.py:571} INFO - "0" : 16
[2024-05-18T21:50:07.666+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:50:07.666+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:50:07.666+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:50:07.667+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:50:07.667+0000] {spark_submit.py:571} INFO - "0" : 17
[2024-05-18T21:50:07.667+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:50:07.667+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:50:07.667+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:50:07.667+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:50:07.668+0000] {spark_submit.py:571} INFO - "0" : 17
[2024-05-18T21:50:07.668+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:50:07.668+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:50:07.668+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:50:07.668+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:50:07.668+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.779423226812159,
[2024-05-18T21:50:07.668+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:50:07.668+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:50:07.669+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:50:07.669+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:50:07.669+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:50:07.669+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:50:07.669+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:50:07.669+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:50:07.669+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:50:07.669+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:50:07.669+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:50:17.672+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:50:27.664+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:50:37.766+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:50:37.775+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:37 INFO BlockManagerInfo: Removed broadcast_14_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:50:37.803+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:37 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:50:47.685+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:50:57.683+0000] {spark_submit.py:571} INFO - 24/05/18 21:50:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:51:07.690+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:51:17.694+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:51:27.696+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:51:37.705+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:51:47.706+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:51:50.359+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/15 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.15.a5994e7a-8541-4cc9-8b0d-ef06d1eff492.tmp
[2024-05-18T21:51:50.394+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.15.a5994e7a-8541-4cc9-8b0d-ef06d1eff492.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/15
[2024-05-18T21:51:50.395+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:50 INFO MicroBatchExecution: Committed offsets for batch 15. Metadata OffsetSeqMetadata(0,1716069110307,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:51:50.463+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:51:50.466+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:51:50.498+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:51:50.501+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:51:50.533+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:51:50.589+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:50 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:51:50.591+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:50 INFO DAGScheduler: Got job 15 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:51:50.592+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:50 INFO DAGScheduler: Final stage: ResultStage 15 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:51:50.593+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:50 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:51:50.593+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:50 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:51:50.593+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:50 INFO DAGScheduler: Submitting ResultStage 15 (PythonRDD[159] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:51:50.595+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:50 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:51:50.626+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:50 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:51:50.627+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:50 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:51:50.627+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:50 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:51:50.628+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (PythonRDD[159] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:51:50.628+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:50 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2024-05-18T21:51:50.631+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:50 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:51:50.708+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:50 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:51:51.720+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:51 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 1076 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:51:51.730+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:51 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2024-05-18T21:51:51.731+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:51 INFO DAGScheduler: ResultStage 15 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 1.135 s
[2024-05-18T21:51:51.733+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:51 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:51:51.734+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
[2024-05-18T21:51:51.738+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:51 INFO DAGScheduler: Job 15 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 1.142259 s
[2024-05-18T21:51:51.786+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/15 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.15.d9d9842b-cee0-415c-b3f2-7d506ff73f63.tmp
[2024-05-18T21:51:51.817+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.15.d9d9842b-cee0-415c-b3f2-7d506ff73f63.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/15
[2024-05-18T21:51:51.821+0000] {spark_submit.py:571} INFO - 24/05/18 21:51:51 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:51:51.821+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:51:51.821+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:51:51.821+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:51:51.821+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:51:50.306Z",
[2024-05-18T21:51:51.822+0000] {spark_submit.py:571} INFO - "batchId" : 15,
[2024-05-18T21:51:51.822+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:51:51.822+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-18T21:51:51.822+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6618133686300464,
[2024-05-18T21:51:51.822+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:51:51.822+0000] {spark_submit.py:571} INFO - "addBatch" : 1275,
[2024-05-18T21:51:51.823+0000] {spark_submit.py:571} INFO - "commitOffsets" : 68,
[2024-05-18T21:51:51.823+0000] {spark_submit.py:571} INFO - "getBatch" : 1,
[2024-05-18T21:51:51.823+0000] {spark_submit.py:571} INFO - "latestOffset" : 1,
[2024-05-18T21:51:51.823+0000] {spark_submit.py:571} INFO - "queryPlanning" : 69,
[2024-05-18T21:51:51.823+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1511,
[2024-05-18T21:51:51.823+0000] {spark_submit.py:571} INFO - "walCommit" : 88
[2024-05-18T21:51:51.824+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:51:51.824+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:51:51.824+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:51:51.824+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:51:51.824+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:51:51.824+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:51:51.824+0000] {spark_submit.py:571} INFO - "0" : 17
[2024-05-18T21:51:51.825+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:51:51.825+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:51:51.825+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:51:51.825+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:51:51.825+0000] {spark_submit.py:571} INFO - "0" : 18
[2024-05-18T21:51:51.825+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:51:51.825+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:51:51.825+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:51:51.826+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:51:51.826+0000] {spark_submit.py:571} INFO - "0" : 18
[2024-05-18T21:51:51.826+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:51:51.826+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:51:51.826+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:51:51.826+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-18T21:51:51.826+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6618133686300464,
[2024-05-18T21:51:51.826+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:51:51.826+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:51:51.827+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:51:51.827+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:51:51.827+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:51:51.827+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:51:51.827+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:51:51.827+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:51:51.827+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:51:51.827+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:51:51.828+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:52:01.843+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:52:11.851+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:52:13.145+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:13 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/16 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.16.c0bd8d28-dbf7-4694-be97-4c58d0c3477a.tmp
[2024-05-18T21:52:13.199+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:13 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.16.c0bd8d28-dbf7-4694-be97-4c58d0c3477a.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/16
[2024-05-18T21:52:13.200+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:13 INFO MicroBatchExecution: Committed offsets for batch 16. Metadata OffsetSeqMetadata(0,1716069133110,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:52:13.273+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:52:13.281+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:52:13.310+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:52:13.311+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:52:13.366+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:52:13.428+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:13 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:52:13.430+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:13 INFO DAGScheduler: Got job 16 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:52:13.430+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:13 INFO DAGScheduler: Final stage: ResultStage 16 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:52:13.431+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:13 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:52:13.431+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:13 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:52:13.432+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:13 INFO DAGScheduler: Submitting ResultStage 16 (PythonRDD[169] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:52:13.436+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:13 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 69.8 KiB, free 434.2 MiB)
[2024-05-18T21:52:13.439+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:13 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.2 MiB)
[2024-05-18T21:52:13.439+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:13 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:52:13.440+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:13 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:52:13.440+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (PythonRDD[169] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:52:13.441+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:13 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
[2024-05-18T21:52:13.442+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:13 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:52:13.507+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:13 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:52:14.492+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:14 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 1031 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:52:14.498+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:14 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2024-05-18T21:52:14.504+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:14 INFO DAGScheduler: ResultStage 16 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 1.066 s
[2024-05-18T21:52:14.507+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:14 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:52:14.509+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
[2024-05-18T21:52:14.509+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:14 INFO DAGScheduler: Job 16 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 1.080383 s
[2024-05-18T21:52:14.586+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/16 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.16.e71a556d-6698-42fa-aa35-c793ede3e5bb.tmp
[2024-05-18T21:52:14.652+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.16.e71a556d-6698-42fa-aa35-c793ede3e5bb.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/16
[2024-05-18T21:52:14.657+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:14 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:52:14.658+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:52:14.658+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:52:14.658+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:52:14.659+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:52:13.109Z",
[2024-05-18T21:52:14.660+0000] {spark_submit.py:571} INFO - "batchId" : 16,
[2024-05-18T21:52:14.660+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:52:14.661+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:52:14.661+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6480881399870383,
[2024-05-18T21:52:14.661+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:52:14.662+0000] {spark_submit.py:571} INFO - "addBatch" : 1252,
[2024-05-18T21:52:14.662+0000] {spark_submit.py:571} INFO - "commitOffsets" : 108,
[2024-05-18T21:52:14.662+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-18T21:52:14.663+0000] {spark_submit.py:571} INFO - "latestOffset" : 1,
[2024-05-18T21:52:14.663+0000] {spark_submit.py:571} INFO - "queryPlanning" : 82,
[2024-05-18T21:52:14.663+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1543,
[2024-05-18T21:52:14.663+0000] {spark_submit.py:571} INFO - "walCommit" : 91
[2024-05-18T21:52:14.664+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:52:14.664+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:52:14.664+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:52:14.664+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:52:14.665+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:52:14.665+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:52:14.665+0000] {spark_submit.py:571} INFO - "0" : 18
[2024-05-18T21:52:14.665+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:52:14.665+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:52:14.665+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:52:14.665+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:52:14.666+0000] {spark_submit.py:571} INFO - "0" : 19
[2024-05-18T21:52:14.666+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:52:14.666+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:52:14.666+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:52:14.666+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:52:14.666+0000] {spark_submit.py:571} INFO - "0" : 19
[2024-05-18T21:52:14.666+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:52:14.667+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:52:14.667+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:52:14.667+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:52:14.667+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6480881399870383,
[2024-05-18T21:52:14.667+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:52:14.667+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:52:14.667+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:52:14.668+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:52:14.668+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:52:14.668+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:52:14.668+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:52:14.669+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:52:14.669+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:52:14.669+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:52:14.669+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:52:24.676+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:52:26.752+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:26 INFO BlockManagerInfo: Removed broadcast_16_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:52:26.770+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:26 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:52:26.792+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:26 INFO BlockManagerInfo: Removed broadcast_15_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:52:26.803+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:26 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:52:34.681+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:52:44.684+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:52:49.057+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/17 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.17.89db47db-f789-4d97-b7f8-cb3c189b97a8.tmp
[2024-05-18T21:52:49.091+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.17.89db47db-f789-4d97-b7f8-cb3c189b97a8.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/17
[2024-05-18T21:52:49.094+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:49 INFO MicroBatchExecution: Committed offsets for batch 17. Metadata OffsetSeqMetadata(0,1716069169013,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:52:49.274+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:52:49.331+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:52:49.348+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:52:49.348+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:52:49.399+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:52:49.450+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:49 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:52:49.454+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:49 INFO DAGScheduler: Got job 17 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:52:49.454+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:49 INFO DAGScheduler: Final stage: ResultStage 17 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:52:49.454+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:49 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:52:49.455+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:49 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:52:49.455+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:49 INFO DAGScheduler: Submitting ResultStage 17 (PythonRDD[179] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:52:49.456+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:49 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:52:49.458+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:49 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:52:49.459+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:49 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:52:49.459+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:49 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:52:49.459+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (PythonRDD[179] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:52:49.459+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:49 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2024-05-18T21:52:49.461+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:49 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:52:49.520+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:49 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:52:50.331+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:50 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 865 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:52:50.337+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:50 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2024-05-18T21:52:50.338+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:50 INFO DAGScheduler: ResultStage 17 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.879 s
[2024-05-18T21:52:50.340+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:50 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:52:50.341+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
[2024-05-18T21:52:50.341+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:50 INFO DAGScheduler: Job 17 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.888672 s
[2024-05-18T21:52:50.396+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/17 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.17.11f28dd9-6d6d-494a-aa8a-321e105a496e.tmp
[2024-05-18T21:52:50.461+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.17.11f28dd9-6d6d-494a-aa8a-321e105a496e.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/17
[2024-05-18T21:52:50.493+0000] {spark_submit.py:571} INFO - 24/05/18 21:52:50 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:52:50.494+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:52:50.495+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:52:50.496+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:52:50.497+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:52:49.010Z",
[2024-05-18T21:52:50.497+0000] {spark_submit.py:571} INFO - "batchId" : 17,
[2024-05-18T21:52:50.498+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:52:50.499+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 62.5,
[2024-05-18T21:52:50.499+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.683526999316473,
[2024-05-18T21:52:50.500+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:52:50.500+0000] {spark_submit.py:571} INFO - "addBatch" : 1009,
[2024-05-18T21:52:50.501+0000] {spark_submit.py:571} INFO - "commitOffsets" : 113,
[2024-05-18T21:52:50.501+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-18T21:52:50.501+0000] {spark_submit.py:571} INFO - "latestOffset" : 3,
[2024-05-18T21:52:50.501+0000] {spark_submit.py:571} INFO - "queryPlanning" : 240,
[2024-05-18T21:52:50.501+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1463,
[2024-05-18T21:52:50.502+0000] {spark_submit.py:571} INFO - "walCommit" : 78
[2024-05-18T21:52:50.502+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:52:50.502+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:52:50.503+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:52:50.503+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:52:50.503+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:52:50.503+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:52:50.503+0000] {spark_submit.py:571} INFO - "0" : 19
[2024-05-18T21:52:50.503+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:52:50.504+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:52:50.504+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:52:50.504+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:52:50.504+0000] {spark_submit.py:571} INFO - "0" : 20
[2024-05-18T21:52:50.505+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:52:50.505+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:52:50.505+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:52:50.505+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:52:50.505+0000] {spark_submit.py:571} INFO - "0" : 20
[2024-05-18T21:52:50.505+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:52:50.506+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:52:50.507+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:52:50.508+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 62.5,
[2024-05-18T21:52:50.508+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.683526999316473,
[2024-05-18T21:52:50.509+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:52:50.509+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:52:50.509+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:52:50.509+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:52:50.509+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:52:50.510+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:52:50.510+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:52:50.510+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:52:50.511+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:52:50.512+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:52:50.512+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:53:00.490+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:53:02.037+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:02 INFO BlockManagerInfo: Removed broadcast_17_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:53:02.058+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:02 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:53:10.494+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:53:20.490+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:53:24.134+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/18 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.18.d7989f43-9854-4dc6-83e3-53b21a696e55.tmp
[2024-05-18T21:53:24.316+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:24 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.18.d7989f43-9854-4dc6-83e3-53b21a696e55.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/18
[2024-05-18T21:53:24.317+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:24 INFO MicroBatchExecution: Committed offsets for batch 18. Metadata OffsetSeqMetadata(0,1716069204051,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:53:24.413+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:53:24.417+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:53:24.440+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:53:24.442+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:53:24.504+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:53:24.566+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:24 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:53:24.568+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:24 INFO DAGScheduler: Got job 18 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:53:24.568+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:24 INFO DAGScheduler: Final stage: ResultStage 18 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:53:24.568+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:24 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:53:24.568+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:24 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:53:24.569+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:24 INFO DAGScheduler: Submitting ResultStage 18 (PythonRDD[189] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:53:24.587+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:24 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:53:24.593+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:24 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:53:24.594+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:24 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:53:24.594+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:24 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:53:24.595+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (PythonRDD[189] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:53:24.596+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:24 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2024-05-18T21:53:24.598+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:24 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:53:24.736+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:24 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:53:26.060+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:26 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 1458 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:53:26.075+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:26 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2024-05-18T21:53:26.082+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:26 INFO DAGScheduler: ResultStage 18 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 1.501 s
[2024-05-18T21:53:26.097+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:26 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:53:26.104+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
[2024-05-18T21:53:26.106+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:26 INFO DAGScheduler: Job 18 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 1.534565 s
[2024-05-18T21:53:26.204+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/18 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.18.7adb1633-e1be-4276-92f0-3aea58a4ece6.tmp
[2024-05-18T21:53:26.423+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.18.7adb1633-e1be-4276-92f0-3aea58a4ece6.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/18
[2024-05-18T21:53:26.485+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:26 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:53:26.485+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:53:26.486+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:53:26.486+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:53:26.486+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:53:24.049Z",
[2024-05-18T21:53:26.486+0000] {spark_submit.py:571} INFO - "batchId" : 18,
[2024-05-18T21:53:26.487+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:53:26.487+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-18T21:53:26.487+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.4208754208754209,
[2024-05-18T21:53:26.487+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:53:26.487+0000] {spark_submit.py:571} INFO - "addBatch" : 1703,
[2024-05-18T21:53:26.487+0000] {spark_submit.py:571} INFO - "commitOffsets" : 293,
[2024-05-18T21:53:26.487+0000] {spark_submit.py:571} INFO - "getBatch" : 1,
[2024-05-18T21:53:26.488+0000] {spark_submit.py:571} INFO - "latestOffset" : 2,
[2024-05-18T21:53:26.488+0000] {spark_submit.py:571} INFO - "queryPlanning" : 98,
[2024-05-18T21:53:26.488+0000] {spark_submit.py:571} INFO - "triggerExecution" : 2376,
[2024-05-18T21:53:26.488+0000] {spark_submit.py:571} INFO - "walCommit" : 268
[2024-05-18T21:53:26.488+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:53:26.488+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:53:26.488+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:53:26.489+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:53:26.489+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:53:26.489+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:53:26.489+0000] {spark_submit.py:571} INFO - "0" : 20
[2024-05-18T21:53:26.489+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:53:26.490+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:53:26.490+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:53:26.491+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:53:26.491+0000] {spark_submit.py:571} INFO - "0" : 21
[2024-05-18T21:53:26.492+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:53:26.492+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:53:26.492+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:53:26.493+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:53:26.493+0000] {spark_submit.py:571} INFO - "0" : 21
[2024-05-18T21:53:26.493+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:53:26.493+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:53:26.493+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:53:26.493+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-18T21:53:26.494+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.4208754208754209,
[2024-05-18T21:53:26.494+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:53:26.494+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:53:26.494+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:53:26.495+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:53:26.495+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:53:26.495+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:53:26.495+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:53:26.495+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:53:26.495+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:53:26.495+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:53:26.496+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:53:36.530+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:53:37.236+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:37 INFO BlockManagerInfo: Removed broadcast_18_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:53:37.240+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:37 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:53:46.523+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:53:49.427+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/19 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.19.9d7a6693-ae63-436d-9901-21a7d3cc9e04.tmp
[2024-05-18T21:53:49.545+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.19.9d7a6693-ae63-436d-9901-21a7d3cc9e04.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/19
[2024-05-18T21:53:49.546+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:49 INFO MicroBatchExecution: Committed offsets for batch 19. Metadata OffsetSeqMetadata(0,1716069229382,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:53:49.618+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:53:49.622+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:53:49.642+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:53:49.643+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:53:49.700+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:53:49.787+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:49 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:53:49.809+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:49 INFO DAGScheduler: Got job 19 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:53:49.813+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:49 INFO DAGScheduler: Final stage: ResultStage 19 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:53:49.822+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:49 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:53:49.824+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:49 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:53:49.825+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:49 INFO DAGScheduler: Submitting ResultStage 19 (PythonRDD[199] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:53:49.828+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:49 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:53:49.840+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:49 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:53:49.849+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:49 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:53:49.854+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:49 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:53:49.859+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (PythonRDD[199] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:53:49.860+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:49 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2024-05-18T21:53:49.867+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:49 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:53:49.955+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:49 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:53:50.925+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:50 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 1041 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:53:50.964+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:50 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2024-05-18T21:53:50.968+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:50 INFO DAGScheduler: ResultStage 19 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 1.116 s
[2024-05-18T21:53:50.971+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:50 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:53:50.973+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
[2024-05-18T21:53:50.978+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:50 INFO DAGScheduler: Job 19 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 1.186279 s
[2024-05-18T21:53:51.042+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/19 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.19.c395f37b-fc92-4795-a6d7-d025b7371172.tmp
[2024-05-18T21:53:51.121+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.19.c395f37b-fc92-4795-a6d7-d025b7371172.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/19
[2024-05-18T21:53:51.126+0000] {spark_submit.py:571} INFO - 24/05/18 21:53:51 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:53:51.127+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:53:51.127+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:53:51.128+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:53:51.128+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:53:49.365Z",
[2024-05-18T21:53:51.129+0000] {spark_submit.py:571} INFO - "batchId" : 19,
[2024-05-18T21:53:51.129+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:53:51.130+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:53:51.131+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.5694760820045558,
[2024-05-18T21:53:51.132+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:53:51.133+0000] {spark_submit.py:571} INFO - "addBatch" : 1363,
[2024-05-18T21:53:51.133+0000] {spark_submit.py:571} INFO - "commitOffsets" : 128,
[2024-05-18T21:53:51.133+0000] {spark_submit.py:571} INFO - "getBatch" : 1,
[2024-05-18T21:53:51.134+0000] {spark_submit.py:571} INFO - "latestOffset" : 17,
[2024-05-18T21:53:51.134+0000] {spark_submit.py:571} INFO - "queryPlanning" : 76,
[2024-05-18T21:53:51.135+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1756,
[2024-05-18T21:53:51.135+0000] {spark_submit.py:571} INFO - "walCommit" : 164
[2024-05-18T21:53:51.135+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:53:51.136+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:53:51.136+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:53:51.136+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:53:51.136+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:53:51.136+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:53:51.137+0000] {spark_submit.py:571} INFO - "0" : 21
[2024-05-18T21:53:51.137+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:53:51.137+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:53:51.137+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:53:51.137+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:53:51.137+0000] {spark_submit.py:571} INFO - "0" : 22
[2024-05-18T21:53:51.137+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:53:51.138+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:53:51.138+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:53:51.138+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:53:51.138+0000] {spark_submit.py:571} INFO - "0" : 22
[2024-05-18T21:53:51.138+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:53:51.138+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:53:51.138+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:53:51.138+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:53:51.139+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.5694760820045558,
[2024-05-18T21:53:51.139+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:53:51.139+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:53:51.139+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:53:51.139+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:53:51.139+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:53:51.139+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:53:51.139+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:53:51.140+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:53:51.140+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:53:51.140+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:53:51.140+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:54:01.131+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:54:11.146+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:54:12.597+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:12 INFO BlockManagerInfo: Removed broadcast_19_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:54:12.642+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:12 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:54:21.157+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:54:31.168+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:54:41.177+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:54:45.586+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/20 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.20.82ea45d9-9f3b-4561-a218-866aa9a9e079.tmp
[2024-05-18T21:54:45.626+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.20.82ea45d9-9f3b-4561-a218-866aa9a9e079.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/20
[2024-05-18T21:54:45.627+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:45 INFO MicroBatchExecution: Committed offsets for batch 20. Metadata OffsetSeqMetadata(0,1716069285540,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:54:45.735+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:54:45.738+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:54:45.756+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:54:45.757+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:54:45.799+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:54:45.836+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:45 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:54:45.839+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:45 INFO DAGScheduler: Got job 20 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:54:45.840+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:45 INFO DAGScheduler: Final stage: ResultStage 20 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:54:45.840+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:45 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:54:45.840+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:45 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:54:45.841+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:45 INFO DAGScheduler: Submitting ResultStage 20 (PythonRDD[209] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:54:45.844+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:45 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:54:45.847+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:45 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:54:45.847+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:45 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:54:45.848+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:45 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:54:45.848+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (PythonRDD[209] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:54:45.849+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:45 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
[2024-05-18T21:54:45.850+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:45 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:54:45.916+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:45 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:54:46.705+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:46 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 853 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:54:46.709+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:46 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2024-05-18T21:54:46.712+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:46 INFO DAGScheduler: ResultStage 20 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.866 s
[2024-05-18T21:54:46.713+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:46 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:54:46.713+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
[2024-05-18T21:54:46.713+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:46 INFO DAGScheduler: Job 20 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.872704 s
[2024-05-18T21:54:46.824+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:46 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/20 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.20.ef82d688-4ef7-4d8e-8d04-e0d8aef1b853.tmp
[2024-05-18T21:54:46.836+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:46 INFO BlockManagerInfo: Removed broadcast_20_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:54:46.880+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.20.ef82d688-4ef7-4d8e-8d04-e0d8aef1b853.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/20
[2024-05-18T21:54:46.884+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:46 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:54:46.884+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:54:46.885+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:54:46.885+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:54:46.886+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:54:45.539Z",
[2024-05-18T21:54:46.887+0000] {spark_submit.py:571} INFO - "batchId" : 20,
[2024-05-18T21:54:46.888+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:54:46.888+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:54:46.888+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7451564828614009,
[2024-05-18T21:54:46.889+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:54:46.889+0000] {spark_submit.py:571} INFO - "addBatch" : 997,
[2024-05-18T21:54:46.889+0000] {spark_submit.py:571} INFO - "commitOffsets" : 134,
[2024-05-18T21:54:46.889+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-18T21:54:46.889+0000] {spark_submit.py:571} INFO - "latestOffset" : 1,
[2024-05-18T21:54:46.890+0000] {spark_submit.py:571} INFO - "queryPlanning" : 114,
[2024-05-18T21:54:46.890+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1342,
[2024-05-18T21:54:46.890+0000] {spark_submit.py:571} INFO - "walCommit" : 88
[2024-05-18T21:54:46.890+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:54:46.890+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:54:46.890+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:54:46.891+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:54:46.891+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:54:46.891+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:54:46.891+0000] {spark_submit.py:571} INFO - "0" : 22
[2024-05-18T21:54:46.892+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:54:46.892+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:54:46.892+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:54:46.892+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:54:46.892+0000] {spark_submit.py:571} INFO - "0" : 23
[2024-05-18T21:54:46.893+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:54:46.893+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:54:46.893+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:54:46.893+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:54:46.893+0000] {spark_submit.py:571} INFO - "0" : 23
[2024-05-18T21:54:46.893+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:54:46.893+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:54:46.893+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:54:46.893+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:54:46.893+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7451564828614009,
[2024-05-18T21:54:46.894+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:54:46.894+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:54:46.894+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:54:46.894+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:54:46.894+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:54:46.894+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:54:46.894+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:54:46.894+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:54:46.894+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:54:46.894+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:54:46.894+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:54:46.894+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:46 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:54:56.885+0000] {spark_submit.py:571} INFO - 24/05/18 21:54:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:55:06.893+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:55:14.255+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/21 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.21.dc5ec7b5-6ffc-44d1-9172-d441f40feb29.tmp
[2024-05-18T21:55:14.306+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.21.dc5ec7b5-6ffc-44d1-9172-d441f40feb29.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/21
[2024-05-18T21:55:14.306+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:14 INFO MicroBatchExecution: Committed offsets for batch 21. Metadata OffsetSeqMetadata(0,1716069314189,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:55:14.417+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:55:14.420+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:55:14.446+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:55:14.448+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:55:14.504+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:55:14.570+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:14 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:55:14.575+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:14 INFO DAGScheduler: Got job 21 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:55:14.580+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:14 INFO DAGScheduler: Final stage: ResultStage 21 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:55:14.586+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:14 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:55:14.589+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:14 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:55:14.590+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:14 INFO DAGScheduler: Submitting ResultStage 21 (PythonRDD[219] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:55:14.598+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:14 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:55:14.600+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:14 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:55:14.602+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:14 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:55:14.602+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:14 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:55:14.603+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (PythonRDD[219] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:55:14.603+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:14 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
[2024-05-18T21:55:14.605+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:14 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:55:14.679+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:14 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:55:15.713+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:15 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 1104 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:55:15.724+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:15 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
[2024-05-18T21:55:15.727+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:15 INFO DAGScheduler: ResultStage 21 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 1.125 s
[2024-05-18T21:55:15.735+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:15 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:55:15.740+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
[2024-05-18T21:55:15.745+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:15 INFO DAGScheduler: Job 21 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 1.151087 s
[2024-05-18T21:55:15.812+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/21 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.21.0f26f26f-c546-4ee7-9005-2fd9d0c53079.tmp
[2024-05-18T21:55:15.971+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.21.0f26f26f-c546-4ee7-9005-2fd9d0c53079.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/21
[2024-05-18T21:55:15.995+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:15 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:55:15.995+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:55:15.996+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:55:15.996+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:55:15.997+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:55:14.188Z",
[2024-05-18T21:55:15.997+0000] {spark_submit.py:571} INFO - "batchId" : 21,
[2024-05-18T21:55:15.998+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:55:15.999+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:55:15.999+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.5605381165919282,
[2024-05-18T21:55:16.000+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:55:16.000+0000] {spark_submit.py:571} INFO - "addBatch" : 1322,
[2024-05-18T21:55:16.000+0000] {spark_submit.py:571} INFO - "commitOffsets" : 220,
[2024-05-18T21:55:16.001+0000] {spark_submit.py:571} INFO - "getBatch" : 1,
[2024-05-18T21:55:16.001+0000] {spark_submit.py:571} INFO - "latestOffset" : 1,
[2024-05-18T21:55:16.001+0000] {spark_submit.py:571} INFO - "queryPlanning" : 113,
[2024-05-18T21:55:16.002+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1784,
[2024-05-18T21:55:16.002+0000] {spark_submit.py:571} INFO - "walCommit" : 118
[2024-05-18T21:55:16.002+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:55:16.002+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:55:16.004+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:55:16.004+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:55:16.004+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:55:16.005+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:55:16.005+0000] {spark_submit.py:571} INFO - "0" : 23
[2024-05-18T21:55:16.005+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:55:16.005+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:55:16.005+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:55:16.005+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:55:16.006+0000] {spark_submit.py:571} INFO - "0" : 24
[2024-05-18T21:55:16.007+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:55:16.007+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:55:16.008+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:55:16.008+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:55:16.008+0000] {spark_submit.py:571} INFO - "0" : 24
[2024-05-18T21:55:16.009+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:55:16.009+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:55:16.010+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:55:16.010+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:55:16.011+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.5605381165919282,
[2024-05-18T21:55:16.011+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:55:16.011+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:55:16.012+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:55:16.013+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:55:16.013+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:55:16.013+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:55:16.014+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:55:16.015+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:55:16.015+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:55:16.016+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:55:16.016+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:55:21.630+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:21 INFO BlockManagerInfo: Removed broadcast_21_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:55:21.672+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:21 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:55:25.992+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:55:36.011+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:55:46.014+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:55:46.621+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:46 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/22 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.22.7426106e-11ce-442d-b97d-4374b90d3d48.tmp
[2024-05-18T21:55:46.688+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.22.7426106e-11ce-442d-b97d-4374b90d3d48.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/22
[2024-05-18T21:55:46.691+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:46 INFO MicroBatchExecution: Committed offsets for batch 22. Metadata OffsetSeqMetadata(0,1716069346572,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:55:46.730+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:55:46.739+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:55:46.753+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:55:46.754+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:55:46.797+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:55:46.845+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:46 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:55:46.848+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:46 INFO DAGScheduler: Got job 22 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:55:46.849+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:46 INFO DAGScheduler: Final stage: ResultStage 22 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:55:46.849+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:46 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:55:46.849+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:46 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:55:46.854+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:46 INFO DAGScheduler: Submitting ResultStage 22 (PythonRDD[229] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:55:46.858+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:46 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:55:46.863+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:46 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:55:46.864+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:46 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:55:46.869+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:46 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:55:46.869+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (PythonRDD[229] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:55:46.869+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:46 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2024-05-18T21:55:46.876+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:46 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:55:46.907+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:46 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:55:47.716+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:47 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 839 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:55:47.731+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:47 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
[2024-05-18T21:55:47.735+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:47 INFO DAGScheduler: ResultStage 22 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.863 s
[2024-05-18T21:55:47.737+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:47 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:55:47.738+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
[2024-05-18T21:55:47.738+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:47 INFO DAGScheduler: Job 22 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.876463 s
[2024-05-18T21:55:47.780+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/22 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.22.f096028c-4417-43a6-966f-54ec411b29a3.tmp
[2024-05-18T21:55:47.841+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.22.f096028c-4417-43a6-966f-54ec411b29a3.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/22
[2024-05-18T21:55:47.846+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:47 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:55:47.847+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:55:47.848+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:55:47.848+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:55:47.848+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:55:46.570Z",
[2024-05-18T21:55:47.849+0000] {spark_submit.py:571} INFO - "batchId" : 22,
[2024-05-18T21:55:47.849+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:55:47.849+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:55:47.850+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7867820613690009,
[2024-05-18T21:55:47.850+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:55:47.850+0000] {spark_submit.py:571} INFO - "addBatch" : 1005,
[2024-05-18T21:55:47.850+0000] {spark_submit.py:571} INFO - "commitOffsets" : 91,
[2024-05-18T21:55:47.851+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-18T21:55:47.851+0000] {spark_submit.py:571} INFO - "latestOffset" : 2,
[2024-05-18T21:55:47.851+0000] {spark_submit.py:571} INFO - "queryPlanning" : 52,
[2024-05-18T21:55:47.851+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1271,
[2024-05-18T21:55:47.851+0000] {spark_submit.py:571} INFO - "walCommit" : 117
[2024-05-18T21:55:47.852+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:55:47.852+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:55:47.853+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:55:47.853+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:55:47.853+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:55:47.854+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:55:47.854+0000] {spark_submit.py:571} INFO - "0" : 24
[2024-05-18T21:55:47.854+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:55:47.854+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:55:47.855+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:55:47.855+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:55:47.855+0000] {spark_submit.py:571} INFO - "0" : 25
[2024-05-18T21:55:47.855+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:55:47.856+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:55:47.856+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:55:47.856+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:55:47.856+0000] {spark_submit.py:571} INFO - "0" : 25
[2024-05-18T21:55:47.857+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:55:47.858+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:55:47.859+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:55:47.860+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:55:47.861+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7867820613690009,
[2024-05-18T21:55:47.861+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:55:47.861+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:55:47.862+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:55:47.862+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:55:47.862+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:55:47.862+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:55:47.862+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:55:47.863+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:55:47.863+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:55:47.863+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:55:47.863+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:55:57.075+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:56 INFO BlockManagerInfo: Removed broadcast_22_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:55:57.345+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:57 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:55:57.858+0000] {spark_submit.py:571} INFO - 24/05/18 21:55:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:56:07.870+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:56:17.662+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/23 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.23.46223b3d-df98-44b0-bb9d-978ffbf0e76e.tmp
[2024-05-18T21:56:17.706+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.23.46223b3d-df98-44b0-bb9d-978ffbf0e76e.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/23
[2024-05-18T21:56:17.709+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:17 INFO MicroBatchExecution: Committed offsets for batch 23. Metadata OffsetSeqMetadata(0,1716069377586,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:56:17.768+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:56:17.770+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:56:17.786+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:56:17.789+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:56:17.898+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:56:17.973+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:17 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:56:17.977+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:17 INFO DAGScheduler: Got job 23 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:56:17.978+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:17 INFO DAGScheduler: Final stage: ResultStage 23 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:56:17.979+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:17 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:56:17.980+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:17 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:56:17.980+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:17 INFO DAGScheduler: Submitting ResultStage 23 (PythonRDD[239] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:56:17.983+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:17 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:56:17.987+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:17 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:56:17.988+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:17 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:56:17.990+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:17 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:56:17.991+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (PythonRDD[239] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:56:17.991+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:17 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
[2024-05-18T21:56:17.994+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:17 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:56:18.088+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:18 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:56:18.841+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:18 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 845 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:56:18.847+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:18 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
[2024-05-18T21:56:18.848+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:18 INFO DAGScheduler: ResultStage 23 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.862 s
[2024-05-18T21:56:18.848+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:18 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:56:18.849+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
[2024-05-18T21:56:18.849+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:18 INFO DAGScheduler: Job 23 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.870256 s
[2024-05-18T21:56:18.891+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/23 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.23.3773ab66-0ecb-4650-8c5f-57e7a9810046.tmp
[2024-05-18T21:56:18.931+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.23.3773ab66-0ecb-4650-8c5f-57e7a9810046.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/23
[2024-05-18T21:56:18.934+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:18 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:56:18.934+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:56:18.935+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:56:18.935+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:56:18.935+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:56:17.581Z",
[2024-05-18T21:56:18.935+0000] {spark_submit.py:571} INFO - "batchId" : 23,
[2024-05-18T21:56:18.935+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:56:18.936+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:56:18.936+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7407407407407407,
[2024-05-18T21:56:18.936+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:56:18.936+0000] {spark_submit.py:571} INFO - "addBatch" : 1082,
[2024-05-18T21:56:18.936+0000] {spark_submit.py:571} INFO - "commitOffsets" : 73,
[2024-05-18T21:56:18.936+0000] {spark_submit.py:571} INFO - "getBatch" : 1,
[2024-05-18T21:56:18.937+0000] {spark_submit.py:571} INFO - "latestOffset" : 3,
[2024-05-18T21:56:18.937+0000] {spark_submit.py:571} INFO - "queryPlanning" : 58,
[2024-05-18T21:56:18.937+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1350,
[2024-05-18T21:56:18.937+0000] {spark_submit.py:571} INFO - "walCommit" : 123
[2024-05-18T21:56:18.937+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:56:18.937+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:56:18.937+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:56:18.938+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:56:18.938+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:56:18.938+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:56:18.938+0000] {spark_submit.py:571} INFO - "0" : 25
[2024-05-18T21:56:18.938+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:56:18.938+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:56:18.938+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:56:18.939+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:56:18.939+0000] {spark_submit.py:571} INFO - "0" : 26
[2024-05-18T21:56:18.939+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:56:18.939+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:56:18.939+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:56:18.939+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:56:18.939+0000] {spark_submit.py:571} INFO - "0" : 26
[2024-05-18T21:56:18.939+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:56:18.940+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:56:18.940+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:56:18.940+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:56:18.940+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7407407407407407,
[2024-05-18T21:56:18.940+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:56:18.940+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:56:18.940+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:56:18.940+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:56:18.941+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:56:18.941+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:56:18.941+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:56:18.941+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:56:18.941+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:56:18.941+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:56:18.941+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:56:28.951+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:56:29.411+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:29 INFO BlockManagerInfo: Removed broadcast_23_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:56:29.418+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:29 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:56:38.957+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:56:48.373+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/24 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.24.7bed092a-1b53-4f3d-9a41-ee15cbf9c631.tmp
[2024-05-18T21:56:48.428+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.24.7bed092a-1b53-4f3d-9a41-ee15cbf9c631.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/24
[2024-05-18T21:56:48.428+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:48 INFO MicroBatchExecution: Committed offsets for batch 24. Metadata OffsetSeqMetadata(0,1716069408321,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:56:48.483+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:56:48.486+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:56:48.505+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:56:48.506+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:56:48.530+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:56:48.574+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:48 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:56:48.575+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:48 INFO DAGScheduler: Got job 24 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:56:48.579+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:48 INFO DAGScheduler: Final stage: ResultStage 24 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:56:48.579+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:48 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:56:48.580+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:48 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:56:48.580+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:48 INFO DAGScheduler: Submitting ResultStage 24 (PythonRDD[249] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:56:48.580+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:48 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:56:48.581+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:48 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:56:48.582+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:48 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:56:48.582+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:48 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:56:48.583+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (PythonRDD[249] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:56:48.583+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:48 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
[2024-05-18T21:56:48.585+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:48 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:56:48.627+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:48 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:56:49.433+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:49 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 847 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:56:49.438+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:49 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
[2024-05-18T21:56:49.439+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:49 INFO DAGScheduler: ResultStage 24 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.859 s
[2024-05-18T21:56:49.439+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:49 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:56:49.440+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
[2024-05-18T21:56:49.440+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:49 INFO DAGScheduler: Job 24 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.863573 s
[2024-05-18T21:56:49.464+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/24 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.24.08ed1faa-ae77-40f9-bc9f-6200364dda32.tmp
[2024-05-18T21:56:49.515+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.24.08ed1faa-ae77-40f9-bc9f-6200364dda32.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/24
[2024-05-18T21:56:49.518+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:49 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:56:49.519+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:56:49.519+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:56:49.520+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:56:49.520+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:56:48.319Z",
[2024-05-18T21:56:49.521+0000] {spark_submit.py:571} INFO - "batchId" : 24,
[2024-05-18T21:56:49.521+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:56:49.521+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:56:49.521+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8361204013377926,
[2024-05-18T21:56:49.522+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:56:49.522+0000] {spark_submit.py:571} INFO - "addBatch" : 951,
[2024-05-18T21:56:49.522+0000] {spark_submit.py:571} INFO - "commitOffsets" : 70,
[2024-05-18T21:56:49.522+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-18T21:56:49.522+0000] {spark_submit.py:571} INFO - "latestOffset" : 2,
[2024-05-18T21:56:49.523+0000] {spark_submit.py:571} INFO - "queryPlanning" : 59,
[2024-05-18T21:56:49.523+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1196,
[2024-05-18T21:56:49.523+0000] {spark_submit.py:571} INFO - "walCommit" : 108
[2024-05-18T21:56:49.523+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:56:49.523+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:56:49.523+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:56:49.524+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:56:49.524+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:56:49.524+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:56:49.524+0000] {spark_submit.py:571} INFO - "0" : 26
[2024-05-18T21:56:49.524+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:56:49.524+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:56:49.524+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:56:49.524+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:56:49.525+0000] {spark_submit.py:571} INFO - "0" : 27
[2024-05-18T21:56:49.525+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:56:49.525+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:56:49.525+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:56:49.525+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:56:49.525+0000] {spark_submit.py:571} INFO - "0" : 27
[2024-05-18T21:56:49.525+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:56:49.526+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:56:49.526+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:56:49.526+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:56:49.526+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8361204013377926,
[2024-05-18T21:56:49.526+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:56:49.526+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:56:49.526+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:56:49.527+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:56:49.527+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:56:49.527+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:56:49.527+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:56:49.527+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:56:49.527+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:56:49.528+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:56:49.528+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:56:59.528+0000] {spark_submit.py:571} INFO - 24/05/18 21:56:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:57:01.840+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:01 INFO BlockManagerInfo: Removed broadcast_24_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:57:01.871+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:01 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:57:09.543+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:57:19.539+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:57:21.934+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/25 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.25.a8935e81-74fd-422c-9220-5ff66b29941d.tmp
[2024-05-18T21:57:22.005+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.25.a8935e81-74fd-422c-9220-5ff66b29941d.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/25
[2024-05-18T21:57:22.006+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:22 INFO MicroBatchExecution: Committed offsets for batch 25. Metadata OffsetSeqMetadata(0,1716069441852,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:57:22.107+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:57:22.111+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:57:22.132+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:57:22.133+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:57:22.189+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:57:22.246+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:22 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:57:22.249+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:22 INFO DAGScheduler: Got job 25 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:57:22.250+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:22 INFO DAGScheduler: Final stage: ResultStage 25 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:57:22.251+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:22 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:57:22.251+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:22 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:57:22.251+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:22 INFO DAGScheduler: Submitting ResultStage 25 (PythonRDD[259] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:57:22.255+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:22 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:57:22.258+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:22 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:57:22.259+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:22 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:57:22.259+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:22 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:57:22.260+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (PythonRDD[259] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:57:22.260+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:22 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
[2024-05-18T21:57:22.262+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:22 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 25) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:57:22.370+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:22 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:57:23.272+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:23 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 25) in 1006 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:57:23.283+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:23 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool
[2024-05-18T21:57:23.284+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:23 INFO DAGScheduler: ResultStage 25 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 1.025 s
[2024-05-18T21:57:23.296+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:23 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:57:23.298+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
[2024-05-18T21:57:23.299+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:23 INFO DAGScheduler: Job 25 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 1.036436 s
[2024-05-18T21:57:23.376+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/25 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.25.f105d796-1e51-40d9-a104-f981dc45261c.tmp
[2024-05-18T21:57:23.444+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.25.f105d796-1e51-40d9-a104-f981dc45261c.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/25
[2024-05-18T21:57:23.477+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:23 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:57:23.479+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:57:23.480+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:57:23.481+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:57:23.481+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:57:21.849Z",
[2024-05-18T21:57:23.482+0000] {spark_submit.py:571} INFO - "batchId" : 25,
[2024-05-18T21:57:23.482+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:57:23.483+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 66.66666666666667,
[2024-05-18T21:57:23.483+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6253908692933083,
[2024-05-18T21:57:23.484+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:57:23.484+0000] {spark_submit.py:571} INFO - "addBatch" : 1213,
[2024-05-18T21:57:23.485+0000] {spark_submit.py:571} INFO - "commitOffsets" : 109,
[2024-05-18T21:57:23.485+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-18T21:57:23.486+0000] {spark_submit.py:571} INFO - "latestOffset" : 3,
[2024-05-18T21:57:23.487+0000] {spark_submit.py:571} INFO - "queryPlanning" : 105,
[2024-05-18T21:57:23.488+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1598,
[2024-05-18T21:57:23.488+0000] {spark_submit.py:571} INFO - "walCommit" : 155
[2024-05-18T21:57:23.489+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:57:23.489+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:57:23.490+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:57:23.491+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:57:23.491+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:57:23.492+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:57:23.492+0000] {spark_submit.py:571} INFO - "0" : 27
[2024-05-18T21:57:23.493+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:57:23.493+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:57:23.494+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:57:23.494+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:57:23.495+0000] {spark_submit.py:571} INFO - "0" : 28
[2024-05-18T21:57:23.495+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:57:23.496+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:57:23.496+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:57:23.497+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:57:23.497+0000] {spark_submit.py:571} INFO - "0" : 28
[2024-05-18T21:57:23.497+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:57:23.498+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:57:23.499+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:57:23.499+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 66.66666666666667,
[2024-05-18T21:57:23.500+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.6253908692933083,
[2024-05-18T21:57:23.501+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:57:23.503+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:57:23.504+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:57:23.505+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:57:23.506+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:57:23.507+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:57:23.507+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:57:23.508+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:57:23.511+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:57:23.514+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:57:23.514+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:57:33.490+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:57:34.806+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:34 INFO BlockManagerInfo: Removed broadcast_25_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:57:34.840+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:34 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:57:43.491+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:57:53.497+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:57:56.508+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/26 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.26.54e2661a-eff5-492e-b80a-fafd5bad48bf.tmp
[2024-05-18T21:57:56.563+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.26.54e2661a-eff5-492e-b80a-fafd5bad48bf.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/26
[2024-05-18T21:57:56.565+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:56 INFO MicroBatchExecution: Committed offsets for batch 26. Metadata OffsetSeqMetadata(0,1716069476451,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:57:56.808+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:57:56.814+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:57:56.871+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:57:56.877+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:57:56.952+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:57:57.044+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:57 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:57:57.047+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:57 INFO DAGScheduler: Got job 26 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:57:57.047+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:57 INFO DAGScheduler: Final stage: ResultStage 26 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:57:57.048+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:57 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:57:57.048+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:57 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:57:57.048+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:57 INFO DAGScheduler: Submitting ResultStage 26 (PythonRDD[269] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:57:57.052+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:57 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:57:57.055+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:57 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:57:57.056+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:57 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:57:57.056+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:57 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:57:57.056+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (PythonRDD[269] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:57:57.056+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:57 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
[2024-05-18T21:57:57.058+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:57 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 26) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:57:57.109+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:57 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:57:58.117+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:58 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 26) in 1059 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:57:58.119+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:58 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
[2024-05-18T21:57:58.122+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:58 INFO DAGScheduler: ResultStage 26 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 1.073 s
[2024-05-18T21:57:58.126+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:58 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:57:58.128+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
[2024-05-18T21:57:58.128+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:58 INFO DAGScheduler: Job 26 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 1.080202 s
[2024-05-18T21:57:58.237+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/26 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.26.60332ea3-dd51-455e-9efe-da3af8dce805.tmp
[2024-05-18T21:57:58.302+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.26.60332ea3-dd51-455e-9efe-da3af8dce805.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/26
[2024-05-18T21:57:58.308+0000] {spark_submit.py:571} INFO - 24/05/18 21:57:58 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:57:58.309+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:57:58.309+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:57:58.310+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:57:58.310+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:57:56.449Z",
[2024-05-18T21:57:58.311+0000] {spark_submit.py:571} INFO - "batchId" : 26,
[2024-05-18T21:57:58.311+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:57:58.312+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:57:58.312+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.5393743257820928,
[2024-05-18T21:57:58.312+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:57:58.313+0000] {spark_submit.py:571} INFO - "addBatch" : 1329,
[2024-05-18T21:57:58.313+0000] {spark_submit.py:571} INFO - "commitOffsets" : 126,
[2024-05-18T21:57:58.314+0000] {spark_submit.py:571} INFO - "getBatch" : 2,
[2024-05-18T21:57:58.314+0000] {spark_submit.py:571} INFO - "latestOffset" : 0,
[2024-05-18T21:57:58.314+0000] {spark_submit.py:571} INFO - "queryPlanning" : 250,
[2024-05-18T21:57:58.314+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1854,
[2024-05-18T21:57:58.315+0000] {spark_submit.py:571} INFO - "walCommit" : 115
[2024-05-18T21:57:58.315+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:57:58.316+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:57:58.316+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:57:58.316+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:57:58.316+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:57:58.317+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:57:58.317+0000] {spark_submit.py:571} INFO - "0" : 28
[2024-05-18T21:57:58.317+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:57:58.317+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:57:58.317+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:57:58.317+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:57:58.318+0000] {spark_submit.py:571} INFO - "0" : 29
[2024-05-18T21:57:58.318+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:57:58.318+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:57:58.318+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:57:58.318+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:57:58.318+0000] {spark_submit.py:571} INFO - "0" : 29
[2024-05-18T21:57:58.319+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:57:58.319+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:57:58.319+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:57:58.319+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:57:58.319+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.5393743257820928,
[2024-05-18T21:57:58.319+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:57:58.319+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:57:58.320+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:57:58.320+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:57:58.320+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:57:58.320+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:57:58.320+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:57:58.320+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:57:58.321+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:57:58.321+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:57:58.321+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:58:08.316+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:58:09.433+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:09 INFO BlockManagerInfo: Removed broadcast_26_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:58:09.454+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:09 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:58:18.319+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:58:28.319+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:58:33.889+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/27 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.27.c53dd1fc-c8f2-4ef2-bf4e-884177252720.tmp
[2024-05-18T21:58:33.929+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.27.c53dd1fc-c8f2-4ef2-bf4e-884177252720.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/27
[2024-05-18T21:58:33.933+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:33 INFO MicroBatchExecution: Committed offsets for batch 27. Metadata OffsetSeqMetadata(0,1716069513854,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:58:33.995+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:58:33.997+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:58:34.040+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:58:34.041+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:58:34.069+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:58:34.123+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:34 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:58:34.125+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:34 INFO DAGScheduler: Got job 27 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:58:34.125+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:34 INFO DAGScheduler: Final stage: ResultStage 27 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:58:34.125+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:34 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:58:34.125+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:34 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:58:34.127+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:34 INFO DAGScheduler: Submitting ResultStage 27 (PythonRDD[279] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:58:34.130+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:34 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:58:34.133+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:34 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:58:34.133+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:34 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:58:34.134+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:34 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:58:34.134+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (PythonRDD[279] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:58:34.134+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:34 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
[2024-05-18T21:58:34.136+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:34 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 27) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:58:34.196+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:34 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:58:34.979+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:34 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 27) in 840 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:58:34.986+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:34 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
[2024-05-18T21:58:34.989+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:34 INFO DAGScheduler: ResultStage 27 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.857 s
[2024-05-18T21:58:34.990+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:34 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:58:34.992+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
[2024-05-18T21:58:34.993+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:34 INFO DAGScheduler: Job 27 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.868801 s
[2024-05-18T21:58:35.029+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/27 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.27.32effa99-5d01-4e39-955d-0a80ac211671.tmp
[2024-05-18T21:58:35.097+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.27.32effa99-5d01-4e39-955d-0a80ac211671.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/27
[2024-05-18T21:58:35.100+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:35 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:58:35.100+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:58:35.101+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:58:35.101+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:58:35.101+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:58:33.854Z",
[2024-05-18T21:58:35.101+0000] {spark_submit.py:571} INFO - "batchId" : 27,
[2024-05-18T21:58:35.101+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:58:35.102+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-18T21:58:35.102+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8045052292839903,
[2024-05-18T21:58:35.102+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:58:35.102+0000] {spark_submit.py:571} INFO - "addBatch" : 999,
[2024-05-18T21:58:35.102+0000] {spark_submit.py:571} INFO - "commitOffsets" : 92,
[2024-05-18T21:58:35.102+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-18T21:58:35.103+0000] {spark_submit.py:571} INFO - "latestOffset" : 0,
[2024-05-18T21:58:35.103+0000] {spark_submit.py:571} INFO - "queryPlanning" : 66,
[2024-05-18T21:58:35.103+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1243,
[2024-05-18T21:58:35.103+0000] {spark_submit.py:571} INFO - "walCommit" : 76
[2024-05-18T21:58:35.103+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:58:35.103+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:58:35.103+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:58:35.104+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:58:35.104+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:58:35.104+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:58:35.104+0000] {spark_submit.py:571} INFO - "0" : 29
[2024-05-18T21:58:35.104+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:58:35.104+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:58:35.104+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:58:35.104+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:58:35.104+0000] {spark_submit.py:571} INFO - "0" : 30
[2024-05-18T21:58:35.105+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:58:35.105+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:58:35.105+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:58:35.105+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:58:35.105+0000] {spark_submit.py:571} INFO - "0" : 30
[2024-05-18T21:58:35.105+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:58:35.105+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:58:35.105+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:58:35.106+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-18T21:58:35.106+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8045052292839903,
[2024-05-18T21:58:35.106+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:58:35.106+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:58:35.106+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:58:35.106+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:58:35.106+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:58:35.106+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:58:35.107+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:58:35.107+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:58:35.107+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:58:35.107+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:58:35.107+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:58:43.360+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:43 INFO BlockManagerInfo: Removed broadcast_27_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:58:43.393+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:43 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:58:45.107+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:58:55.118+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:58:56.702+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/28 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.28.f500996a-8468-4662-96db-46ecc96ffc74.tmp
[2024-05-18T21:58:56.747+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.28.f500996a-8468-4662-96db-46ecc96ffc74.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/28
[2024-05-18T21:58:56.748+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:56 INFO MicroBatchExecution: Committed offsets for batch 28. Metadata OffsetSeqMetadata(0,1716069536656,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:58:56.839+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:58:56.843+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:58:56.881+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:58:56.882+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:58:56.930+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:58:56.987+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:56 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:58:56.990+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:56 INFO DAGScheduler: Got job 28 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:58:56.990+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:56 INFO DAGScheduler: Final stage: ResultStage 28 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:58:56.990+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:56 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:58:56.991+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:56 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:58:56.991+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:56 INFO DAGScheduler: Submitting ResultStage 28 (PythonRDD[289] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:58:56.994+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:56 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:58:56.997+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:56 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:58:56.997+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:56 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:58:56.999+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:56 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:58:56.999+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (PythonRDD[289] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:58:56.999+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:56 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0
[2024-05-18T21:58:57.000+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:57 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 28) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:58:57.047+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:57 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:58:58.782+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:58 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 28) in 1778 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:58:58.802+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:58 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool
[2024-05-18T21:58:58.803+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:58 INFO DAGScheduler: ResultStage 28 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 1.802 s
[2024-05-18T21:58:58.804+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:58 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:58:58.804+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
[2024-05-18T21:58:58.805+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:58 INFO DAGScheduler: Job 28 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 1.816981 s
[2024-05-18T21:58:58.867+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/28 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.28.e8f30afb-1699-4527-809a-1f74805c8171.tmp
[2024-05-18T21:58:59.036+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.28.e8f30afb-1699-4527-809a-1f74805c8171.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/28
[2024-05-18T21:58:59.039+0000] {spark_submit.py:571} INFO - 24/05/18 21:58:59 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:58:59.039+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:58:59.040+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:58:59.040+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:58:59.040+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:58:56.653Z",
[2024-05-18T21:58:59.040+0000] {spark_submit.py:571} INFO - "batchId" : 28,
[2024-05-18T21:58:59.040+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:58:59.041+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-18T21:58:59.041+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.419639110365086,
[2024-05-18T21:58:59.041+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:58:59.041+0000] {spark_submit.py:571} INFO - "addBatch" : 1956,
[2024-05-18T21:58:59.042+0000] {spark_submit.py:571} INFO - "commitOffsets" : 213,
[2024-05-18T21:58:59.042+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-18T21:58:59.042+0000] {spark_submit.py:571} INFO - "latestOffset" : 3,
[2024-05-18T21:58:59.042+0000] {spark_submit.py:571} INFO - "queryPlanning" : 97,
[2024-05-18T21:58:59.042+0000] {spark_submit.py:571} INFO - "triggerExecution" : 2383,
[2024-05-18T21:58:59.042+0000] {spark_submit.py:571} INFO - "walCommit" : 93
[2024-05-18T21:58:59.042+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:58:59.043+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:58:59.043+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:58:59.043+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:58:59.043+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:58:59.043+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:58:59.043+0000] {spark_submit.py:571} INFO - "0" : 30
[2024-05-18T21:58:59.043+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:58:59.044+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:58:59.044+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:58:59.044+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:58:59.044+0000] {spark_submit.py:571} INFO - "0" : 31
[2024-05-18T21:58:59.044+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:58:59.044+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:58:59.044+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:58:59.045+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:58:59.045+0000] {spark_submit.py:571} INFO - "0" : 31
[2024-05-18T21:58:59.045+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:58:59.045+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:58:59.045+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:58:59.045+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-18T21:58:59.045+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.419639110365086,
[2024-05-18T21:58:59.045+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:58:59.046+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:58:59.046+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:58:59.046+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:58:59.046+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:58:59.046+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:58:59.046+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:58:59.046+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:58:59.046+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:58:59.047+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:58:59.047+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:59:09.054+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:59:18.830+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:18 INFO BlockManagerInfo: Removed broadcast_28_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:59:18.871+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:18 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:59:19.054+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:59:22.532+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/29 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.29.9e1b4f83-e9f9-4a64-b91a-ca989441fe6b.tmp
[2024-05-18T21:59:22.631+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.29.9e1b4f83-e9f9-4a64-b91a-ca989441fe6b.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/29
[2024-05-18T21:59:22.636+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:22 INFO MicroBatchExecution: Committed offsets for batch 29. Metadata OffsetSeqMetadata(0,1716069562440,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:59:22.745+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:59:22.749+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:59:22.791+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:59:22.793+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:59:22.829+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:59:22.910+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:22 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:59:22.912+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:22 INFO DAGScheduler: Got job 29 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:59:22.912+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:22 INFO DAGScheduler: Final stage: ResultStage 29 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:59:22.912+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:22 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:59:22.913+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:22 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:59:22.914+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:22 INFO DAGScheduler: Submitting ResultStage 29 (PythonRDD[299] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:59:22.918+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:22 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T21:59:22.921+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:22 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T21:59:22.922+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:22 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:59:22.922+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:22 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:59:22.923+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (PythonRDD[299] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:59:22.923+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:22 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
[2024-05-18T21:59:22.927+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:22 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 29) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:59:23.011+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:23 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:59:24.015+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:24 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 29) in 1085 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:59:24.026+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:24 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool
[2024-05-18T21:59:24.027+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:24 INFO DAGScheduler: ResultStage 29 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 1.103 s
[2024-05-18T21:59:24.028+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:24 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:59:24.028+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
[2024-05-18T21:59:24.028+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:24 INFO DAGScheduler: Job 29 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 1.111458 s
[2024-05-18T21:59:24.137+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/29 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.29.0236f025-c897-4e8a-85d8-325b1dafcf80.tmp
[2024-05-18T21:59:24.303+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:24 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.29.0236f025-c897-4e8a-85d8-325b1dafcf80.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/29
[2024-05-18T21:59:24.309+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:24 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:59:24.310+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:59:24.311+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:59:24.312+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:59:24.312+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:59:22.437Z",
[2024-05-18T21:59:24.313+0000] {spark_submit.py:571} INFO - "batchId" : 29,
[2024-05-18T21:59:24.313+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:59:24.314+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-05-18T21:59:24.314+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.5359056806002144,
[2024-05-18T21:59:24.314+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:59:24.315+0000] {spark_submit.py:571} INFO - "addBatch" : 1292,
[2024-05-18T21:59:24.315+0000] {spark_submit.py:571} INFO - "commitOffsets" : 252,
[2024-05-18T21:59:24.315+0000] {spark_submit.py:571} INFO - "getBatch" : 5,
[2024-05-18T21:59:24.316+0000] {spark_submit.py:571} INFO - "latestOffset" : 3,
[2024-05-18T21:59:24.316+0000] {spark_submit.py:571} INFO - "queryPlanning" : 104,
[2024-05-18T21:59:24.316+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1866,
[2024-05-18T21:59:24.316+0000] {spark_submit.py:571} INFO - "walCommit" : 200
[2024-05-18T21:59:24.317+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:59:24.317+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:59:24.317+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:59:24.319+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:59:24.321+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:59:24.321+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:59:24.322+0000] {spark_submit.py:571} INFO - "0" : 31
[2024-05-18T21:59:24.322+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:59:24.323+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:59:24.323+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:59:24.323+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:59:24.324+0000] {spark_submit.py:571} INFO - "0" : 32
[2024-05-18T21:59:24.324+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:59:24.324+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:59:24.324+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:59:24.325+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:59:24.325+0000] {spark_submit.py:571} INFO - "0" : 32
[2024-05-18T21:59:24.325+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:59:24.326+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:59:24.327+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:59:24.328+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-05-18T21:59:24.328+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.5359056806002144,
[2024-05-18T21:59:24.328+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:59:24.328+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:59:24.328+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:59:24.328+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:59:24.328+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:59:24.329+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:59:24.329+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:59:24.329+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:59:24.329+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:59:24.329+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:59:24.329+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:59:34.313+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:59:44.320+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T21:59:44.846+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/30 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.30.5710f84b-b83e-4566-a8df-cd00a21c2e62.tmp
[2024-05-18T21:59:44.894+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.30.5710f84b-b83e-4566-a8df-cd00a21c2e62.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/30
[2024-05-18T21:59:44.895+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:44 INFO MicroBatchExecution: Committed offsets for batch 30. Metadata OffsetSeqMetadata(0,1716069584820,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T21:59:44.938+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:59:44.941+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:59:44.958+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:59:44.959+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T21:59:44.994+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T21:59:45.054+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:45 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T21:59:45.055+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:45 INFO DAGScheduler: Got job 30 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T21:59:45.056+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:45 INFO DAGScheduler: Final stage: ResultStage 30 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T21:59:45.056+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:45 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T21:59:45.056+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:45 INFO DAGScheduler: Missing parents: List()
[2024-05-18T21:59:45.056+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:45 INFO DAGScheduler: Submitting ResultStage 30 (PythonRDD[309] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T21:59:45.069+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:45 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 69.8 KiB, free 434.2 MiB)
[2024-05-18T21:59:45.092+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:45 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.2 MiB)
[2024-05-18T21:59:45.093+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:45 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:59:45.093+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:45 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1585
[2024-05-18T21:59:45.094+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (PythonRDD[309] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T21:59:45.094+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:45 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
[2024-05-18T21:59:45.096+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:45 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 30) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T21:59:45.148+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:45 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:59:45.952+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:45 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 30) in 855 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T21:59:45.953+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:45 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
[2024-05-18T21:59:45.955+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:45 INFO DAGScheduler: ResultStage 30 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.897 s
[2024-05-18T21:59:45.957+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:45 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T21:59:45.959+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
[2024-05-18T21:59:45.961+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:45 INFO DAGScheduler: Job 30 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.902292 s
[2024-05-18T21:59:45.984+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/30 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.30.f66cfdcf-2c9c-4efd-bed1-e82c65d94b64.tmp
[2024-05-18T21:59:46.025+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.30.f66cfdcf-2c9c-4efd-bed1-e82c65d94b64.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/30
[2024-05-18T21:59:46.028+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:46 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T21:59:46.028+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T21:59:46.028+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T21:59:46.029+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T21:59:46.029+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T21:59:44.817Z",
[2024-05-18T21:59:46.029+0000] {spark_submit.py:571} INFO - "batchId" : 30,
[2024-05-18T21:59:46.029+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:59:46.030+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:59:46.030+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8278145695364238,
[2024-05-18T21:59:46.030+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T21:59:46.030+0000] {spark_submit.py:571} INFO - "addBatch" : 1020,
[2024-05-18T21:59:46.030+0000] {spark_submit.py:571} INFO - "commitOffsets" : 59,
[2024-05-18T21:59:46.030+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-18T21:59:46.030+0000] {spark_submit.py:571} INFO - "latestOffset" : 3,
[2024-05-18T21:59:46.031+0000] {spark_submit.py:571} INFO - "queryPlanning" : 45,
[2024-05-18T21:59:46.031+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1208,
[2024-05-18T21:59:46.031+0000] {spark_submit.py:571} INFO - "walCommit" : 75
[2024-05-18T21:59:46.031+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:59:46.031+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T21:59:46.031+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T21:59:46.031+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T21:59:46.031+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T21:59:46.031+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:59:46.032+0000] {spark_submit.py:571} INFO - "0" : 32
[2024-05-18T21:59:46.032+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:59:46.032+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:59:46.032+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T21:59:46.032+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:59:46.032+0000] {spark_submit.py:571} INFO - "0" : 33
[2024-05-18T21:59:46.032+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:59:46.032+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:59:46.032+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T21:59:46.033+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T21:59:46.033+0000] {spark_submit.py:571} INFO - "0" : 33
[2024-05-18T21:59:46.033+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:59:46.033+0000] {spark_submit.py:571} INFO - },
[2024-05-18T21:59:46.033+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T21:59:46.033+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T21:59:46.033+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8278145695364238,
[2024-05-18T21:59:46.033+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T21:59:46.034+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T21:59:46.034+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T21:59:46.034+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T21:59:46.034+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:59:46.034+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T21:59:46.034+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T21:59:46.034+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T21:59:46.034+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T21:59:46.035+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:59:46.035+0000] {spark_submit.py:571} INFO - }
[2024-05-18T21:59:46.917+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:46 INFO BlockManagerInfo: Removed broadcast_30_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:59:46.922+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:46 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:59:46.928+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:46 INFO BlockManagerInfo: Removed broadcast_29_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:59:46.929+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:46 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T21:59:56.049+0000] {spark_submit.py:571} INFO - 24/05/18 21:59:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:00:06.047+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:00:16.055+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:00:17.547+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/31 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.31.73cdb3f4-c7c0-4e5d-8b6b-316358fbef89.tmp
[2024-05-18T22:00:17.613+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.31.73cdb3f4-c7c0-4e5d-8b6b-316358fbef89.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/31
[2024-05-18T22:00:17.615+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:17 INFO MicroBatchExecution: Committed offsets for batch 31. Metadata OffsetSeqMetadata(0,1716069617484,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T22:00:17.750+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:00:17.766+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:00:17.823+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:00:17.826+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:00:17.871+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T22:00:17.910+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:17 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T22:00:17.912+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:17 INFO DAGScheduler: Got job 31 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T22:00:17.912+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:17 INFO DAGScheduler: Final stage: ResultStage 31 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T22:00:17.913+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:17 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T22:00:17.914+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:17 INFO DAGScheduler: Missing parents: List()
[2024-05-18T22:00:17.914+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:17 INFO DAGScheduler: Submitting ResultStage 31 (PythonRDD[319] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T22:00:17.918+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:17 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T22:00:17.921+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:17 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T22:00:17.921+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:17 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:00:17.922+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:17 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1585
[2024-05-18T22:00:17.922+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (PythonRDD[319] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T22:00:17.923+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:17 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0
[2024-05-18T22:00:17.924+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:17 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 31) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T22:00:17.994+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:17 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:00:18.752+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:18 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 31) in 824 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T22:00:18.758+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:18 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool
[2024-05-18T22:00:18.759+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:18 INFO DAGScheduler: ResultStage 31 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.840 s
[2024-05-18T22:00:18.759+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:18 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T22:00:18.760+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished
[2024-05-18T22:00:18.762+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:18 INFO DAGScheduler: Job 31 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.850075 s
[2024-05-18T22:00:18.832+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/31 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.31.753d8937-56a7-4222-972e-de0862d9df4a.tmp
[2024-05-18T22:00:18.869+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.31.753d8937-56a7-4222-972e-de0862d9df4a.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/31
[2024-05-18T22:00:18.890+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:18 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T22:00:18.893+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T22:00:18.894+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T22:00:18.895+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T22:00:18.896+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T22:00:17.479Z",
[2024-05-18T22:00:18.896+0000] {spark_submit.py:571} INFO - "batchId" : 31,
[2024-05-18T22:00:18.897+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T22:00:18.898+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-18T22:00:18.898+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7194244604316548,
[2024-05-18T22:00:18.898+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T22:00:18.898+0000] {spark_submit.py:571} INFO - "addBatch" : 992,
[2024-05-18T22:00:18.899+0000] {spark_submit.py:571} INFO - "commitOffsets" : 101,
[2024-05-18T22:00:18.899+0000] {spark_submit.py:571} INFO - "getBatch" : 1,
[2024-05-18T22:00:18.899+0000] {spark_submit.py:571} INFO - "latestOffset" : 5,
[2024-05-18T22:00:18.899+0000] {spark_submit.py:571} INFO - "queryPlanning" : 151,
[2024-05-18T22:00:18.899+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1390,
[2024-05-18T22:00:18.900+0000] {spark_submit.py:571} INFO - "walCommit" : 130
[2024-05-18T22:00:18.901+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:00:18.901+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T22:00:18.901+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T22:00:18.902+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T22:00:18.902+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T22:00:18.903+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:00:18.903+0000] {spark_submit.py:571} INFO - "0" : 33
[2024-05-18T22:00:18.904+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:00:18.904+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:00:18.904+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T22:00:18.904+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:00:18.905+0000] {spark_submit.py:571} INFO - "0" : 34
[2024-05-18T22:00:18.905+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:00:18.905+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:00:18.905+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T22:00:18.905+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:00:18.906+0000] {spark_submit.py:571} INFO - "0" : 34
[2024-05-18T22:00:18.906+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:00:18.906+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:00:18.906+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T22:00:18.906+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-18T22:00:18.906+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7194244604316548,
[2024-05-18T22:00:18.907+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T22:00:18.907+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T22:00:18.908+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T22:00:18.908+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T22:00:18.909+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:00:18.909+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T22:00:18.910+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T22:00:18.910+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T22:00:18.910+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T22:00:18.910+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:00:18.910+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:00:20.042+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:20 INFO BlockManagerInfo: Removed broadcast_31_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:00:20.058+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:20 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:00:28.888+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:00:38.886+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:00:48.888+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:00:58.897+0000] {spark_submit.py:571} INFO - 24/05/18 22:00:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:01:08.907+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:01:18.914+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:01:27.541+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/32 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.32.a0a8331c-a23b-4f71-863d-90995cbbf64c.tmp
[2024-05-18T22:01:27.622+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.32.a0a8331c-a23b-4f71-863d-90995cbbf64c.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/32
[2024-05-18T22:01:27.623+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:27 INFO MicroBatchExecution: Committed offsets for batch 32. Metadata OffsetSeqMetadata(0,1716069687469,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T22:01:27.700+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:01:27.703+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:01:27.721+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:01:27.722+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:01:27.768+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T22:01:27.808+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:27 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T22:01:27.810+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:27 INFO DAGScheduler: Got job 32 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T22:01:27.810+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:27 INFO DAGScheduler: Final stage: ResultStage 32 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T22:01:27.811+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:27 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T22:01:27.811+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:27 INFO DAGScheduler: Missing parents: List()
[2024-05-18T22:01:27.811+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:27 INFO DAGScheduler: Submitting ResultStage 32 (PythonRDD[329] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T22:01:27.822+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:27 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T22:01:27.836+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:27 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T22:01:27.843+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:27 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:01:27.844+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:27 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1585
[2024-05-18T22:01:27.846+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (PythonRDD[329] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T22:01:27.848+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:27 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
[2024-05-18T22:01:27.851+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:27 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 32) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T22:01:27.910+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:27 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:01:28.808+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:28 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 32) in 965 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T22:01:28.813+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:28 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool
[2024-05-18T22:01:28.814+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:28 INFO DAGScheduler: ResultStage 32 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.998 s
[2024-05-18T22:01:28.816+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:28 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T22:01:28.821+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
[2024-05-18T22:01:28.821+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:28 INFO DAGScheduler: Job 32 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 1.007760 s
[2024-05-18T22:01:28.860+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:28 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/32 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.32.7537de31-7169-4124-b1b1-0ca3dfecbd3e.tmp
[2024-05-18T22:01:28.895+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:28 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.32.7537de31-7169-4124-b1b1-0ca3dfecbd3e.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/32
[2024-05-18T22:01:28.905+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:28 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T22:01:28.906+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T22:01:28.906+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T22:01:28.906+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T22:01:28.907+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T22:01:27.468Z",
[2024-05-18T22:01:28.907+0000] {spark_submit.py:571} INFO - "batchId" : 32,
[2024-05-18T22:01:28.907+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T22:01:28.907+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T22:01:28.907+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.700280112044818,
[2024-05-18T22:01:28.907+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T22:01:28.908+0000] {spark_submit.py:571} INFO - "addBatch" : 1129,
[2024-05-18T22:01:28.908+0000] {spark_submit.py:571} INFO - "commitOffsets" : 56,
[2024-05-18T22:01:28.908+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-18T22:01:28.908+0000] {spark_submit.py:571} INFO - "latestOffset" : 1,
[2024-05-18T22:01:28.908+0000] {spark_submit.py:571} INFO - "queryPlanning" : 70,
[2024-05-18T22:01:28.908+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1428,
[2024-05-18T22:01:28.908+0000] {spark_submit.py:571} INFO - "walCommit" : 159
[2024-05-18T22:01:28.909+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:01:28.909+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T22:01:28.909+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T22:01:28.909+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T22:01:28.909+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T22:01:28.909+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:01:28.909+0000] {spark_submit.py:571} INFO - "0" : 34
[2024-05-18T22:01:28.909+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:01:28.910+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:01:28.910+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T22:01:28.910+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:01:28.910+0000] {spark_submit.py:571} INFO - "0" : 35
[2024-05-18T22:01:28.910+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:01:28.910+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:01:28.910+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T22:01:28.910+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:01:28.910+0000] {spark_submit.py:571} INFO - "0" : 35
[2024-05-18T22:01:28.911+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:01:28.911+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:01:28.911+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T22:01:28.911+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T22:01:28.911+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.700280112044818,
[2024-05-18T22:01:28.911+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T22:01:28.911+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T22:01:28.911+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T22:01:28.912+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T22:01:28.912+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:01:28.912+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T22:01:28.912+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T22:01:28.912+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T22:01:28.912+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T22:01:28.912+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:01:28.912+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:01:31.512+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:31 INFO BlockManagerInfo: Removed broadcast_32_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:01:31.531+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:31 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:01:38.914+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:01:48.919+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:01:51.706+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/33 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.33.bc07cb0e-fbff-4e00-b55d-d68a99cd0984.tmp
[2024-05-18T22:01:51.739+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.33.bc07cb0e-fbff-4e00-b55d-d68a99cd0984.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/33
[2024-05-18T22:01:51.740+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:51 INFO MicroBatchExecution: Committed offsets for batch 33. Metadata OffsetSeqMetadata(0,1716069711665,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T22:01:51.791+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:01:51.794+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:01:51.807+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:01:51.808+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:01:51.869+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T22:01:51.906+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:51 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T22:01:51.908+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:51 INFO DAGScheduler: Got job 33 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T22:01:51.908+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:51 INFO DAGScheduler: Final stage: ResultStage 33 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T22:01:51.909+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:51 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T22:01:51.909+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:51 INFO DAGScheduler: Missing parents: List()
[2024-05-18T22:01:51.909+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:51 INFO DAGScheduler: Submitting ResultStage 33 (PythonRDD[339] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T22:01:51.919+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:51 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T22:01:51.926+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:51 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T22:01:51.927+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:51 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:01:51.928+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:51 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1585
[2024-05-18T22:01:51.930+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (PythonRDD[339] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T22:01:51.931+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:51 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
[2024-05-18T22:01:51.937+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:51 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 33) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T22:01:52.007+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:52 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:01:52.735+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:52 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 33) in 800 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T22:01:52.736+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:52 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool
[2024-05-18T22:01:52.737+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:52 INFO DAGScheduler: ResultStage 33 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.822 s
[2024-05-18T22:01:52.737+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:52 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T22:01:52.738+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
[2024-05-18T22:01:52.738+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:52 INFO DAGScheduler: Job 33 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.831333 s
[2024-05-18T22:01:52.783+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/33 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.33.622123e5-6844-4910-9420-0ed12f1ffad9.tmp
[2024-05-18T22:01:52.820+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.33.622123e5-6844-4910-9420-0ed12f1ffad9.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/33
[2024-05-18T22:01:52.822+0000] {spark_submit.py:571} INFO - 24/05/18 22:01:52 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T22:01:52.823+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T22:01:52.823+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T22:01:52.823+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T22:01:52.823+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T22:01:51.663Z",
[2024-05-18T22:01:52.823+0000] {spark_submit.py:571} INFO - "batchId" : 33,
[2024-05-18T22:01:52.824+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T22:01:52.824+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 52.631578947368425,
[2024-05-18T22:01:52.824+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8643042350907519,
[2024-05-18T22:01:52.824+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T22:01:52.824+0000] {spark_submit.py:571} INFO - "addBatch" : 942,
[2024-05-18T22:01:52.825+0000] {spark_submit.py:571} INFO - "commitOffsets" : 77,
[2024-05-18T22:01:52.825+0000] {spark_submit.py:571} INFO - "getBatch" : 1,
[2024-05-18T22:01:52.825+0000] {spark_submit.py:571} INFO - "latestOffset" : 2,
[2024-05-18T22:01:52.825+0000] {spark_submit.py:571} INFO - "queryPlanning" : 54,
[2024-05-18T22:01:52.825+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1157,
[2024-05-18T22:01:52.825+0000] {spark_submit.py:571} INFO - "walCommit" : 75
[2024-05-18T22:01:52.825+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:01:52.826+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T22:01:52.826+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T22:01:52.826+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T22:01:52.826+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T22:01:52.826+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:01:52.826+0000] {spark_submit.py:571} INFO - "0" : 35
[2024-05-18T22:01:52.826+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:01:52.827+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:01:52.827+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T22:01:52.827+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:01:52.827+0000] {spark_submit.py:571} INFO - "0" : 36
[2024-05-18T22:01:52.827+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:01:52.827+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:01:52.827+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T22:01:52.827+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:01:52.828+0000] {spark_submit.py:571} INFO - "0" : 36
[2024-05-18T22:01:52.828+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:01:52.828+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:01:52.828+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T22:01:52.828+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 52.631578947368425,
[2024-05-18T22:01:52.828+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8643042350907519,
[2024-05-18T22:01:52.828+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T22:01:52.828+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T22:01:52.828+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T22:01:52.829+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T22:01:52.829+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:01:52.829+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T22:01:52.829+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T22:01:52.829+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T22:01:52.829+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T22:01:52.829+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:01:52.829+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:02:02.840+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:02:04.601+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:04 INFO BlockManagerInfo: Removed broadcast_33_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:02:04.620+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:04 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:02:12.839+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:02:22.850+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:02:27.786+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/34 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.34.75475ff9-75a9-4e28-9e43-e5c98cc562cb.tmp
[2024-05-18T22:02:27.820+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.34.75475ff9-75a9-4e28-9e43-e5c98cc562cb.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/34
[2024-05-18T22:02:27.820+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:27 INFO MicroBatchExecution: Committed offsets for batch 34. Metadata OffsetSeqMetadata(0,1716069747743,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T22:02:27.868+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:02:27.870+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:02:27.884+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:02:27.885+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:02:27.917+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T22:02:27.976+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:27 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T22:02:27.985+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:27 INFO DAGScheduler: Got job 34 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T22:02:27.986+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:27 INFO DAGScheduler: Final stage: ResultStage 34 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T22:02:27.986+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:27 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T22:02:27.987+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:27 INFO DAGScheduler: Missing parents: List()
[2024-05-18T22:02:27.991+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:27 INFO DAGScheduler: Submitting ResultStage 34 (PythonRDD[349] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T22:02:27.995+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:27 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T22:02:28.000+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:27 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T22:02:28.000+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:28 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:02:28.001+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:28 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1585
[2024-05-18T22:02:28.001+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (PythonRDD[349] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T22:02:28.002+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:28 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0
[2024-05-18T22:02:28.003+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:28 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 34) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T22:02:28.036+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:28 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:02:28.879+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:28 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 34) in 873 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T22:02:28.882+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:28 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool
[2024-05-18T22:02:28.884+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:28 INFO DAGScheduler: ResultStage 34 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.890 s
[2024-05-18T22:02:28.886+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:28 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T22:02:28.887+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
[2024-05-18T22:02:28.890+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:28 INFO DAGScheduler: Job 34 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.910570 s
[2024-05-18T22:02:28.933+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:28 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/34 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.34.742cd4fa-bc56-408a-966f-b9d909840974.tmp
[2024-05-18T22:02:28.962+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:28 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.34.742cd4fa-bc56-408a-966f-b9d909840974.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/34
[2024-05-18T22:02:28.965+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:28 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T22:02:28.966+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T22:02:28.966+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T22:02:28.966+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T22:02:28.966+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T22:02:27.741Z",
[2024-05-18T22:02:28.966+0000] {spark_submit.py:571} INFO - "batchId" : 34,
[2024-05-18T22:02:28.967+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T22:02:28.967+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T22:02:28.967+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8190008190008189,
[2024-05-18T22:02:28.967+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T22:02:28.967+0000] {spark_submit.py:571} INFO - "addBatch" : 1022,
[2024-05-18T22:02:28.967+0000] {spark_submit.py:571} INFO - "commitOffsets" : 64,
[2024-05-18T22:02:28.967+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-18T22:02:28.968+0000] {spark_submit.py:571} INFO - "latestOffset" : 2,
[2024-05-18T22:02:28.968+0000] {spark_submit.py:571} INFO - "queryPlanning" : 50,
[2024-05-18T22:02:28.968+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1221,
[2024-05-18T22:02:28.968+0000] {spark_submit.py:571} INFO - "walCommit" : 77
[2024-05-18T22:02:28.968+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:02:28.968+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T22:02:28.968+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T22:02:28.968+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T22:02:28.969+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T22:02:28.969+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:02:28.969+0000] {spark_submit.py:571} INFO - "0" : 36
[2024-05-18T22:02:28.969+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:02:28.969+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:02:28.969+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T22:02:28.969+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:02:28.969+0000] {spark_submit.py:571} INFO - "0" : 37
[2024-05-18T22:02:28.969+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:02:28.970+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:02:28.970+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T22:02:28.970+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:02:28.970+0000] {spark_submit.py:571} INFO - "0" : 37
[2024-05-18T22:02:28.970+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:02:28.970+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:02:28.970+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T22:02:28.970+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T22:02:28.970+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8190008190008189,
[2024-05-18T22:02:28.971+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T22:02:28.971+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T22:02:28.971+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T22:02:28.971+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T22:02:28.971+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:02:28.971+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T22:02:28.971+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T22:02:28.971+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T22:02:28.971+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T22:02:28.972+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:02:28.972+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:02:37.151+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:37 INFO BlockManagerInfo: Removed broadcast_34_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:02:37.157+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:37 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:02:38.965+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:02:48.986+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:02:58.990+0000] {spark_submit.py:571} INFO - 24/05/18 22:02:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:03:00.050+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/35 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.35.2bf01341-0172-4ac6-a023-8d76455aeb83.tmp
[2024-05-18T22:03:00.105+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.35.2bf01341-0172-4ac6-a023-8d76455aeb83.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/35
[2024-05-18T22:03:00.105+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:00 INFO MicroBatchExecution: Committed offsets for batch 35. Metadata OffsetSeqMetadata(0,1716069779973,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T22:03:00.176+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:03:00.181+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:03:00.239+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:03:00.241+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:03:00.301+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T22:03:00.374+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:00 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T22:03:00.376+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:00 INFO DAGScheduler: Got job 35 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T22:03:00.377+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:00 INFO DAGScheduler: Final stage: ResultStage 35 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T22:03:00.377+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:00 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T22:03:00.377+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:00 INFO DAGScheduler: Missing parents: List()
[2024-05-18T22:03:00.378+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:00 INFO DAGScheduler: Submitting ResultStage 35 (PythonRDD[359] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T22:03:00.383+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:00 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T22:03:00.385+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:00 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T22:03:00.386+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:00 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:03:00.386+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:00 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1585
[2024-05-18T22:03:00.386+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (PythonRDD[359] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T22:03:00.387+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:00 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
[2024-05-18T22:03:00.390+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:00 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 35) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T22:03:00.459+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:00 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:03:01.404+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:01 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 35) in 1010 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T22:03:01.411+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:01 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool
[2024-05-18T22:03:01.415+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:01 INFO DAGScheduler: ResultStage 35 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 1.028 s
[2024-05-18T22:03:01.421+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:01 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T22:03:01.422+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
[2024-05-18T22:03:01.425+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:01 INFO DAGScheduler: Job 35 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 1.050342 s
[2024-05-18T22:03:01.571+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/35 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.35.4e616bdd-63c2-48f0-bf52-43bd09800688.tmp
[2024-05-18T22:03:01.676+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.35.4e616bdd-63c2-48f0-bf52-43bd09800688.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/35
[2024-05-18T22:03:01.708+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:01 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T22:03:01.710+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T22:03:01.710+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T22:03:01.711+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T22:03:01.711+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T22:02:59.967Z",
[2024-05-18T22:03:01.711+0000] {spark_submit.py:571} INFO - "batchId" : 35,
[2024-05-18T22:03:01.712+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T22:03:01.712+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-05-18T22:03:01.713+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.5851375073142188,
[2024-05-18T22:03:01.714+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T22:03:01.714+0000] {spark_submit.py:571} INFO - "addBatch" : 1248,
[2024-05-18T22:03:01.715+0000] {spark_submit.py:571} INFO - "commitOffsets" : 207,
[2024-05-18T22:03:01.716+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-18T22:03:01.716+0000] {spark_submit.py:571} INFO - "latestOffset" : 6,
[2024-05-18T22:03:01.716+0000] {spark_submit.py:571} INFO - "queryPlanning" : 76,
[2024-05-18T22:03:01.716+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1709,
[2024-05-18T22:03:01.717+0000] {spark_submit.py:571} INFO - "walCommit" : 133
[2024-05-18T22:03:01.718+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:03:01.718+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T22:03:01.719+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T22:03:01.719+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T22:03:01.719+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T22:03:01.720+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:03:01.721+0000] {spark_submit.py:571} INFO - "0" : 37
[2024-05-18T22:03:01.721+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:03:01.722+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:03:01.722+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T22:03:01.723+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:03:01.723+0000] {spark_submit.py:571} INFO - "0" : 38
[2024-05-18T22:03:01.723+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:03:01.723+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:03:01.724+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T22:03:01.724+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:03:01.724+0000] {spark_submit.py:571} INFO - "0" : 38
[2024-05-18T22:03:01.725+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:03:01.725+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:03:01.725+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T22:03:01.726+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-05-18T22:03:01.726+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.5851375073142188,
[2024-05-18T22:03:01.726+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T22:03:01.727+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T22:03:01.727+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T22:03:01.728+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T22:03:01.728+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:03:01.729+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T22:03:01.730+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T22:03:01.730+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T22:03:01.730+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T22:03:01.731+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:03:01.731+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:03:10.657+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:10 INFO BlockManagerInfo: Removed broadcast_35_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:03:10.673+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:10 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:03:11.711+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:03:21.712+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:03:28.203+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:28 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/36 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.36.f9a09135-5268-4abb-be08-16b2565725a3.tmp
[2024-05-18T22:03:28.237+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:28 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.36.f9a09135-5268-4abb-be08-16b2565725a3.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/36
[2024-05-18T22:03:28.238+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:28 INFO MicroBatchExecution: Committed offsets for batch 36. Metadata OffsetSeqMetadata(0,1716069808167,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T22:03:28.289+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:03:28.291+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:03:28.322+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:03:28.323+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:03:28.385+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T22:03:28.457+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:28 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T22:03:28.459+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:28 INFO DAGScheduler: Got job 36 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T22:03:28.460+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:28 INFO DAGScheduler: Final stage: ResultStage 36 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T22:03:28.461+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:28 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T22:03:28.462+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:28 INFO DAGScheduler: Missing parents: List()
[2024-05-18T22:03:28.463+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:28 INFO DAGScheduler: Submitting ResultStage 36 (PythonRDD[369] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T22:03:28.467+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:28 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T22:03:28.470+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:28 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T22:03:28.472+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:28 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:03:28.473+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:28 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1585
[2024-05-18T22:03:28.474+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (PythonRDD[369] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T22:03:28.474+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:28 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
[2024-05-18T22:03:28.475+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:28 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 36) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T22:03:28.548+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:28 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:03:29.322+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:29 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 36) in 845 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T22:03:29.336+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:29 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool
[2024-05-18T22:03:29.337+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:29 INFO DAGScheduler: ResultStage 36 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.863 s
[2024-05-18T22:03:29.342+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:29 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T22:03:29.343+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished
[2024-05-18T22:03:29.344+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:29 INFO DAGScheduler: Job 36 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.875365 s
[2024-05-18T22:03:29.419+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/36 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.36.4f59c668-c7ff-47c9-90cc-aef0ead9462b.tmp
[2024-05-18T22:03:29.453+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.36.4f59c668-c7ff-47c9-90cc-aef0ead9462b.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/36
[2024-05-18T22:03:29.476+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:29 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T22:03:29.477+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T22:03:29.477+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T22:03:29.477+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T22:03:29.477+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T22:03:28.166Z",
[2024-05-18T22:03:29.477+0000] {spark_submit.py:571} INFO - "batchId" : 36,
[2024-05-18T22:03:29.477+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T22:03:29.478+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T22:03:29.478+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7770007770007771,
[2024-05-18T22:03:29.478+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T22:03:29.478+0000] {spark_submit.py:571} INFO - "addBatch" : 1090,
[2024-05-18T22:03:29.478+0000] {spark_submit.py:571} INFO - "commitOffsets" : 61,
[2024-05-18T22:03:29.479+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-18T22:03:29.479+0000] {spark_submit.py:571} INFO - "latestOffset" : 1,
[2024-05-18T22:03:29.479+0000] {spark_submit.py:571} INFO - "queryPlanning" : 52,
[2024-05-18T22:03:29.479+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1287,
[2024-05-18T22:03:29.479+0000] {spark_submit.py:571} INFO - "walCommit" : 72
[2024-05-18T22:03:29.480+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:03:29.480+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T22:03:29.480+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T22:03:29.480+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T22:03:29.480+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T22:03:29.480+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:03:29.480+0000] {spark_submit.py:571} INFO - "0" : 38
[2024-05-18T22:03:29.481+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:03:29.481+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:03:29.481+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T22:03:29.481+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:03:29.481+0000] {spark_submit.py:571} INFO - "0" : 39
[2024-05-18T22:03:29.481+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:03:29.481+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:03:29.481+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T22:03:29.482+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:03:29.482+0000] {spark_submit.py:571} INFO - "0" : 39
[2024-05-18T22:03:29.482+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:03:29.482+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:03:29.482+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T22:03:29.482+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 76.92307692307692,
[2024-05-18T22:03:29.482+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7770007770007771,
[2024-05-18T22:03:29.483+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T22:03:29.483+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T22:03:29.483+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T22:03:29.483+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T22:03:29.483+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:03:29.483+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T22:03:29.483+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T22:03:29.484+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T22:03:29.484+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T22:03:29.484+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:03:29.484+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:03:39.479+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:03:43.623+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:43 INFO BlockManagerInfo: Removed broadcast_36_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:03:43.637+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:43 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:03:49.487+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:03:55.017+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/37 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.37.f0bdbf2e-4cfc-4635-8c18-2885a93fd6e7.tmp
[2024-05-18T22:03:55.062+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.37.f0bdbf2e-4cfc-4635-8c18-2885a93fd6e7.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/37
[2024-05-18T22:03:55.065+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:55 INFO MicroBatchExecution: Committed offsets for batch 37. Metadata OffsetSeqMetadata(0,1716069834969,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T22:03:55.117+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:03:55.120+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:03:55.135+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:03:55.135+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:03:55.158+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T22:03:55.205+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:55 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T22:03:55.207+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:55 INFO DAGScheduler: Got job 37 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T22:03:55.207+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:55 INFO DAGScheduler: Final stage: ResultStage 37 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T22:03:55.208+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:55 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T22:03:55.208+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:55 INFO DAGScheduler: Missing parents: List()
[2024-05-18T22:03:55.208+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:55 INFO DAGScheduler: Submitting ResultStage 37 (PythonRDD[379] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T22:03:55.213+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:55 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T22:03:55.216+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:55 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T22:03:55.216+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:55 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:03:55.216+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:55 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1585
[2024-05-18T22:03:55.217+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (PythonRDD[379] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T22:03:55.217+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:55 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0
[2024-05-18T22:03:55.219+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:55 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 37) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T22:03:55.271+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:55 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:03:56.061+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:56 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 37) in 841 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T22:03:56.067+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:56 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool
[2024-05-18T22:03:56.071+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:56 INFO DAGScheduler: ResultStage 37 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.855 s
[2024-05-18T22:03:56.072+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:56 INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T22:03:56.073+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished
[2024-05-18T22:03:56.073+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:56 INFO DAGScheduler: Job 37 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.861309 s
[2024-05-18T22:03:56.158+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/37 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.37.566fb723-42d7-449c-b282-56d22ba08d8b.tmp
[2024-05-18T22:03:56.215+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.37.566fb723-42d7-449c-b282-56d22ba08d8b.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/37
[2024-05-18T22:03:56.221+0000] {spark_submit.py:571} INFO - 24/05/18 22:03:56 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T22:03:56.221+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T22:03:56.221+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T22:03:56.222+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T22:03:56.222+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T22:03:54.967Z",
[2024-05-18T22:03:56.222+0000] {spark_submit.py:571} INFO - "batchId" : 37,
[2024-05-18T22:03:56.222+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T22:03:56.223+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-05-18T22:03:56.223+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8012820512820513,
[2024-05-18T22:03:56.223+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T22:03:56.223+0000] {spark_submit.py:571} INFO - "addBatch" : 961,
[2024-05-18T22:03:56.224+0000] {spark_submit.py:571} INFO - "commitOffsets" : 128,
[2024-05-18T22:03:56.224+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-18T22:03:56.225+0000] {spark_submit.py:571} INFO - "latestOffset" : 2,
[2024-05-18T22:03:56.225+0000] {spark_submit.py:571} INFO - "queryPlanning" : 56,
[2024-05-18T22:03:56.225+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1248,
[2024-05-18T22:03:56.226+0000] {spark_submit.py:571} INFO - "walCommit" : 93
[2024-05-18T22:03:56.226+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:03:56.226+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T22:03:56.226+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T22:03:56.226+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T22:03:56.226+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T22:03:56.226+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:03:56.227+0000] {spark_submit.py:571} INFO - "0" : 39
[2024-05-18T22:03:56.227+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:03:56.227+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:03:56.227+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T22:03:56.227+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:03:56.227+0000] {spark_submit.py:571} INFO - "0" : 40
[2024-05-18T22:03:56.227+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:03:56.227+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:03:56.228+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T22:03:56.228+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:03:56.228+0000] {spark_submit.py:571} INFO - "0" : 40
[2024-05-18T22:03:56.228+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:03:56.228+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:03:56.228+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T22:03:56.230+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-05-18T22:03:56.230+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.8012820512820513,
[2024-05-18T22:03:56.230+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T22:03:56.230+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T22:03:56.231+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T22:03:56.231+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T22:03:56.231+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:03:56.231+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T22:03:56.231+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T22:03:56.231+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T22:03:56.231+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T22:03:56.231+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:03:56.231+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:04:06.225+0000] {spark_submit.py:571} INFO - 24/05/18 22:04:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:04:16.231+0000] {spark_submit.py:571} INFO - 24/05/18 22:04:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:04:16.714+0000] {spark_submit.py:571} INFO - 24/05/18 22:04:16 INFO BlockManagerInfo: Removed broadcast_37_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:04:16.724+0000] {spark_submit.py:571} INFO - 24/05/18 22:04:16 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:04:26.233+0000] {spark_submit.py:571} INFO - 24/05/18 22:04:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:04:36.240+0000] {spark_submit.py:571} INFO - 24/05/18 22:04:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:04:46.247+0000] {spark_submit.py:571} INFO - 24/05/18 22:04:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:04:56.253+0000] {spark_submit.py:571} INFO - 24/05/18 22:04:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:05:06.271+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:05:16.274+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:05:26.280+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:05:36.282+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:05:40.077+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/38 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.38.e77ad746-ac6a-474a-944f-f531d779a4c0.tmp
[2024-05-18T22:05:40.109+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.38.e77ad746-ac6a-474a-944f-f531d779a4c0.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/38
[2024-05-18T22:05:40.110+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:40 INFO MicroBatchExecution: Committed offsets for batch 38. Metadata OffsetSeqMetadata(0,1716069940029,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T22:05:40.176+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:05:40.180+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:05:40.216+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:05:40.217+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:05:40.244+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T22:05:40.299+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:40 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T22:05:40.301+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:40 INFO DAGScheduler: Got job 38 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T22:05:40.301+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:40 INFO DAGScheduler: Final stage: ResultStage 38 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T22:05:40.302+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:40 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T22:05:40.303+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:40 INFO DAGScheduler: Missing parents: List()
[2024-05-18T22:05:40.303+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:40 INFO DAGScheduler: Submitting ResultStage 38 (PythonRDD[389] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T22:05:40.306+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:40 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T22:05:40.308+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:40 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T22:05:40.309+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:40 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:05:40.310+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:40 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1585
[2024-05-18T22:05:40.310+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (PythonRDD[389] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T22:05:40.311+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:40 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks resource profile 0
[2024-05-18T22:05:40.313+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:40 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 38) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T22:05:40.415+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:40 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:05:41.230+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:41 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 38) in 915 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T22:05:41.235+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:41 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool
[2024-05-18T22:05:41.236+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:41 INFO DAGScheduler: ResultStage 38 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.930 s
[2024-05-18T22:05:41.237+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:41 INFO DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T22:05:41.238+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished
[2024-05-18T22:05:41.240+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:41 INFO DAGScheduler: Job 38 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.937215 s
[2024-05-18T22:05:41.285+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/38 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.38.e8fb90ee-1c4c-4dce-aa88-44f6af132aef.tmp
[2024-05-18T22:05:41.311+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.38.e8fb90ee-1c4c-4dce-aa88-44f6af132aef.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/38
[2024-05-18T22:05:41.315+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:41 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T22:05:41.315+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T22:05:41.316+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T22:05:41.316+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T22:05:41.316+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T22:05:40.025Z",
[2024-05-18T22:05:41.316+0000] {spark_submit.py:571} INFO - "batchId" : 38,
[2024-05-18T22:05:41.316+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T22:05:41.317+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-18T22:05:41.317+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7776049766718507,
[2024-05-18T22:05:41.317+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T22:05:41.317+0000] {spark_submit.py:571} INFO - "addBatch" : 1043,
[2024-05-18T22:05:41.317+0000] {spark_submit.py:571} INFO - "commitOffsets" : 64,
[2024-05-18T22:05:41.317+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-18T22:05:41.317+0000] {spark_submit.py:571} INFO - "latestOffset" : 4,
[2024-05-18T22:05:41.317+0000] {spark_submit.py:571} INFO - "queryPlanning" : 71,
[2024-05-18T22:05:41.318+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1286,
[2024-05-18T22:05:41.318+0000] {spark_submit.py:571} INFO - "walCommit" : 82
[2024-05-18T22:05:41.318+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:05:41.318+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T22:05:41.318+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T22:05:41.318+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T22:05:41.318+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T22:05:41.318+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:05:41.319+0000] {spark_submit.py:571} INFO - "0" : 40
[2024-05-18T22:05:41.319+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:05:41.319+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:05:41.319+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T22:05:41.319+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:05:41.319+0000] {spark_submit.py:571} INFO - "0" : 41
[2024-05-18T22:05:41.319+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:05:41.320+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:05:41.320+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T22:05:41.320+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:05:41.320+0000] {spark_submit.py:571} INFO - "0" : 41
[2024-05-18T22:05:41.320+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:05:41.320+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:05:41.320+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T22:05:41.320+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 83.33333333333333,
[2024-05-18T22:05:41.321+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7776049766718507,
[2024-05-18T22:05:41.321+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T22:05:41.321+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T22:05:41.321+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T22:05:41.321+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T22:05:41.321+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:05:41.321+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T22:05:41.321+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T22:05:41.321+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T22:05:41.322+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T22:05:41.322+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:05:41.322+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:05:51.328+0000] {spark_submit.py:571} INFO - 24/05/18 22:05:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:06:01.340+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:06:03.995+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/39 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.39.3ef99d76-7178-40cd-9fec-cdf17a841853.tmp
[2024-05-18T22:06:04.020+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.39.3ef99d76-7178-40cd-9fec-cdf17a841853.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/39
[2024-05-18T22:06:04.021+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:04 INFO MicroBatchExecution: Committed offsets for batch 39. Metadata OffsetSeqMetadata(0,1716069963931,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T22:06:04.071+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:06:04.073+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:06:04.087+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:06:04.088+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:06:04.159+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:04 INFO BlockManagerInfo: Removed broadcast_38_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:06:04.180+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T22:06:04.197+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:04 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:06:04.240+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:04 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T22:06:04.242+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:04 INFO DAGScheduler: Got job 39 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T22:06:04.242+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:04 INFO DAGScheduler: Final stage: ResultStage 39 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T22:06:04.242+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:04 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T22:06:04.243+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:04 INFO DAGScheduler: Missing parents: List()
[2024-05-18T22:06:04.244+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:04 INFO DAGScheduler: Submitting ResultStage 39 (PythonRDD[399] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T22:06:04.246+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:04 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 69.8 KiB, free 434.3 MiB)
[2024-05-18T22:06:04.249+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:04 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.3 MiB)
[2024-05-18T22:06:04.249+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:04 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:06:04.250+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:04 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1585
[2024-05-18T22:06:04.252+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (PythonRDD[399] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T22:06:04.252+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:04 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0
[2024-05-18T22:06:04.253+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:04 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 39) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T22:06:04.278+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:04 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:06:05.144+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:05 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 39) in 875 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T22:06:05.173+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:05 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool
[2024-05-18T22:06:05.174+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:05 INFO DAGScheduler: ResultStage 39 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 0.908 s
[2024-05-18T22:06:05.175+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:05 INFO DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T22:06:05.176+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
[2024-05-18T22:06:05.177+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:05 INFO DAGScheduler: Job 39 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 0.933378 s
[2024-05-18T22:06:05.239+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/39 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.39.05f33189-91c3-4c49-97fa-c2b6abd7e08c.tmp
[2024-05-18T22:06:05.284+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.39.05f33189-91c3-4c49-97fa-c2b6abd7e08c.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/39
[2024-05-18T22:06:05.292+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:05 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T22:06:05.292+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T22:06:05.293+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T22:06:05.294+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T22:06:05.294+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T22:06:03.930Z",
[2024-05-18T22:06:05.294+0000] {spark_submit.py:571} INFO - "batchId" : 39,
[2024-05-18T22:06:05.295+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T22:06:05.295+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-05-18T22:06:05.295+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7380073800738007,
[2024-05-18T22:06:05.296+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T22:06:05.296+0000] {spark_submit.py:571} INFO - "addBatch" : 1108,
[2024-05-18T22:06:05.296+0000] {spark_submit.py:571} INFO - "commitOffsets" : 97,
[2024-05-18T22:06:05.296+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-18T22:06:05.297+0000] {spark_submit.py:571} INFO - "latestOffset" : 1,
[2024-05-18T22:06:05.297+0000] {spark_submit.py:571} INFO - "queryPlanning" : 45,
[2024-05-18T22:06:05.297+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1355,
[2024-05-18T22:06:05.297+0000] {spark_submit.py:571} INFO - "walCommit" : 90
[2024-05-18T22:06:05.297+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:06:05.297+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T22:06:05.297+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T22:06:05.298+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T22:06:05.298+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T22:06:05.298+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:06:05.298+0000] {spark_submit.py:571} INFO - "0" : 41
[2024-05-18T22:06:05.298+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:06:05.298+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:06:05.299+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T22:06:05.299+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:06:05.299+0000] {spark_submit.py:571} INFO - "0" : 42
[2024-05-18T22:06:05.299+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:06:05.299+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:06:05.299+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T22:06:05.299+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:06:05.300+0000] {spark_submit.py:571} INFO - "0" : 42
[2024-05-18T22:06:05.300+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:06:05.300+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:06:05.300+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T22:06:05.300+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-05-18T22:06:05.300+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7380073800738007,
[2024-05-18T22:06:05.300+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T22:06:05.300+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T22:06:05.301+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T22:06:05.301+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T22:06:05.301+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:06:05.301+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T22:06:05.301+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T22:06:05.302+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T22:06:05.302+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T22:06:05.302+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:06:05.302+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:06:15.298+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:06:25.297+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:06:29.556+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/40 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.40.f8d5da87-cb27-420b-9682-bbf6f281f409.tmp
[2024-05-18T22:06:29.584+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/.40.f8d5da87-cb27-420b-9682-bbf6f281f409.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/offsets/40
[2024-05-18T22:06:29.585+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:29 INFO MicroBatchExecution: Committed offsets for batch 40. Metadata OffsetSeqMetadata(0,1716069989491,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-05-18T22:06:29.642+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:06:29.646+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:06:29.661+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:06:29.661+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-05-18T22:06:29.689+0000] {spark_submit.py:571} INFO - INFO:py4j.clientserver:Received command c on object id p0
[2024-05-18T22:06:29.761+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:29 INFO SparkContext: Starting job: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617
[2024-05-18T22:06:29.763+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:29 INFO DAGScheduler: Got job 40 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) with 1 output partitions
[2024-05-18T22:06:29.763+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:29 INFO DAGScheduler: Final stage: ResultStage 40 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617)
[2024-05-18T22:06:29.764+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:29 INFO DAGScheduler: Parents of final stage: List()
[2024-05-18T22:06:29.765+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:29 INFO DAGScheduler: Missing parents: List()
[2024-05-18T22:06:29.766+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:29 INFO DAGScheduler: Submitting ResultStage 40 (PythonRDD[409] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617), which has no missing parents
[2024-05-18T22:06:29.767+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:29 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 69.8 KiB, free 434.2 MiB)
[2024-05-18T22:06:29.769+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:29 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.2 MiB)
[2024-05-18T22:06:29.770+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:29 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on f3c105d69d83:38673 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:06:29.770+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:29 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1585
[2024-05-18T22:06:29.770+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (PythonRDD[409] at call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) (first 15 tasks are for partitions Vector(0))
[2024-05-18T22:06:29.771+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:29 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0
[2024-05-18T22:06:29.772+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:29 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 40) (172.25.0.6, executor 0, partition 0, PROCESS_LOCAL, 14269 bytes)
[2024-05-18T22:06:29.879+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:29 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 172.25.0.6:45173 (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:06:30.773+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:30 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 40) in 998 ms on 172.25.0.6 (executor 0) (1/1)
[2024-05-18T22:06:30.778+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:30 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool
[2024-05-18T22:06:30.782+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:30 INFO DAGScheduler: ResultStage 40 (call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617) finished in 1.011 s
[2024-05-18T22:06:30.790+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:30 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-05-18T22:06:30.791+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished
[2024-05-18T22:06:30.803+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:30 INFO DAGScheduler: Job 40 finished: call at /home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:617, took 1.040717 s
[2024-05-18T22:06:30.854+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/40 using temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.40.75b5e845-21aa-42e7-be81-fdf4741e1132.tmp
[2024-05-18T22:06:30.897+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/.40.75b5e845-21aa-42e7-be81-fdf4741e1132.tmp to file:/tmp/temporary-d9410eb3-aaea-4edf-b758-08021d6da693/commits/40
[2024-05-18T22:06:30.901+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:30 INFO MicroBatchExecution: Streaming query made progress: {
[2024-05-18T22:06:30.902+0000] {spark_submit.py:571} INFO - "id" : "7c05ded5-8d4c-4f2c-ac3c-99865592e6d1",
[2024-05-18T22:06:30.902+0000] {spark_submit.py:571} INFO - "runId" : "bced2123-102e-4a99-bbe9-1d6dba8f5896",
[2024-05-18T22:06:30.902+0000] {spark_submit.py:571} INFO - "name" : null,
[2024-05-18T22:06:30.902+0000] {spark_submit.py:571} INFO - "timestamp" : "2024-05-18T22:06:29.488Z",
[2024-05-18T22:06:30.903+0000] {spark_submit.py:571} INFO - "batchId" : 40,
[2024-05-18T22:06:30.903+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T22:06:30.903+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-05-18T22:06:30.903+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7097232079488999,
[2024-05-18T22:06:30.903+0000] {spark_submit.py:571} INFO - "durationMs" : {
[2024-05-18T22:06:30.904+0000] {spark_submit.py:571} INFO - "addBatch" : 1183,
[2024-05-18T22:06:30.904+0000] {spark_submit.py:571} INFO - "commitOffsets" : 62,
[2024-05-18T22:06:30.904+0000] {spark_submit.py:571} INFO - "getBatch" : 0,
[2024-05-18T22:06:30.904+0000] {spark_submit.py:571} INFO - "latestOffset" : 3,
[2024-05-18T22:06:30.904+0000] {spark_submit.py:571} INFO - "queryPlanning" : 61,
[2024-05-18T22:06:30.904+0000] {spark_submit.py:571} INFO - "triggerExecution" : 1409,
[2024-05-18T22:06:30.905+0000] {spark_submit.py:571} INFO - "walCommit" : 94
[2024-05-18T22:06:30.905+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:06:30.905+0000] {spark_submit.py:571} INFO - "stateOperators" : [ ],
[2024-05-18T22:06:30.905+0000] {spark_submit.py:571} INFO - "sources" : [ {
[2024-05-18T22:06:30.905+0000] {spark_submit.py:571} INFO - "description" : "KafkaV2[Subscribe[redfin_properties]]",
[2024-05-18T22:06:30.905+0000] {spark_submit.py:571} INFO - "startOffset" : {
[2024-05-18T22:06:30.905+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:06:30.905+0000] {spark_submit.py:571} INFO - "0" : 42
[2024-05-18T22:06:30.906+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:06:30.906+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:06:30.906+0000] {spark_submit.py:571} INFO - "endOffset" : {
[2024-05-18T22:06:30.906+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:06:30.907+0000] {spark_submit.py:571} INFO - "0" : 43
[2024-05-18T22:06:30.907+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:06:30.907+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:06:30.907+0000] {spark_submit.py:571} INFO - "latestOffset" : {
[2024-05-18T22:06:30.907+0000] {spark_submit.py:571} INFO - "redfin_properties" : {
[2024-05-18T22:06:30.907+0000] {spark_submit.py:571} INFO - "0" : 43
[2024-05-18T22:06:30.908+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:06:30.908+0000] {spark_submit.py:571} INFO - },
[2024-05-18T22:06:30.908+0000] {spark_submit.py:571} INFO - "numInputRows" : 1,
[2024-05-18T22:06:30.908+0000] {spark_submit.py:571} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-05-18T22:06:30.908+0000] {spark_submit.py:571} INFO - "processedRowsPerSecond" : 0.7097232079488999,
[2024-05-18T22:06:30.908+0000] {spark_submit.py:571} INFO - "metrics" : {
[2024-05-18T22:06:30.908+0000] {spark_submit.py:571} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-05-18T22:06:30.908+0000] {spark_submit.py:571} INFO - "maxOffsetsBehindLatest" : "0",
[2024-05-18T22:06:30.909+0000] {spark_submit.py:571} INFO - "minOffsetsBehindLatest" : "0"
[2024-05-18T22:06:30.909+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:06:30.909+0000] {spark_submit.py:571} INFO - } ],
[2024-05-18T22:06:30.909+0000] {spark_submit.py:571} INFO - "sink" : {
[2024-05-18T22:06:30.909+0000] {spark_submit.py:571} INFO - "description" : "ForeachBatchSink",
[2024-05-18T22:06:30.909+0000] {spark_submit.py:571} INFO - "numOutputRows" : -1
[2024-05-18T22:06:30.909+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:06:30.909+0000] {spark_submit.py:571} INFO - }
[2024-05-18T22:06:35.191+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:35 INFO BlockManagerInfo: Removed broadcast_39_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:06:35.212+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:35 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:06:35.218+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:35 INFO BlockManagerInfo: Removed broadcast_40_piece0 on f3c105d69d83:38673 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:06:35.222+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:35 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 172.25.0.6:45173 in memory (size: 25.4 KiB, free: 434.4 MiB)
[2024-05-18T22:06:40.914+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:06:50.915+0000] {spark_submit.py:571} INFO - 24/05/18 22:06:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:07:00.918+0000] {spark_submit.py:571} INFO - 24/05/18 22:07:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:07:10.932+0000] {spark_submit.py:571} INFO - 24/05/18 22:07:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:07:20.960+0000] {spark_submit.py:571} INFO - 24/05/18 22:07:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:07:30.931+0000] {spark_submit.py:571} INFO - 24/05/18 22:07:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:07:40.940+0000] {spark_submit.py:571} INFO - 24/05/18 22:07:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:07:50.949+0000] {spark_submit.py:571} INFO - 24/05/18 22:07:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-05-18T22:07:55.591+0000] {local_task_job_runner.py:310} WARNING - State of this instance has been externally set to failed. Terminating instance.
[2024-05-18T22:07:55.636+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-05-18T22:07:55.651+0000] {process_utils.py:132} INFO - Sending 15 to group 5543. PIDs of all processes in the group: [5545, 5617, 5543]
[2024-05-18T22:07:55.652+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 5543
[2024-05-18T22:07:55.656+0000] {taskinstance.py:2611} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-05-18T22:07:55.664+0000] {spark_submit.py:697} INFO - Sending kill signal to spark-submit
[2024-05-18T22:07:55.675+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-05-18T22:07:55.800+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=5617, status='terminated', started='21:42:26') (5617) terminated with exit code None
[2024-05-18T22:07:56.169+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=5545, status='terminated', started='21:42:22') (5545) terminated with exit code None
[2024-05-18T22:07:56.177+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=5543, status='terminated', exitcode=0, started='21:42:21') (5543) terminated with exit code 0
